{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conservative-trader",
   "metadata": {
    "id": "thermal-humanity"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "#from dlc_practical_prologue import *\n",
    "#torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "widespread-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTERESTING FACT: WHEN COMPUTING THE BATCH GRADIENT PYTORCH DOES NOT TAKE THE SUM OF THE GRADIENTA OF SINGLE DATA POINTS\n",
    "#BUT THE MEAN. I LEART AT THE EXPENSE OF ABOUT 1H LOL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "smooth-doctor",
   "metadata": {
    "id": "found-annex"
   },
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    def __init__(self):\n",
    "        self.name = ''\n",
    "        self.data = None\n",
    "        self.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "chicken-albany",
   "metadata": {
    "id": "therapeutic-current"
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward (self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_parameters( self ) :\n",
    "        return []   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "large-badge",
   "metadata": {
    "id": "imperial-class",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Losses(object):        \n",
    "    def forward():\n",
    "        return NotImplementedError\n",
    "    def backward():\n",
    "        NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sticky-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def zero_grad(self):\n",
    "        for parameter in self.param : \n",
    "            parameter.grad = 0\n",
    "            \n",
    "    def step(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "seventh-secretary",
   "metadata": {
    "id": "comfortable-calgary"
   },
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    # this is a SGD optimizer\n",
    "    def __init__(self,lr,max_iter, parameters) :  \n",
    "        super().__init__()\n",
    "        self.eta = lr\n",
    "        self.maxStep = max_iter \n",
    "        self.param = parameters\n",
    "        self.number_step = 0\n",
    "\n",
    "    def step(self): \n",
    "        if self.number_step <=self.maxStep:\n",
    "            for parameter in self.param :\n",
    "                parameter.data = parameter.data - self.eta * parameter.grad\n",
    "            self.number_step = self.number_step + 1\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dressed-chaos",
   "metadata": {
    "id": "asian-evanescence"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, input_dim, out_dim, bias = True):\n",
    "        super().__init__()\n",
    "        std = 1/math.sqrt(input_dim)\n",
    "        self.weight = Parameter()\n",
    "        self.parameters = []\n",
    "        \n",
    "        self.weight.data = torch.rand(out_dim, input_dim)\n",
    "        self.weight.data = 2*std*self.weight.data - std\n",
    "        self.weight.name = 'weight'\n",
    "        self.parameters += [self.weight]\n",
    "        \n",
    "        self.with_bias = bias\n",
    "        if bias :\n",
    "            self.bias = Parameter()\n",
    "            self.bias.data = torch.rand(out_dim)\n",
    "            self.bias.data = 2*std*self.bias.data - std\n",
    "            self.bias.data = self.bias.data.unsqueeze(0)\n",
    "            self.bias.name = 'bias'\n",
    "            self.parameters +=[self.bias]\n",
    "            \n",
    "        self.x = None\n",
    "              \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.batch_size = x.shape[0]\n",
    "        return self.x.mm(self.weight.data.T) + self.bias.data\n",
    "        \n",
    "    def backward(self, prev_grad):\n",
    "        \n",
    "        prev_grad = prev_grad.view(self.batch_size, -1, 1)\n",
    "        #print(prev_grad.shape)\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "        \n",
    "        if self.weight.grad is None:\n",
    "            self.weight.grad = torch.zeros_like(self.weight.data)\n",
    "        \n",
    "        grad_on_batch = prev_grad.view(self.batch_size, -1, 1)*self.x.view(self.batch_size, 1, -1)\n",
    "        self.weight.grad += grad_on_batch.mean(0)\n",
    "        \n",
    "        if self.with_bias:\n",
    "            if self.bias.grad is None:\n",
    "                self.bias.grad = torch.zeros_like(self.bias.data)\n",
    "            grad_on_batch = prev_grad.view(self.batch_size, -1)\n",
    "            self.bias.grad += grad_on_batch.mean(0)\n",
    "        \n",
    "        #if the output has dimension one, squeezing creates problems\n",
    "        if prev_grad.shape[1]>1:\n",
    "            prev_grad = prev_grad.squeeze()\n",
    "        next_grad = prev_grad@self.weight.data\n",
    "        return next_grad.squeeze()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ordered-competition",
   "metadata": {
    "id": "written-benjamin",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def forward (self, x):\n",
    "        self.x = x\n",
    "        return torch.tanh(x)\n",
    "        \n",
    "    def backward ( self, prev_grad) :\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "            \n",
    "        def d(x):\n",
    "            return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)\n",
    "        \n",
    "        return d(self.x)*prev_grad\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "objective-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu(Module):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def forward (self, x):\n",
    "        self.x = x\n",
    "        x[x<0]=0\n",
    "        return x\n",
    "        \n",
    "    def backward ( self, prev_grad) :\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "            \n",
    "        def d(x):\n",
    "            x[x<0]=0\n",
    "            x[x>0]=1\n",
    "            return x\n",
    "        \n",
    "        return d(self.x)*prev_grad\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "accompanied-resident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.8904,  0.9226,  0.9735,  0.5800, -0.4941],\n",
       "         [-0.0555, -0.8688, -0.6625,  0.1297, -0.5309],\n",
       "         [ 0.7964, -2.3508,  0.8163, -0.4328,  1.1595],\n",
       "         [-1.0723, -0.0455, -0.9628,  2.6265,  0.6534],\n",
       "         [-0.4571,  1.5463,  1.8305,  0.1868,  1.3160]]),\n",
       " tensor([[0.8904, 0.9226, 0.9735, 0.5800, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.1297, 0.0000],\n",
       "         [0.7964, 0.0000, 0.8163, 0.0000, 1.1595],\n",
       "         [0.0000, 0.0000, 0.0000, 2.6265, 0.6534],\n",
       "         [0.0000, 1.5463, 1.8305, 0.1868, 1.3160]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5, 5)\n",
    "y = x.clone()\n",
    "y[y<0] = 0\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "exotic-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't work and we have to output 1 unit so it's useless\n",
    "class Softmax(Module):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def forward (self, x):\n",
    "        self.x = x\n",
    "        return torch.softmax(x,1)\n",
    "    \n",
    "    def d(self, x):\n",
    "            s = x.softmax(1)\n",
    "            temp = s.unsqueeze(-1)\n",
    "            off_diag = temp.view(-1,1, temp.shape[1] )*temp\n",
    "            diag = torch.diag_embed(torch.diagonal(temp, dim1 = 1, dim2 = 2).sqrt()) \n",
    "            return diag - off_diag\n",
    "        \n",
    "    def backward ( self, prev_grad) :\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "            \n",
    "        def d(x):\n",
    "            s = x.softmax(1)\n",
    "            temp = s.unsqueeze(-1)\n",
    "            off_diag = temp.view(-1,1, temp.shape[1] )*temp\n",
    "            diag = torch.diag_embed(torch.diagonal(temp, dim1 = 1, dim2 = 2).sqrt()) \n",
    "            return -diag + off_diag\n",
    "        print(d(self.x).shape, prev_grad.T.shape)\n",
    "        return torch.einsum('b ij, bj -> bi', d(self.x),prev_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "forty-enclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check on the values:\n",
      " tensor([[0.0553, 0.1815, 0.2808, 0.4004, 0.0820]], grad_fn=<SoftmaxBackward>) \n",
      " tensor([[0.0553, 0.1815, 0.2808, 0.4004, 0.0820]], grad_fn=<SoftmaxBackward>) diff:\n",
      " tensor([[0., 0., 0., 0., 0.]], grad_fn=<AbsBackward>)\n",
      "torch.Size([1, 5, 5]) torch.Size([5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[ 0.0523, -0.0100, -0.0155, -0.0222, -0.0045]]),),\n",
       " tensor([[[0.2322, 0.2252, 0.2197, 0.2131, 0.2307],\n",
       "          [0.2252, 0.2023, 0.1843, 0.1626, 0.2204],\n",
       "          [0.2197, 0.1843, 0.1564, 0.1228, 0.2122],\n",
       "          [0.2131, 0.1626, 0.1228, 0.0749, 0.2024],\n",
       "          [0.2307, 0.2204, 0.2122, 0.2024, 0.2285]]], grad_fn=<SubBackward0>))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 5, requires_grad = True)\n",
    "soft = Softmax()\n",
    "s_b = x.softmax(1)\n",
    "s_h = soft.forward(x.clone())\n",
    "print('check on the values:\\n', s_b ,'\\n', s_h, 'diff:\\n', abs(s_b-s_h) )\n",
    "\n",
    "soft.backward(torch.ones(1, 5))\n",
    "torch.autograd.grad(s_b[0,0], x, retain_graph = True), soft.d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "modified-worcester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5., 12.,  7.,  8.,  9.],\n",
       "         [10., 11., 24., 13., 14.],\n",
       "         [15., 16., 17., 36., 19.],\n",
       "         [20., 21., 22., 23., 48.]],\n",
       "\n",
       "        [[ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5., 12.,  7.,  8.,  9.],\n",
       "         [10., 11., 24., 13., 14.],\n",
       "         [15., 16., 17., 36., 19.],\n",
       "         [20., 21., 22., 23., 48.]],\n",
       "\n",
       "        [[ 0.,  1.,  2.,  3.,  4.],\n",
       "         [ 5., 12.,  7.,  8.,  9.],\n",
       "         [10., 11., 24., 13., 14.],\n",
       "         [15., 16., 17., 36., 19.],\n",
       "         [20., 21., 22., 23., 48.]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1.*torch.arange(25).view(5, 5).unsqueeze(0)\n",
    "a = torch.cat((a, a, a), dim = 0)\n",
    "torch.diag_embed(torch.diagonal(a, dim1 = 1, dim2 = 2).sqrt()) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "latin-editing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-a80662d4ff2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tirocinio\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tirocinio\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tirocinio\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "soft = Softmax()\n",
    "x = torch.randn(1, 5, requires_grad = True)\n",
    "s = x.softmax(1)\n",
    "s.backward()\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "capable-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't work and we have to output 1 unit so it's useless\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def forward (self, x):\n",
    "        self.x = x\n",
    "        return torch.sigmoid(x)\n",
    "        \n",
    "    def backward ( self, prev_grad) :\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "            \n",
    "        def d(x):\n",
    "            y=torch.sigmoid(x)\n",
    "            return y*(1-y)\n",
    "        \n",
    "        return d(self.x)*prev_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "annoying-liberty",
   "metadata": {
    "id": "happy-review"
   },
   "outputs": [],
   "source": [
    "class MSE(Losses):\n",
    "    # Attention! Works well only when the vectors provided are of the form [batch_size, vector dimension]\n",
    "    # Otherwise it doesn know what dimesion to pick for the mean computation\n",
    "    # I'll fix this later\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    def forward(self, x, t):\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        return (x - t).pow(2).mean()\n",
    "    \n",
    "    def backward(self):\n",
    "        if self.x == None or self.t == None:\n",
    "            raise CallForwardFirst\n",
    "        return 2 * (self.x - self.t)/self.x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "approximate-guard",
   "metadata": {
    "id": "active-skirt"
   },
   "outputs": [],
   "source": [
    "class Sequential(object):\n",
    "    def __init__(self, modules):\n",
    "        super().__init__()\n",
    "        self.modules=modules\n",
    "        self.parameters = []\n",
    "        for m in self.modules:\n",
    "            param = m.get_parameters()\n",
    "            if param:\n",
    "                self.parameters += param\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for m in self.modules:\n",
    "            x=m.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, loss_grad):\n",
    "        x = loss_grad\n",
    "        for m in reversed(self.modules):\n",
    "            #print(m)\n",
    "            x = m.backward(x)\n",
    "            \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "    def set_parameters(self , params):\n",
    "        self.parameters = params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-gateway",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "excess-designation",
   "metadata": {
    "id": "legal-buying"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x59abdc8>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input_ = torch.empty(nb, 2).uniform_(0, 1)\n",
    "    target = (input_-0.5).pow(2).sum(1).sub(1 / (math.pi*2)).sign().add(1).div(2).long()\n",
    "    return input_, target\n",
    "input_data, output_data=generate_disc_set(1000)\n",
    "output_data=1-output_data\n",
    "input_data-=input_data.mean(0)\n",
    "input_data/=input_data.std(0)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(input_data[:,0][output_data==1],input_data[:,1][output_data==1],'bo')\n",
    "plt.plot(input_data[:,0][output_data!=1],input_data[:,1][output_data!=1],'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "human-lawrence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2653,  0.9834],\n",
      "        [-1.2886, -1.7250],\n",
      "        [ 0.6601,  1.0128],\n",
      "        ...,\n",
      "        [-0.2250,  0.4314],\n",
      "        [-0.4552, -1.4729],\n",
      "        [ 0.1431,  1.3598]]) torch.Size([1000, 2]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "#output_data = 2*output_data -1\n",
    "print(input_data, input_data.shape, output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "constitutional-intake",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we should use one hot label embedding with MSELoss. Hence it is necessary to implement a Softmax\n",
    "#Maybe we should also implement CrossEntropyLoss\n",
    "#print(input_data.shape, output_data.shape)\n",
    "#output_data = convert_to_one_hot_labels(input_data, output_data)\n",
    "#print(input_data.shape, output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "sacred-camera",
   "metadata": {
    "id": "legal-buying"
   },
   "outputs": [],
   "source": [
    "#handmade sequential linear + relu \n",
    "linear1 = Linear(2, 25, True)\n",
    "linear2 = Linear(25,25,True)\n",
    "linear3 = Linear(25,1,True)\n",
    "sigma1 = Tanh()\n",
    "sigma2 = Tanh()\n",
    "sigma3 = Tanh()\n",
    "loss = MSE()\n",
    "\n",
    "net = Sequential([\n",
    "    linear1, \n",
    "    sigma1 ,\n",
    "    linear2,\n",
    "    sigma2 ,\n",
    "    linear3,\n",
    "    sigma3 ,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "decimal-exhaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    MSE loss =  1049.139892578125\n",
      "10    MSE loss =  951.9638671875\n",
      "20    MSE loss =  897.6994018554688\n",
      "30    MSE loss =  828.3200073242188\n",
      "40    MSE loss =  743.03173828125\n",
      "50    MSE loss =  649.316650390625\n",
      "60    MSE loss =  559.2086181640625\n",
      "70    MSE loss =  481.24493408203125\n",
      "80    MSE loss =  417.7786865234375\n",
      "90    MSE loss =  367.5589599609375\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(lr = 1e-4,max_iter = 100, parameters = net.get_parameters())\n",
    "n=10**2\n",
    "N=output_data.shape[0]\n",
    "for t in range(n):\n",
    "    optimizer.zero_grad()\n",
    "    acc_loss=0\n",
    "    permuted_index = torch.randperm(input_data.size()[0])\n",
    "    input_data_shuffled = input_data[permuted_index]\n",
    "    output_data_shuffled = output_data[permuted_index]\n",
    "    for i in range(N):\n",
    "        x=input_data_shuffled[i]\n",
    "        y=2*output_data_shuffled[i]-1\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = net.forward(x.unsqueeze(0))\n",
    "        # Compute and print loss.\n",
    "        acc_loss += loss.forward(y_pred,y.unsqueeze(0))\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        net.backward(loss.backward())\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    new_par = optimizer.step()\n",
    "    #print(len(new_par))\n",
    "    net.set_parameters(new_par)\n",
    "    \n",
    "    \n",
    "    if t%10==0:\n",
    "        print(t, '   MSE loss = ' , acc_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "adjusted-cornwall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions after 100 training steps: 96.7 %\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "for i in range(output_data.shape[0]):\n",
    "        x=input_data[i]\n",
    "        y=2*output_data[i]-1\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = net.forward(x.unsqueeze(0))\n",
    "        if abs(y_pred-output_data[i])<1:\n",
    "            correct+=1\n",
    "print('Correct predictions after '+str(n)+' training steps: '+str(correct/N*100)+' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "middle-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reinitialize net\n",
    "#handmade sequential linear + relu \n",
    "linear1 = Linear(2, 25, True)\n",
    "linear2 = Linear(25,25,True)\n",
    "linear3 = Linear(25,1,True)\n",
    "sigma1 = Tanh()\n",
    "sigma2 = Tanh()\n",
    "sigma3 = Tanh()\n",
    "loss = MSE()\n",
    "\n",
    "net = Sequential([\n",
    "    linear1, \n",
    "    sigma1 ,\n",
    "    linear2,\n",
    "    sigma2 ,\n",
    "    linear3,\n",
    "    sigma3 ,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "south-grain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    MSE loss =  2.6662144660949707\n",
      "1    MSE loss =  2.2822318077087402\n",
      "2    MSE loss =  2.215752363204956\n",
      "3    MSE loss =  2.156341552734375\n",
      "4    MSE loss =  2.093371868133545\n",
      "5    MSE loss =  2.023648262023926\n",
      "6    MSE loss =  1.9451017379760742\n",
      "7    MSE loss =  1.8829989433288574\n",
      "8    MSE loss =  1.79679274559021\n",
      "9    MSE loss =  1.7139122486114502\n",
      "10    MSE loss =  1.6537455320358276\n",
      "11    MSE loss =  1.6488524675369263\n",
      "12    MSE loss =  1.6488525867462158\n",
      "13    MSE loss =  1.6488524675369263\n",
      "14    MSE loss =  1.6488525867462158\n",
      "15    MSE loss =  1.6488525867462158\n",
      "16    MSE loss =  1.6488524675369263\n",
      "17    MSE loss =  1.6488524675369263\n",
      "18    MSE loss =  1.6488527059555054\n",
      "19    MSE loss =  1.6488525867462158\n",
      "20    MSE loss =  1.6488524675369263\n",
      "21    MSE loss =  1.6488525867462158\n",
      "22    MSE loss =  1.6488525867462158\n",
      "23    MSE loss =  1.6488524675369263\n",
      "24    MSE loss =  1.6488525867462158\n",
      "25    MSE loss =  1.6488525867462158\n",
      "26    MSE loss =  1.6488525867462158\n",
      "27    MSE loss =  1.6488525867462158\n",
      "28    MSE loss =  1.6488525867462158\n",
      "29    MSE loss =  1.6488525867462158\n",
      "30    MSE loss =  1.6488525867462158\n",
      "31    MSE loss =  1.6488525867462158\n",
      "32    MSE loss =  1.6488525867462158\n",
      "33    MSE loss =  1.6488524675369263\n",
      "34    MSE loss =  1.6488525867462158\n",
      "35    MSE loss =  1.6488525867462158\n",
      "36    MSE loss =  1.6488525867462158\n",
      "37    MSE loss =  1.6488525867462158\n",
      "38    MSE loss =  1.6488525867462158\n",
      "39    MSE loss =  1.6488524675369263\n",
      "40    MSE loss =  1.6488525867462158\n",
      "41    MSE loss =  1.6488524675369263\n",
      "42    MSE loss =  1.6488523483276367\n",
      "43    MSE loss =  1.6488523483276367\n",
      "44    MSE loss =  1.6488525867462158\n",
      "45    MSE loss =  1.6488524675369263\n",
      "46    MSE loss =  1.6488525867462158\n",
      "47    MSE loss =  1.6488523483276367\n",
      "48    MSE loss =  1.6488524675369263\n",
      "49    MSE loss =  1.6488523483276367\n",
      "50    MSE loss =  1.6488523483276367\n",
      "51    MSE loss =  1.6488525867462158\n",
      "52    MSE loss =  1.6488525867462158\n",
      "53    MSE loss =  1.6488525867462158\n",
      "54    MSE loss =  1.6488525867462158\n",
      "55    MSE loss =  1.6488524675369263\n",
      "56    MSE loss =  1.6488525867462158\n",
      "57    MSE loss =  1.6488524675369263\n",
      "58    MSE loss =  1.6488525867462158\n",
      "59    MSE loss =  1.6488525867462158\n",
      "60    MSE loss =  1.6488525867462158\n",
      "61    MSE loss =  1.6488525867462158\n",
      "62    MSE loss =  1.6488524675369263\n",
      "63    MSE loss =  1.6488524675369263\n",
      "64    MSE loss =  1.6488524675369263\n",
      "65    MSE loss =  1.6488524675369263\n",
      "66    MSE loss =  1.6488523483276367\n",
      "67    MSE loss =  1.6488525867462158\n",
      "68    MSE loss =  1.6488524675369263\n",
      "69    MSE loss =  1.6488524675369263\n",
      "70    MSE loss =  1.6488525867462158\n",
      "71    MSE loss =  1.6488525867462158\n",
      "72    MSE loss =  1.6488524675369263\n",
      "73    MSE loss =  1.6488524675369263\n",
      "74    MSE loss =  1.6488525867462158\n",
      "75    MSE loss =  1.6488525867462158\n",
      "76    MSE loss =  1.6488525867462158\n",
      "77    MSE loss =  1.6488525867462158\n",
      "78    MSE loss =  1.6488524675369263\n",
      "79    MSE loss =  1.6488525867462158\n",
      "80    MSE loss =  1.6488527059555054\n",
      "81    MSE loss =  1.6488524675369263\n",
      "82    MSE loss =  1.6488524675369263\n",
      "83    MSE loss =  1.6488525867462158\n",
      "84    MSE loss =  1.6488525867462158\n",
      "85    MSE loss =  1.6488525867462158\n",
      "86    MSE loss =  1.6488525867462158\n",
      "87    MSE loss =  1.6488524675369263\n",
      "88    MSE loss =  1.6488525867462158\n",
      "89    MSE loss =  1.6488524675369263\n",
      "90    MSE loss =  1.6488525867462158\n",
      "91    MSE loss =  1.6488524675369263\n",
      "92    MSE loss =  1.6488524675369263\n",
      "93    MSE loss =  1.6488525867462158\n",
      "94    MSE loss =  1.6488525867462158\n",
      "95    MSE loss =  1.6488524675369263\n",
      "96    MSE loss =  1.6488524675369263\n",
      "97    MSE loss =  1.6488525867462158\n",
      "98    MSE loss =  1.6488527059555054\n",
      "99    MSE loss =  1.6488525867462158\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(lr = 1e-1,max_iter = 100, parameters = net.get_parameters())\n",
    "n=10**2\n",
    "N=output_data.shape[0]\n",
    "batch_size = 100\n",
    "for t in range(n):\n",
    "    acc_loss=0\n",
    "    permuted_index = torch.randperm(input_data.size()[0])\n",
    "    input_data_shuffled = input_data[permuted_index]\n",
    "    output_data_shuffled = output_data[permuted_index]\n",
    "    for b in range(0, N, batch_size):\n",
    "        predictions = net.forward(input_data_shuffled[b:b+batch_size])\n",
    "        l= loss.forward(predictions, output_data_shuffled[b:b+batch_size].unsqueeze(-1))\n",
    "        acc_loss += l\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        net.backward(loss.backward())\n",
    "        \n",
    "        new_par = optimizer.step()\n",
    "        net.set_parameters(new_par)\n",
    "    \n",
    "    print(t, '   MSE loss = ' , acc_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "exceptional-funeral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions after 100 training steps: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "for i in range(output_data.shape[0]):\n",
    "        x=input_data[i]\n",
    "        y=2*output_data[i]-1\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = net.forward(x.unsqueeze(0))\n",
    "        if abs(y_pred-output_data[i])<1:\n",
    "            correct+=1\n",
    "print('Correct predictions after '+str(n)+' training steps: '+str(correct/N*100)+' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "incredible-pearl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    MSE loss =  253.6742401123047\n",
      "10    MSE loss =  250.0392303466797\n",
      "20    MSE loss =  247.07595825195312\n",
      "30    MSE loss =  244.49795532226562\n",
      "40    MSE loss =  242.05059814453125\n",
      "50    MSE loss =  239.64300537109375\n",
      "60    MSE loss =  237.228759765625\n",
      "70    MSE loss =  234.7724151611328\n",
      "80    MSE loss =  232.2540283203125\n",
      "90    MSE loss =  229.65501403808594\n",
      "Correct predictions after 100 training steps: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "#handmade sequential linear + relu with sigmoid to end and no batches\n",
    "linear1 = Linear(2, 25, True)\n",
    "linear2 = Linear(25,25,True)\n",
    "linear3 = Linear(25,1,True)\n",
    "sigma1 = ReLu()\n",
    "sigma2 = ReLu()\n",
    "sigma3 = Sigmoid()\n",
    "loss = MSE()\n",
    "\n",
    "net = Sequential([\n",
    "    linear1, \n",
    "    sigma1 ,\n",
    "    linear2,\n",
    "    sigma2 ,\n",
    "    linear3,\n",
    "    sigma3 ,\n",
    "])\n",
    "optimizer = SGD(lr = 1e-4,max_iter = 100, parameters = net.get_parameters())\n",
    "n=10**2\n",
    "N=output_data.shape[0]\n",
    "for t in range(n):\n",
    "    optimizer.zero_grad()\n",
    "    acc_loss=0\n",
    "    permuted_index = torch.randperm(input_data.size()[0])\n",
    "    input_data_shuffled = input_data[permuted_index]\n",
    "    output_data_shuffled = output_data[permuted_index]\n",
    "    for i in range(N):\n",
    "        x=input_data_shuffled[i]\n",
    "        y=output_data_shuffled[i]\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = net.forward(x.unsqueeze(0))\n",
    "        # Compute and print loss.\n",
    "        acc_loss += loss.forward(y_pred,y.unsqueeze(0))\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        net.backward(loss.backward())\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    new_par = optimizer.step()\n",
    "    #print(len(new_par))\n",
    "    net.set_parameters(new_par)\n",
    "    \n",
    "    \n",
    "    if t%10==0:\n",
    "        print(t, '   MSE loss = ' , acc_loss.item())\n",
    "    \n",
    "correct=0\n",
    "for i in range(output_data.shape[0]):\n",
    "        x=input_data[i]\n",
    "        y=2*output_data[i]-1\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = net.forward(x.unsqueeze(0))\n",
    "        if abs(y_pred-output_data[i])<1:\n",
    "            correct+=1\n",
    "print('Correct predictions after '+str(n)+' training steps: '+str(correct/N*100)+' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "elegant-romance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    MSE loss =  2.4813714027404785\n",
      "1    MSE loss =  2.46077036857605\n",
      "2    MSE loss =  2.4431357383728027\n",
      "3    MSE loss =  2.427443265914917\n",
      "4    MSE loss =  2.4113965034484863\n",
      "5    MSE loss =  2.3954713344573975\n",
      "6    MSE loss =  2.3800158500671387\n",
      "7    MSE loss =  2.3642022609710693\n",
      "8    MSE loss =  2.3473432064056396\n",
      "9    MSE loss =  2.3301541805267334\n",
      "10    MSE loss =  2.319033145904541\n",
      "11    MSE loss =  2.318113327026367\n",
      "12    MSE loss =  2.318113088607788\n",
      "13    MSE loss =  2.318113088607788\n",
      "14    MSE loss =  2.318113088607788\n",
      "15    MSE loss =  2.318112850189209\n",
      "16    MSE loss =  2.318113088607788\n",
      "17    MSE loss =  2.318112850189209\n",
      "18    MSE loss =  2.318113088607788\n",
      "19    MSE loss =  2.318113088607788\n",
      "20    MSE loss =  2.318113088607788\n",
      "21    MSE loss =  2.318113327026367\n",
      "22    MSE loss =  2.318112850189209\n",
      "23    MSE loss =  2.318113327026367\n",
      "24    MSE loss =  2.318113088607788\n",
      "25    MSE loss =  2.318113088607788\n",
      "26    MSE loss =  2.318112850189209\n",
      "27    MSE loss =  2.318112850189209\n",
      "28    MSE loss =  2.318113327026367\n",
      "29    MSE loss =  2.318113088607788\n",
      "30    MSE loss =  2.318113088607788\n",
      "31    MSE loss =  2.318113088607788\n",
      "32    MSE loss =  2.318113088607788\n",
      "33    MSE loss =  2.318113088607788\n",
      "34    MSE loss =  2.318113088607788\n",
      "35    MSE loss =  2.318113088607788\n",
      "36    MSE loss =  2.318113327026367\n",
      "37    MSE loss =  2.318113088607788\n",
      "38    MSE loss =  2.318113088607788\n",
      "39    MSE loss =  2.318113088607788\n",
      "40    MSE loss =  2.318112850189209\n",
      "41    MSE loss =  2.318113088607788\n",
      "42    MSE loss =  2.318113088607788\n",
      "43    MSE loss =  2.318113327026367\n",
      "44    MSE loss =  2.318113088607788\n",
      "45    MSE loss =  2.318112850189209\n",
      "46    MSE loss =  2.318112850189209\n",
      "47    MSE loss =  2.318112850189209\n",
      "48    MSE loss =  2.318113088607788\n",
      "49    MSE loss =  2.318113088607788\n",
      "50    MSE loss =  2.318113088607788\n",
      "51    MSE loss =  2.318113327026367\n",
      "52    MSE loss =  2.318113088607788\n",
      "53    MSE loss =  2.318113327026367\n",
      "54    MSE loss =  2.318113088607788\n",
      "55    MSE loss =  2.318113088607788\n",
      "56    MSE loss =  2.318113088607788\n",
      "57    MSE loss =  2.318113088607788\n",
      "58    MSE loss =  2.318113088607788\n",
      "59    MSE loss =  2.318113088607788\n",
      "60    MSE loss =  2.318113088607788\n",
      "61    MSE loss =  2.318113088607788\n",
      "62    MSE loss =  2.318113088607788\n",
      "63    MSE loss =  2.318113088607788\n",
      "64    MSE loss =  2.318113088607788\n",
      "65    MSE loss =  2.318112850189209\n",
      "66    MSE loss =  2.318112850189209\n",
      "67    MSE loss =  2.318113327026367\n",
      "68    MSE loss =  2.318113088607788\n",
      "69    MSE loss =  2.318113088607788\n",
      "70    MSE loss =  2.318113088607788\n",
      "71    MSE loss =  2.318113088607788\n",
      "72    MSE loss =  2.318113088607788\n",
      "73    MSE loss =  2.318113088607788\n",
      "74    MSE loss =  2.318113088607788\n",
      "75    MSE loss =  2.318113088607788\n",
      "76    MSE loss =  2.318112850189209\n",
      "77    MSE loss =  2.318113088607788\n",
      "78    MSE loss =  2.318113088607788\n",
      "79    MSE loss =  2.318113088607788\n",
      "80    MSE loss =  2.318113088607788\n",
      "81    MSE loss =  2.318112850189209\n",
      "82    MSE loss =  2.318113327026367\n",
      "83    MSE loss =  2.318113088607788\n",
      "84    MSE loss =  2.318112850189209\n",
      "85    MSE loss =  2.318113088607788\n",
      "86    MSE loss =  2.318113088607788\n",
      "87    MSE loss =  2.318112850189209\n",
      "88    MSE loss =  2.318113088607788\n",
      "89    MSE loss =  2.318113327026367\n",
      "90    MSE loss =  2.318113088607788\n",
      "91    MSE loss =  2.318113088607788\n",
      "92    MSE loss =  2.318113088607788\n",
      "93    MSE loss =  2.318113088607788\n",
      "94    MSE loss =  2.318112850189209\n",
      "95    MSE loss =  2.318113088607788\n",
      "96    MSE loss =  2.318113327026367\n",
      "97    MSE loss =  2.318113088607788\n",
      "98    MSE loss =  2.318113088607788\n",
      "99    MSE loss =  2.318113088607788\n",
      "Correct predictions after 100 training steps: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "#handmade sequential linear + relu with softmax with batches\n",
    "linear1 = Linear(2, 25, True)\n",
    "linear2 = Linear(25,25,True)\n",
    "linear3 = Linear(25,1,True)\n",
    "sigma1 = ReLu()\n",
    "sigma2 = ReLu()\n",
    "sigma3 = Sigmoid()\n",
    "loss = MSE()\n",
    "\n",
    "net = Sequential([\n",
    "    linear1, \n",
    "    sigma1 ,\n",
    "    linear2,\n",
    "    sigma2 ,\n",
    "    linear3,\n",
    "    sigma3 ,\n",
    "])\n",
    "optimizer = SGD(lr = 1e-1,max_iter = 100, parameters = net.get_parameters())\n",
    "n=10**2\n",
    "N=output_data.shape[0]\n",
    "batch_size = 100\n",
    "for t in range(n):\n",
    "    acc_loss=0\n",
    "    permuted_index = torch.randperm(input_data.size()[0])\n",
    "    input_data_shuffled = input_data[permuted_index]\n",
    "    output_data_shuffled = output_data[permuted_index]\n",
    "    for b in range(0, N, batch_size):\n",
    "        predictions = net.forward(input_data_shuffled[b:b+batch_size])\n",
    "        l= loss.forward(predictions, output_data_shuffled[b:b+batch_size].unsqueeze(-1))\n",
    "        acc_loss += l\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        net.backward(loss.backward())\n",
    "        \n",
    "        new_par = optimizer.step()\n",
    "        net.set_parameters(new_par)\n",
    "    \n",
    "    print(t, '   MSE loss = ' , acc_loss.item())\n",
    "    \n",
    "correct=0\n",
    "for i in range(output_data.shape[0]):\n",
    "        x=input_data[i]\n",
    "        y=2*output_data[i]-1\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = net.forward(x.unsqueeze(0))\n",
    "        if abs(y_pred-output_data[i])<1:\n",
    "            correct+=1\n",
    "print('Correct predictions after '+str(n)+' training steps: '+str(correct/N*100)+' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-radio",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "modules+sequential_notokyet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tirocinio",
   "language": "python",
   "name": "tirocinio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
