{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "thermal-humanity"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "#torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "therapeutic-current"
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward (self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_parameters( self ) :\n",
    "        return []   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "imperial-class",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Losses(object):        \n",
    "    def forward():\n",
    "        return NotImplementedError\n",
    "    def backward():\n",
    "        NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "comfortable-calgary"
   },
   "outputs": [],
   "source": [
    "class Optimizers(object):\n",
    "    # this is a SGD optimizer\n",
    "    def __init__(self,lr,max_iter, parameters) :  # should we add a \"tolerance_grad\" argument ? \n",
    "        super().__init__()\n",
    "        self.eta = lr\n",
    "        self.maxStep = max_iter # maybe this shouldn't be put inside the module\n",
    "        self.param = parameters\n",
    "        self.number_step = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for parameter in self.param : \n",
    "            parameter.grad = 0\n",
    "\n",
    "    def step(self): #batch de datapoint  --> confused : how can we do it stochastic ? ou alors on l'appelle step(batch)\n",
    "        # right now, eta is considered constant \n",
    "        #print(self.param[1].data)\n",
    "        #print('step')\n",
    "        if self.number_step <=self.maxStep:\n",
    "            for parameter in self.param :\n",
    "                #print(parameter)\n",
    "                #print(parameter.data[1])\n",
    "                parameter.data = parameter.data - self.eta * parameter.grad\n",
    "                #print(parameter.data[1])\n",
    "            self.number_step = self.number_step + 1\n",
    "            #print('after update',self.param[1].data)\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "found-annex"
   },
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    def __init__(self):\n",
    "        self.name = ''\n",
    "        self.data = None\n",
    "        self.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "asian-evanescence"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, input_dim, out_dim, bias = True):\n",
    "        super().__init__()\n",
    "        std = 1/math.sqrt(input_dim)\n",
    "        self.weight = Parameter()\n",
    "        self.parameters = []\n",
    "        \n",
    "        self.weight.data = torch.rand(out_dim, input_dim)\n",
    "        self.weight.data = 2*std*self.weight.data - std\n",
    "        self.weight.name = 'weight'\n",
    "        self.parameters += [self.weight]\n",
    "        \n",
    "        self.with_bias = bias\n",
    "        if bias :\n",
    "            self.bias = Parameter()\n",
    "            self.bias.data = torch.rand(out_dim)\n",
    "            self.bias.data = 2*std*self.bias.data - std\n",
    "            self.bias.name = 'bias'\n",
    "            self.parameters +=[self.bias]\n",
    "            \n",
    "        self.x = None\n",
    "              \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.weight.data.mv(x) + self.bias.data\n",
    "        \n",
    "    def backward(self, prev_grad):\n",
    "        \n",
    "        prev_grad = prev_grad.view(-1, 1)\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "        \n",
    "        if self.weight.grad is None:\n",
    "            self.weight.grad = torch.zeros_like(self.weight.data)\n",
    "        \n",
    "        self.weight.grad += prev_grad.view(-1, 1)*self.x.view(1, -1)\n",
    "        \n",
    "        if self.with_bias:\n",
    "            if self.bias.grad is None:\n",
    "                self.bias.grad = torch.zeros_like(self.bias.data)\n",
    "            self.bias.grad += prev_grad.view(-1)\n",
    "        \n",
    "        next_grad = prev_grad.view(1, -1)@self.weight.data\n",
    "        next_grad = next_grad.view(-1, 1)\n",
    "        return next_grad\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "written-benjamin",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def forward (self, x):\n",
    "        self.x = x\n",
    "        return torch.tanh(x)\n",
    "        \n",
    "    def backward ( self, prev_grad) :\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "            \n",
    "        def d(x):\n",
    "            return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)\n",
    "        \n",
    "        return d(self.x)*prev_grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "happy-review"
   },
   "outputs": [],
   "source": [
    "class MSE(Losses):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    def forward(self, x, t):\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        return (x - t).pow(2).mean()\n",
    "    \n",
    "    def backward(self):\n",
    "        if self.x == None or self.t == None:\n",
    "            raise CallForwardFirst\n",
    "        return 2 * (self.x - self.t)/len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "active-skirt"
   },
   "outputs": [],
   "source": [
    "class Sequential(object):\n",
    "    def __init__(self, modules):\n",
    "        super().__init__()\n",
    "        self.modules=modules\n",
    "        self.parameters = []\n",
    "        for m in self.modules:\n",
    "            param = m.get_parameters()\n",
    "            if param:\n",
    "                self.parameters += param\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for m in self.modules:\n",
    "            x=m.forward(x)\n",
    "        return x\n",
    "    \n",
    "#    def backward(self,weights,values,dl_dw,target):\n",
    "#        xn=values[-1]\n",
    "#        dl_dx=[]\n",
    "#        dl_dx.append(dloss(x_n,t))\n",
    "#        for i in range(len(self.modules)).reversed():\n",
    "#            m=self.modules[i]\n",
    "#            dl_dx.append(m.backwardoutput(dl_dx[-1],values[i]))#backward is implemented for each module\n",
    "#        for i in range(len(self.modules)).reversed():\n",
    "#            m=self.modules[i]\n",
    "#            dl_dw=m.backwardweights(dl_dw,dl_dx[i+1],values[i])\n",
    "    \n",
    "    def backward(self, loss_grad):\n",
    "        x = loss_grad\n",
    "        for m in reversed(self.modules):\n",
    "            x = m.backward(x)\n",
    "            \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "    def set_parameters(self , params):\n",
    "        #print(self.parameters[1].data)\n",
    "        self.parameters = params\n",
    "        #print('after',self.parameters[1].data)\n",
    "        #for i in range (len(new_par)):\n",
    "         #   self.parameters[i] = params[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "legal-buying"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(9, requires_grad = False)\n",
    "y = torch.randn(6,requires_grad = False)\n",
    "\n",
    "#handmade sequential linear + relu \n",
    "linear = Linear(9, 6, True)\n",
    "sigma = Tanh()\n",
    "loss = MSE()\n",
    "\n",
    "net = Sequential([\n",
    "    linear, \n",
    "    sigma\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "julian-memory",
    "outputId": "a5efb75a-b13b-42c9-cd38-58fc6a45b7aa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias\n",
      "weight\n"
     ]
    }
   ],
   "source": [
    "for param in reversed(net.get_parameters()):\n",
    "    print(param.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "arbitrary-customer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = net.forward(x)\n",
    "loss.forward(output, y)\n",
    "\n",
    "net.backward(loss.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "russian-pocket"
   },
   "outputs": [],
   "source": [
    "#comparing with builtin methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "agricultural-kidney"
   },
   "outputs": [],
   "source": [
    "b_linear = torch.nn.Linear(9, 6, True)\n",
    "b_linear.weight.data = linear.weight.data\n",
    "b_linear.bias.data = linear.bias.data\n",
    "l = torch.nn.MSELoss()(torch.tanh(b_linear(x)), y)\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "biological-savage",
    "outputId": "3e6ee187-e9c5-439d-b321-1ed0af701448",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3711, -0.8117, -0.3824,  1.8273,  0.5904,  0.8132, -0.2082, -0.7149,\n",
       "          -0.8794],\n",
       "         [-0.1280, -0.2799, -0.1319,  0.6302,  0.2036,  0.2804, -0.0718, -0.2466,\n",
       "          -0.3033],\n",
       "         [-0.2584, -0.5651, -0.2662,  1.2721,  0.4110,  0.5661, -0.1449, -0.4977,\n",
       "          -0.6122],\n",
       "         [ 0.1353,  0.2960,  0.1394, -0.6663, -0.2153, -0.2965,  0.0759,  0.2607,\n",
       "           0.3207],\n",
       "         [ 0.0082,  0.0178,  0.0084, -0.0402, -0.0130, -0.0179,  0.0046,  0.0157,\n",
       "           0.0193],\n",
       "         [-0.0287, -0.0629, -0.0296,  0.1415,  0.0457,  0.0630, -0.0161, -0.0554,\n",
       "          -0.0681]]),\n",
       " tensor([[-0.3711, -0.8117, -0.3824,  1.8273,  0.5904,  0.8132, -0.2082, -0.7149,\n",
       "          -0.8794],\n",
       "         [-0.1280, -0.2799, -0.1319,  0.6302,  0.2036,  0.2804, -0.0718, -0.2466,\n",
       "          -0.3033],\n",
       "         [-0.2584, -0.5651, -0.2662,  1.2721,  0.4110,  0.5661, -0.1449, -0.4977,\n",
       "          -0.6122],\n",
       "         [ 0.1353,  0.2960,  0.1394, -0.6663, -0.2153, -0.2965,  0.0759,  0.2607,\n",
       "           0.3207],\n",
       "         [ 0.0082,  0.0178,  0.0084, -0.0402, -0.0130, -0.0179,  0.0046,  0.0157,\n",
       "           0.0193],\n",
       "         [-0.0287, -0.0629, -0.0296,  0.1415,  0.0457,  0.0630, -0.0161, -0.0554,\n",
       "          -0.0681]]),\n",
       " tensor(2.3842e-07))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_linear.weight.grad, linear.weight.grad,  abs(b_linear.weight.grad - linear.weight.grad).max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exact-rotation",
    "outputId": "78a34f26-7048-4da4-a30a-68dbafb55519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.8492,  0.2929,  0.5911, -0.3096, -0.0187,  0.0658]),\n",
       " tensor([ 0.8492,  0.2929,  0.5911, -0.3096, -0.0187,  0.0658]),\n",
       " tensor(1.1921e-07))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_linear.bias.grad, linear.bias.grad,  abs(b_linear.bias.grad - linear.bias.grad).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e0HB0UJzdFw",
    "outputId": "ac09630f-db97-466d-e511-5334ec5b9fd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    MSE loss =  0.5442897081375122\n",
      "99    MSE loss =  0.5442832112312317\n",
      "198    MSE loss =  0.5442830920219421\n",
      "297    MSE loss =  0.5442830920219421\n",
      "396    MSE loss =  0.5442830920219421\n",
      "495    MSE loss =  0.5442830920219421\n",
      "594    MSE loss =  0.5442830920219421\n",
      "693    MSE loss =  0.5442830920219421\n",
      "792    MSE loss =  0.5442830920219421\n",
      "891    MSE loss =  0.5442830920219421\n",
      "990    MSE loss =  0.5442830920219421\n",
      "1089    MSE loss =  0.5442830920219421\n",
      "1188    MSE loss =  0.5442830920219421\n",
      "1287    MSE loss =  0.5442830920219421\n",
      "1386    MSE loss =  0.5442830920219421\n",
      "1485    MSE loss =  0.5442830920219421\n",
      "1584    MSE loss =  0.5442830920219421\n",
      "1683    MSE loss =  0.5442830920219421\n",
      "1782    MSE loss =  0.5442830920219421\n",
      "1881    MSE loss =  0.5442830920219421\n",
      "1980    MSE loss =  0.5442830920219421\n",
      "2079    MSE loss =  0.5442830920219421\n",
      "2178    MSE loss =  0.5442830920219421\n",
      "2277    MSE loss =  0.5442830920219421\n",
      "2376    MSE loss =  0.5442830920219421\n",
      "2475    MSE loss =  0.5442830920219421\n",
      "2574    MSE loss =  0.5442830920219421\n",
      "2673    MSE loss =  0.5442830920219421\n",
      "2772    MSE loss =  0.5442830920219421\n",
      "2871    MSE loss =  0.5442830920219421\n",
      "2970    MSE loss =  0.5442830920219421\n",
      "3069    MSE loss =  0.5442830920219421\n",
      "3168    MSE loss =  0.5442830920219421\n",
      "3267    MSE loss =  0.5442830920219421\n",
      "3366    MSE loss =  0.5442830920219421\n",
      "3465    MSE loss =  0.5442830920219421\n",
      "3564    MSE loss =  0.5442830920219421\n",
      "3663    MSE loss =  0.5442830920219421\n",
      "3762    MSE loss =  0.5442830920219421\n",
      "3861    MSE loss =  0.5442830920219421\n",
      "3960    MSE loss =  0.5442830920219421\n",
      "4059    MSE loss =  0.5442830920219421\n",
      "4158    MSE loss =  0.5442830920219421\n",
      "4257    MSE loss =  0.5442830920219421\n",
      "4356    MSE loss =  0.5442830920219421\n",
      "4455    MSE loss =  0.5442830920219421\n",
      "4554    MSE loss =  0.5442830920219421\n",
      "4653    MSE loss =  0.5442830920219421\n",
      "4752    MSE loss =  0.5442830920219421\n",
      "4851    MSE loss =  0.5442830920219421\n",
      "4950    MSE loss =  0.5442830920219421\n",
      "5049    MSE loss =  0.5442830920219421\n",
      "5148    MSE loss =  0.5442830920219421\n",
      "5247    MSE loss =  0.5442830920219421\n",
      "5346    MSE loss =  0.5442830920219421\n",
      "5445    MSE loss =  0.5442830920219421\n",
      "5544    MSE loss =  0.5442830920219421\n",
      "5643    MSE loss =  0.5442830920219421\n",
      "5742    MSE loss =  0.5442830920219421\n",
      "5841    MSE loss =  0.5442830920219421\n",
      "5940    MSE loss =  0.5442830920219421\n",
      "6039    MSE loss =  0.5442830920219421\n",
      "6138    MSE loss =  0.5442830920219421\n",
      "6237    MSE loss =  0.5442830920219421\n",
      "6336    MSE loss =  0.5442830920219421\n",
      "6435    MSE loss =  0.5442830920219421\n",
      "6534    MSE loss =  0.5442830920219421\n",
      "6633    MSE loss =  0.5442830920219421\n",
      "6732    MSE loss =  0.5442830920219421\n",
      "6831    MSE loss =  0.5442830920219421\n",
      "6930    MSE loss =  0.5442830920219421\n",
      "7029    MSE loss =  0.5442830920219421\n",
      "7128    MSE loss =  0.5442830920219421\n",
      "7227    MSE loss =  0.5442830920219421\n",
      "7326    MSE loss =  0.5442830920219421\n",
      "7425    MSE loss =  0.5442830920219421\n",
      "7524    MSE loss =  0.5442830920219421\n",
      "7623    MSE loss =  0.5442830920219421\n",
      "7722    MSE loss =  0.5442830920219421\n",
      "7821    MSE loss =  0.5442830920219421\n",
      "7920    MSE loss =  0.5442830920219421\n",
      "8019    MSE loss =  0.5442830920219421\n",
      "8118    MSE loss =  0.5442830920219421\n",
      "8217    MSE loss =  0.5442830920219421\n",
      "8316    MSE loss =  0.5442830920219421\n",
      "8415    MSE loss =  0.5442830920219421\n",
      "8514    MSE loss =  0.5442830920219421\n",
      "8613    MSE loss =  0.5442830920219421\n",
      "8712    MSE loss =  0.5442830920219421\n",
      "8811    MSE loss =  0.5442830920219421\n",
      "8910    MSE loss =  0.5442830920219421\n",
      "9009    MSE loss =  0.5442830920219421\n",
      "9108    MSE loss =  0.5442830920219421\n",
      "9207    MSE loss =  0.5442830920219421\n",
      "9306    MSE loss =  0.5442830920219421\n",
      "9405    MSE loss =  0.5442830920219421\n",
      "9504    MSE loss =  0.5442830920219421\n",
      "9603    MSE loss =  0.5442830920219421\n",
      "9702    MSE loss =  0.5442830920219421\n",
      "9801    MSE loss =  0.5442830920219421\n",
      "9900    MSE loss =  0.5442830920219421\n",
      "9999    MSE loss =  0.5442830920219421\n"
     ]
    }
   ],
   "source": [
    "# Declare model \n",
    "model = Sequential([\n",
    "    linear, \n",
    "    sigma\n",
    "])\n",
    "# Choose loss\n",
    "loss = MSE()\n",
    "\n",
    "optimizer = Optimizers(lr = 1e-4,max_iter = 100, parameters = model.get_parameters())\n",
    "\n",
    "for t in range(10**3):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model.forward(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    mse = loss.forward(y_pred, y)\n",
    "    if t%99==0:\n",
    "        print(t, '   MSE loss = ' , mse.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    model.backward(loss.backward())\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    new_par = optimizer.step()\n",
    "    #print(len(new_par))\n",
    "    model.set_parameters(new_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "modules+sequential_notokyet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
