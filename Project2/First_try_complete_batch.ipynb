{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supported-helena",
   "metadata": {
    "id": "thermal-humanity"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from dlc_practical_prologue import *\n",
    "#torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "english-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTERESTING FACT: WHEN COMPUTING THE BATCH GRADIENT PYTORCH DOES NOT TAKE THE SUM OF THE GRADIENTA OF SINGLE DATA POINTS\n",
    "#BUT THE MEAN. I LEART AT THE EXPENSE OF ABOUT 1H LOL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "searching-tract",
   "metadata": {
    "id": "found-annex"
   },
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    def __init__(self):\n",
    "        self.name = ''\n",
    "        self.data = None\n",
    "        self.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unlimited-samoa",
   "metadata": {
    "id": "therapeutic-current"
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward (self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_parameters( self ) :\n",
    "        return []   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "driving-grade",
   "metadata": {
    "id": "imperial-class",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Losses(object):        \n",
    "    def forward():\n",
    "        return NotImplementedError\n",
    "    def backward():\n",
    "        NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "historic-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def zero_grad(self):\n",
    "        for parameter in self.param : \n",
    "            parameter.grad = 0\n",
    "            \n",
    "    def step(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "egyptian-progress",
   "metadata": {
    "id": "comfortable-calgary"
   },
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    # this is a SGD optimizer\n",
    "    def __init__(self,lr,max_iter, parameters) :  \n",
    "        super().__init__()\n",
    "        self.eta = lr\n",
    "        self.maxStep = max_iter \n",
    "        self.param = parameters\n",
    "        self.number_step = 0\n",
    "\n",
    "    def step(self): \n",
    "        if self.number_step <=self.maxStep:\n",
    "            for parameter in self.param :\n",
    "                parameter.data = parameter.data - self.eta * parameter.grad\n",
    "            self.number_step = self.number_step + 1\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "external-evolution",
   "metadata": {
    "id": "asian-evanescence"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, input_dim, out_dim, bias = True):\n",
    "        super().__init__()\n",
    "        std = 1/math.sqrt(input_dim)\n",
    "        self.weight = Parameter()\n",
    "        self.parameters = []\n",
    "        \n",
    "        self.weight.data = torch.rand(out_dim, input_dim)\n",
    "        self.weight.data = 2*std*self.weight.data - std\n",
    "        self.weight.name = 'weight'\n",
    "        self.parameters += [self.weight]\n",
    "        \n",
    "        self.with_bias = bias\n",
    "        if bias :\n",
    "            self.bias = Parameter()\n",
    "            self.bias.data = torch.rand(out_dim)\n",
    "            self.bias.data = 2*std*self.bias.data - std\n",
    "            self.bias.data = self.bias.data.unsqueeze(0)\n",
    "            self.bias.name = 'bias'\n",
    "            self.parameters +=[self.bias]\n",
    "            \n",
    "        self.x = None\n",
    "              \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.batch_size = x.shape[0]\n",
    "        return self.x.mm(self.weight.data.T) + self.bias.data\n",
    "        \n",
    "    def backward(self, prev_grad):\n",
    "        \n",
    "        prev_grad = prev_grad.view(self.batch_size, -1, 1)\n",
    "        #print(prev_grad.shape)\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "        \n",
    "        if self.weight.grad is None:\n",
    "            self.weight.grad = torch.zeros_like(self.weight.data)\n",
    "        \n",
    "        grad_on_batch = prev_grad.view(self.batch_size, -1, 1)*self.x.view(self.batch_size, 1, -1)\n",
    "        self.weight.grad += grad_on_batch.mean(0)\n",
    "        \n",
    "        if self.with_bias:\n",
    "            if self.bias.grad is None:\n",
    "                self.bias.grad = torch.zeros_like(self.bias.data)\n",
    "            grad_on_batch = prev_grad.view(self.batch_size, -1)\n",
    "            self.bias.grad += grad_on_batch.mean(0)\n",
    "        \n",
    "        #if the output has dimension one, squeezing creates problems\n",
    "        if prev_grad.shape[1]>1:\n",
    "            prev_grad = prev_grad.squeeze()\n",
    "        next_grad = prev_grad@self.weight.data\n",
    "        return next_grad.squeeze()\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "secret-trader",
   "metadata": {
    "id": "written-benjamin",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def forward (self, x):\n",
    "        self.x = x\n",
    "        return torch.tanh(x)\n",
    "        \n",
    "    def backward ( self, prev_grad) :\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "            \n",
    "        def d(x):\n",
    "            return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)\n",
    "        \n",
    "        return d(self.x)*prev_grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spiritual-orchestra",
   "metadata": {
    "id": "happy-review"
   },
   "outputs": [],
   "source": [
    "class MSE(Losses):\n",
    "    # Attention! Works well only when the vectors provided are of the form [batch_size, vector dimension]\n",
    "    # Otherwise it doesn know what dimesion to pick for the mean computation\n",
    "    #I'll fix this later\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    def forward(self, x, t):\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        return (x - t).pow(2).mean()\n",
    "    \n",
    "    def backward(self):\n",
    "        if self.x == None or self.t == None:\n",
    "            raise CallForwardFirst\n",
    "        return 2 * (self.x - self.t)/self.x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emerging-karma",
   "metadata": {
    "id": "active-skirt"
   },
   "outputs": [],
   "source": [
    "class Sequential(object):\n",
    "    def __init__(self, modules):\n",
    "        super().__init__()\n",
    "        self.modules=modules\n",
    "        self.parameters = []\n",
    "        for m in self.modules:\n",
    "            param = m.get_parameters()\n",
    "            if param:\n",
    "                self.parameters += param\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for m in self.modules:\n",
    "            x=m.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, loss_grad):\n",
    "        x = loss_grad\n",
    "        for m in reversed(self.modules):\n",
    "            x = m.backward(x)\n",
    "            \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "    def set_parameters(self , params):\n",
    "        self.parameters = params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-million",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bacterial-cooperation",
   "metadata": {
    "id": "legal-buying"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x225f75b0a48>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_disc_set(nb):\n",
    "    input_ = torch.empty(nb, 2).uniform_(0, 1)\n",
    "    target = (input_-0.5).pow(2).sum(1).sub(1 / (math.pi*2)).sign().add(1).div(2).long()\n",
    "    return input_, target\n",
    "input_data, output_data=generate_disc_set(1000)\n",
    "output_data=1-output_data\n",
    "input_data-=input_data.mean(0)\n",
    "input_data/=input_data.std(0)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(input_data[:,0][output_data==1],input_data[:,1][output_data==1],'bo')\n",
    "plt.plot(input_data[:,0][output_data!=1],input_data[:,1][output_data!=1],'go')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "occasional-accident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0231,  0.9301],\n",
      "        [-1.4414, -1.2243],\n",
      "        [-0.6799,  0.4758],\n",
      "        ...,\n",
      "        [-0.9371,  1.0547],\n",
      "        [ 0.1064,  0.0881],\n",
      "        [-0.5383,  1.2975]]) torch.Size([1000, 2]) torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "#output_data = 2*output_data -1\n",
    "print(input_data, input_data.shape, output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "powered-miller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we should use one hot label embedding with MSELoss. Hence it is necessary to implement a Softmax\n",
    "#Maybe we should also implement CrossEntropyLoss\n",
    "#print(input_data.shape, output_data.shape)\n",
    "#output_data = convert_to_one_hot_labels(input_data, output_data)\n",
    "#print(input_data.shape, output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "northern-constitutional",
   "metadata": {
    "id": "legal-buying"
   },
   "outputs": [],
   "source": [
    "#handmade sequential linear + relu \n",
    "linear1 = Linear(2, 25, True)\n",
    "linear2 = Linear(25,25,True)\n",
    "linear3 = Linear(25,1,True)\n",
    "sigma1 = Tanh()\n",
    "sigma2 = Tanh()\n",
    "sigma3 = Tanh()\n",
    "loss = MSE()\n",
    "\n",
    "net = Sequential([\n",
    "    linear1, \n",
    "    sigma1 ,\n",
    "    linear2,\n",
    "    sigma2 ,\n",
    "    linear3,\n",
    "    sigma3 ,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "appropriate-ecology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    MSE loss =  964.0357666015625\n",
      "10    MSE loss =  918.4002075195312\n",
      "20    MSE loss =  860.0148315429688\n",
      "30    MSE loss =  781.876708984375\n",
      "40    MSE loss =  683.5006103515625\n",
      "50    MSE loss =  577.1246948242188\n",
      "60    MSE loss =  482.00128173828125\n",
      "70    MSE loss =  407.9869689941406\n",
      "80    MSE loss =  353.34814453125\n",
      "90    MSE loss =  312.87115478515625\n",
      "100    MSE loss =  282.1074523925781\n",
      "110    MSE loss =  279.4369812011719\n",
      "120    MSE loss =  279.4369812011719\n",
      "130    MSE loss =  279.4369812011719\n",
      "140    MSE loss =  279.4369812011719\n",
      "150    MSE loss =  279.4369812011719\n",
      "160    MSE loss =  279.4369812011719\n",
      "170    MSE loss =  279.4369812011719\n",
      "180    MSE loss =  279.4369812011719\n",
      "190    MSE loss =  279.4369812011719\n",
      "200    MSE loss =  279.4369812011719\n",
      "210    MSE loss =  279.4369812011719\n",
      "220    MSE loss =  279.4369812011719\n",
      "230    MSE loss =  279.4369812011719\n",
      "240    MSE loss =  279.4369812011719\n",
      "250    MSE loss =  279.4369812011719\n",
      "260    MSE loss =  279.4369812011719\n",
      "270    MSE loss =  279.4369812011719\n",
      "280    MSE loss =  279.4369812011719\n",
      "290    MSE loss =  279.4369812011719\n",
      "300    MSE loss =  279.4369812011719\n",
      "310    MSE loss =  279.4369812011719\n",
      "320    MSE loss =  279.4369812011719\n",
      "330    MSE loss =  279.4369812011719\n",
      "340    MSE loss =  279.4369812011719\n",
      "350    MSE loss =  279.4369812011719\n",
      "360    MSE loss =  279.4369812011719\n",
      "370    MSE loss =  279.4369812011719\n",
      "380    MSE loss =  279.4369812011719\n",
      "390    MSE loss =  279.4369812011719\n",
      "400    MSE loss =  279.4369812011719\n",
      "410    MSE loss =  279.4369812011719\n",
      "420    MSE loss =  279.4369812011719\n",
      "430    MSE loss =  279.4369812011719\n",
      "440    MSE loss =  279.4369812011719\n",
      "450    MSE loss =  279.4369812011719\n",
      "460    MSE loss =  279.4369812011719\n",
      "470    MSE loss =  279.4369812011719\n",
      "480    MSE loss =  279.4369812011719\n",
      "490    MSE loss =  279.4369812011719\n",
      "500    MSE loss =  279.4369812011719\n",
      "510    MSE loss =  279.4369812011719\n",
      "520    MSE loss =  279.4369812011719\n",
      "530    MSE loss =  279.4369812011719\n",
      "540    MSE loss =  279.4369812011719\n",
      "550    MSE loss =  279.4369812011719\n",
      "560    MSE loss =  279.4369812011719\n",
      "570    MSE loss =  279.4369812011719\n",
      "580    MSE loss =  279.4369812011719\n",
      "590    MSE loss =  279.4369812011719\n",
      "600    MSE loss =  279.4369812011719\n",
      "610    MSE loss =  279.4369812011719\n",
      "620    MSE loss =  279.4369812011719\n",
      "630    MSE loss =  279.4369812011719\n",
      "640    MSE loss =  279.4369812011719\n",
      "650    MSE loss =  279.4369812011719\n",
      "660    MSE loss =  279.4369812011719\n",
      "670    MSE loss =  279.4369812011719\n",
      "680    MSE loss =  279.4369812011719\n",
      "690    MSE loss =  279.4369812011719\n",
      "700    MSE loss =  279.4369812011719\n",
      "710    MSE loss =  279.4369812011719\n",
      "720    MSE loss =  279.4369812011719\n",
      "730    MSE loss =  279.4369812011719\n",
      "740    MSE loss =  279.4369812011719\n",
      "750    MSE loss =  279.4369812011719\n",
      "760    MSE loss =  279.4369812011719\n",
      "770    MSE loss =  279.4369812011719\n",
      "780    MSE loss =  279.4369812011719\n",
      "790    MSE loss =  279.4369812011719\n",
      "800    MSE loss =  279.4369812011719\n",
      "810    MSE loss =  279.4369812011719\n",
      "820    MSE loss =  279.4369812011719\n",
      "830    MSE loss =  279.4369812011719\n",
      "840    MSE loss =  279.4369812011719\n",
      "850    MSE loss =  279.4369812011719\n",
      "860    MSE loss =  279.4369812011719\n",
      "870    MSE loss =  279.4369812011719\n",
      "880    MSE loss =  279.4369812011719\n",
      "890    MSE loss =  279.4369812011719\n",
      "900    MSE loss =  279.4369812011719\n",
      "910    MSE loss =  279.4369812011719\n",
      "920    MSE loss =  279.4369812011719\n",
      "930    MSE loss =  279.4369812011719\n",
      "940    MSE loss =  279.4369812011719\n",
      "950    MSE loss =  279.4369812011719\n",
      "960    MSE loss =  279.4369812011719\n",
      "970    MSE loss =  279.4369812011719\n",
      "980    MSE loss =  279.4369812011719\n",
      "990    MSE loss =  279.4369812011719\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(lr = 1e-4,max_iter = 100, parameters = net.get_parameters())\n",
    "n=10**3\n",
    "N=output_data.shape[0]\n",
    "for t in range(n):\n",
    "    optimizer.zero_grad()\n",
    "    acc_loss=0\n",
    "    for i in range(N):\n",
    "        x=input_data[i]\n",
    "        y=2*output_data[i]-1\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = net.forward(x.unsqueeze(0))\n",
    "        # Compute and print loss.\n",
    "        acc_loss += loss.forward(y_pred,y.unsqueeze(0))\n",
    "        \n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        net.backward(loss.backward())\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its parameters\n",
    "    new_par = optimizer.step()\n",
    "    #print(len(new_par))\n",
    "    net.set_parameters(new_par)\n",
    "    \n",
    "    \n",
    "    if t%10==0:\n",
    "        print(t, '   MSE loss = ' , acc_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "automatic-zimbabwe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions after 1000 training steps: 97.2 %\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "for i in range(output_data.shape[0]):\n",
    "        x=input_data[i]\n",
    "        y=2*output_data[i]-1\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = net.forward(x.unsqueeze(0))\n",
    "        if abs(y_pred-output_data[i])<1:\n",
    "            correct+=1\n",
    "print('Correct predictions after '+str(n)+' training steps: '+str(correct/N*100)+' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "veterinary-governor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    MSE loss =  3.2218475341796875\n",
      "1    MSE loss =  3.1584525108337402\n",
      "2    MSE loss =  3.0968589782714844\n",
      "3    MSE loss =  3.0370242595672607\n",
      "4    MSE loss =  2.9789063930511475\n",
      "5    MSE loss =  2.9224634170532227\n",
      "6    MSE loss =  2.8676540851593018\n",
      "7    MSE loss =  2.814436435699463\n",
      "8    MSE loss =  2.762770652770996\n",
      "9    MSE loss =  2.712615966796875\n",
      "10    MSE loss =  2.681093215942383\n",
      "11    MSE loss =  2.6805968284606934\n",
      "12    MSE loss =  2.6805968284606934\n",
      "13    MSE loss =  2.6805968284606934\n",
      "14    MSE loss =  2.6805968284606934\n",
      "15    MSE loss =  2.6805968284606934\n",
      "16    MSE loss =  2.6805968284606934\n",
      "17    MSE loss =  2.6805968284606934\n",
      "18    MSE loss =  2.6805968284606934\n",
      "19    MSE loss =  2.6805968284606934\n",
      "20    MSE loss =  2.6805968284606934\n",
      "21    MSE loss =  2.6805968284606934\n",
      "22    MSE loss =  2.6805968284606934\n",
      "23    MSE loss =  2.6805968284606934\n",
      "24    MSE loss =  2.6805968284606934\n",
      "25    MSE loss =  2.6805968284606934\n",
      "26    MSE loss =  2.6805968284606934\n",
      "27    MSE loss =  2.6805968284606934\n",
      "28    MSE loss =  2.6805968284606934\n",
      "29    MSE loss =  2.6805968284606934\n",
      "30    MSE loss =  2.6805968284606934\n",
      "31    MSE loss =  2.6805968284606934\n",
      "32    MSE loss =  2.6805968284606934\n",
      "33    MSE loss =  2.6805968284606934\n",
      "34    MSE loss =  2.6805968284606934\n",
      "35    MSE loss =  2.6805968284606934\n",
      "36    MSE loss =  2.6805968284606934\n",
      "37    MSE loss =  2.6805968284606934\n",
      "38    MSE loss =  2.6805968284606934\n",
      "39    MSE loss =  2.6805968284606934\n",
      "40    MSE loss =  2.6805968284606934\n",
      "41    MSE loss =  2.6805968284606934\n",
      "42    MSE loss =  2.6805968284606934\n",
      "43    MSE loss =  2.6805968284606934\n",
      "44    MSE loss =  2.6805968284606934\n",
      "45    MSE loss =  2.6805968284606934\n",
      "46    MSE loss =  2.6805968284606934\n",
      "47    MSE loss =  2.6805968284606934\n",
      "48    MSE loss =  2.6805968284606934\n",
      "49    MSE loss =  2.6805968284606934\n",
      "50    MSE loss =  2.6805968284606934\n",
      "51    MSE loss =  2.6805968284606934\n",
      "52    MSE loss =  2.6805968284606934\n",
      "53    MSE loss =  2.6805968284606934\n",
      "54    MSE loss =  2.6805968284606934\n",
      "55    MSE loss =  2.6805968284606934\n",
      "56    MSE loss =  2.6805968284606934\n",
      "57    MSE loss =  2.6805968284606934\n",
      "58    MSE loss =  2.6805968284606934\n",
      "59    MSE loss =  2.6805968284606934\n",
      "60    MSE loss =  2.6805968284606934\n",
      "61    MSE loss =  2.6805968284606934\n",
      "62    MSE loss =  2.6805968284606934\n",
      "63    MSE loss =  2.6805968284606934\n",
      "64    MSE loss =  2.6805968284606934\n",
      "65    MSE loss =  2.6805968284606934\n",
      "66    MSE loss =  2.6805968284606934\n",
      "67    MSE loss =  2.6805968284606934\n",
      "68    MSE loss =  2.6805968284606934\n",
      "69    MSE loss =  2.6805968284606934\n",
      "70    MSE loss =  2.6805968284606934\n",
      "71    MSE loss =  2.6805968284606934\n",
      "72    MSE loss =  2.6805968284606934\n",
      "73    MSE loss =  2.6805968284606934\n",
      "74    MSE loss =  2.6805968284606934\n",
      "75    MSE loss =  2.6805968284606934\n",
      "76    MSE loss =  2.6805968284606934\n",
      "77    MSE loss =  2.6805968284606934\n",
      "78    MSE loss =  2.6805968284606934\n",
      "79    MSE loss =  2.6805968284606934\n",
      "80    MSE loss =  2.6805968284606934\n",
      "81    MSE loss =  2.6805968284606934\n",
      "82    MSE loss =  2.6805968284606934\n",
      "83    MSE loss =  2.6805968284606934\n",
      "84    MSE loss =  2.6805968284606934\n",
      "85    MSE loss =  2.6805968284606934\n",
      "86    MSE loss =  2.6805968284606934\n",
      "87    MSE loss =  2.6805968284606934\n",
      "88    MSE loss =  2.6805968284606934\n",
      "89    MSE loss =  2.6805968284606934\n",
      "90    MSE loss =  2.6805968284606934\n",
      "91    MSE loss =  2.6805968284606934\n",
      "92    MSE loss =  2.6805968284606934\n",
      "93    MSE loss =  2.6805968284606934\n",
      "94    MSE loss =  2.6805968284606934\n",
      "95    MSE loss =  2.6805968284606934\n",
      "96    MSE loss =  2.6805968284606934\n",
      "97    MSE loss =  2.6805968284606934\n",
      "98    MSE loss =  2.6805968284606934\n",
      "99    MSE loss =  2.6805968284606934\n",
      "100    MSE loss =  2.6805968284606934\n",
      "101    MSE loss =  2.6805968284606934\n",
      "102    MSE loss =  2.6805968284606934\n",
      "103    MSE loss =  2.6805968284606934\n",
      "104    MSE loss =  2.6805968284606934\n",
      "105    MSE loss =  2.6805968284606934\n",
      "106    MSE loss =  2.6805968284606934\n",
      "107    MSE loss =  2.6805968284606934\n",
      "108    MSE loss =  2.6805968284606934\n",
      "109    MSE loss =  2.6805968284606934\n",
      "110    MSE loss =  2.6805968284606934\n",
      "111    MSE loss =  2.6805968284606934\n",
      "112    MSE loss =  2.6805968284606934\n",
      "113    MSE loss =  2.6805968284606934\n",
      "114    MSE loss =  2.6805968284606934\n",
      "115    MSE loss =  2.6805968284606934\n",
      "116    MSE loss =  2.6805968284606934\n",
      "117    MSE loss =  2.6805968284606934\n",
      "118    MSE loss =  2.6805968284606934\n",
      "119    MSE loss =  2.6805968284606934\n",
      "120    MSE loss =  2.6805968284606934\n",
      "121    MSE loss =  2.6805968284606934\n",
      "122    MSE loss =  2.6805968284606934\n",
      "123    MSE loss =  2.6805968284606934\n",
      "124    MSE loss =  2.6805968284606934\n",
      "125    MSE loss =  2.6805968284606934\n",
      "126    MSE loss =  2.6805968284606934\n",
      "127    MSE loss =  2.6805968284606934\n",
      "128    MSE loss =  2.6805968284606934\n",
      "129    MSE loss =  2.6805968284606934\n",
      "130    MSE loss =  2.6805968284606934\n",
      "131    MSE loss =  2.6805968284606934\n",
      "132    MSE loss =  2.6805968284606934\n",
      "133    MSE loss =  2.6805968284606934\n",
      "134    MSE loss =  2.6805968284606934\n",
      "135    MSE loss =  2.6805968284606934\n",
      "136    MSE loss =  2.6805968284606934\n",
      "137    MSE loss =  2.6805968284606934\n",
      "138    MSE loss =  2.6805968284606934\n",
      "139    MSE loss =  2.6805968284606934\n",
      "140    MSE loss =  2.6805968284606934\n",
      "141    MSE loss =  2.6805968284606934\n",
      "142    MSE loss =  2.6805968284606934\n",
      "143    MSE loss =  2.6805968284606934\n",
      "144    MSE loss =  2.6805968284606934\n",
      "145    MSE loss =  2.6805968284606934\n",
      "146    MSE loss =  2.6805968284606934\n",
      "147    MSE loss =  2.6805968284606934\n",
      "148    MSE loss =  2.6805968284606934\n",
      "149    MSE loss =  2.6805968284606934\n",
      "150    MSE loss =  2.6805968284606934\n",
      "151    MSE loss =  2.6805968284606934\n",
      "152    MSE loss =  2.6805968284606934\n",
      "153    MSE loss =  2.6805968284606934\n",
      "154    MSE loss =  2.6805968284606934\n",
      "155    MSE loss =  2.6805968284606934\n",
      "156    MSE loss =  2.6805968284606934\n",
      "157    MSE loss =  2.6805968284606934\n",
      "158    MSE loss =  2.6805968284606934\n",
      "159    MSE loss =  2.6805968284606934\n",
      "160    MSE loss =  2.6805968284606934\n",
      "161    MSE loss =  2.6805968284606934\n",
      "162    MSE loss =  2.6805968284606934\n",
      "163    MSE loss =  2.6805968284606934\n",
      "164    MSE loss =  2.6805968284606934\n",
      "165    MSE loss =  2.6805968284606934\n",
      "166    MSE loss =  2.6805968284606934\n",
      "167    MSE loss =  2.6805968284606934\n",
      "168    MSE loss =  2.6805968284606934\n",
      "169    MSE loss =  2.6805968284606934\n",
      "170    MSE loss =  2.6805968284606934\n",
      "171    MSE loss =  2.6805968284606934\n",
      "172    MSE loss =  2.6805968284606934\n",
      "173    MSE loss =  2.6805968284606934\n",
      "174    MSE loss =  2.6805968284606934\n",
      "175    MSE loss =  2.6805968284606934\n",
      "176    MSE loss =  2.6805968284606934\n",
      "177    MSE loss =  2.6805968284606934\n",
      "178    MSE loss =  2.6805968284606934\n",
      "179    MSE loss =  2.6805968284606934\n",
      "180    MSE loss =  2.6805968284606934\n",
      "181    MSE loss =  2.6805968284606934\n",
      "182    MSE loss =  2.6805968284606934\n",
      "183    MSE loss =  2.6805968284606934\n",
      "184    MSE loss =  2.6805968284606934\n",
      "185    MSE loss =  2.6805968284606934\n",
      "186    MSE loss =  2.6805968284606934\n",
      "187    MSE loss =  2.6805968284606934\n",
      "188    MSE loss =  2.6805968284606934\n",
      "189    MSE loss =  2.6805968284606934\n",
      "190    MSE loss =  2.6805968284606934\n",
      "191    MSE loss =  2.6805968284606934\n",
      "192    MSE loss =  2.6805968284606934\n",
      "193    MSE loss =  2.6805968284606934\n",
      "194    MSE loss =  2.6805968284606934\n",
      "195    MSE loss =  2.6805968284606934\n",
      "196    MSE loss =  2.6805968284606934\n",
      "197    MSE loss =  2.6805968284606934\n",
      "198    MSE loss =  2.6805968284606934\n",
      "199    MSE loss =  2.6805968284606934\n",
      "200    MSE loss =  2.6805968284606934\n",
      "201    MSE loss =  2.6805968284606934\n",
      "202    MSE loss =  2.6805968284606934\n",
      "203    MSE loss =  2.6805968284606934\n",
      "204    MSE loss =  2.6805968284606934\n",
      "205    MSE loss =  2.6805968284606934\n",
      "206    MSE loss =  2.6805968284606934\n",
      "207    MSE loss =  2.6805968284606934\n",
      "208    MSE loss =  2.6805968284606934\n",
      "209    MSE loss =  2.6805968284606934\n",
      "210    MSE loss =  2.6805968284606934\n",
      "211    MSE loss =  2.6805968284606934\n",
      "212    MSE loss =  2.6805968284606934\n",
      "213    MSE loss =  2.6805968284606934\n",
      "214    MSE loss =  2.6805968284606934\n",
      "215    MSE loss =  2.6805968284606934\n",
      "216    MSE loss =  2.6805968284606934\n",
      "217    MSE loss =  2.6805968284606934\n",
      "218    MSE loss =  2.6805968284606934\n",
      "219    MSE loss =  2.6805968284606934\n",
      "220    MSE loss =  2.6805968284606934\n",
      "221    MSE loss =  2.6805968284606934\n",
      "222    MSE loss =  2.6805968284606934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223    MSE loss =  2.6805968284606934\n",
      "224    MSE loss =  2.6805968284606934\n",
      "225    MSE loss =  2.6805968284606934\n",
      "226    MSE loss =  2.6805968284606934\n",
      "227    MSE loss =  2.6805968284606934\n",
      "228    MSE loss =  2.6805968284606934\n",
      "229    MSE loss =  2.6805968284606934\n",
      "230    MSE loss =  2.6805968284606934\n",
      "231    MSE loss =  2.6805968284606934\n",
      "232    MSE loss =  2.6805968284606934\n",
      "233    MSE loss =  2.6805968284606934\n",
      "234    MSE loss =  2.6805968284606934\n",
      "235    MSE loss =  2.6805968284606934\n",
      "236    MSE loss =  2.6805968284606934\n",
      "237    MSE loss =  2.6805968284606934\n",
      "238    MSE loss =  2.6805968284606934\n",
      "239    MSE loss =  2.6805968284606934\n",
      "240    MSE loss =  2.6805968284606934\n",
      "241    MSE loss =  2.6805968284606934\n",
      "242    MSE loss =  2.6805968284606934\n",
      "243    MSE loss =  2.6805968284606934\n",
      "244    MSE loss =  2.6805968284606934\n",
      "245    MSE loss =  2.6805968284606934\n",
      "246    MSE loss =  2.6805968284606934\n",
      "247    MSE loss =  2.6805968284606934\n",
      "248    MSE loss =  2.6805968284606934\n",
      "249    MSE loss =  2.6805968284606934\n",
      "250    MSE loss =  2.6805968284606934\n",
      "251    MSE loss =  2.6805968284606934\n",
      "252    MSE loss =  2.6805968284606934\n",
      "253    MSE loss =  2.6805968284606934\n",
      "254    MSE loss =  2.6805968284606934\n",
      "255    MSE loss =  2.6805968284606934\n",
      "256    MSE loss =  2.6805968284606934\n",
      "257    MSE loss =  2.6805968284606934\n",
      "258    MSE loss =  2.6805968284606934\n",
      "259    MSE loss =  2.6805968284606934\n",
      "260    MSE loss =  2.6805968284606934\n",
      "261    MSE loss =  2.6805968284606934\n",
      "262    MSE loss =  2.6805968284606934\n",
      "263    MSE loss =  2.6805968284606934\n",
      "264    MSE loss =  2.6805968284606934\n",
      "265    MSE loss =  2.6805968284606934\n",
      "266    MSE loss =  2.6805968284606934\n",
      "267    MSE loss =  2.6805968284606934\n",
      "268    MSE loss =  2.6805968284606934\n",
      "269    MSE loss =  2.6805968284606934\n",
      "270    MSE loss =  2.6805968284606934\n",
      "271    MSE loss =  2.6805968284606934\n",
      "272    MSE loss =  2.6805968284606934\n",
      "273    MSE loss =  2.6805968284606934\n",
      "274    MSE loss =  2.6805968284606934\n",
      "275    MSE loss =  2.6805968284606934\n",
      "276    MSE loss =  2.6805968284606934\n",
      "277    MSE loss =  2.6805968284606934\n",
      "278    MSE loss =  2.6805968284606934\n",
      "279    MSE loss =  2.6805968284606934\n",
      "280    MSE loss =  2.6805968284606934\n",
      "281    MSE loss =  2.6805968284606934\n",
      "282    MSE loss =  2.6805968284606934\n",
      "283    MSE loss =  2.6805968284606934\n",
      "284    MSE loss =  2.6805968284606934\n",
      "285    MSE loss =  2.6805968284606934\n",
      "286    MSE loss =  2.6805968284606934\n",
      "287    MSE loss =  2.6805968284606934\n",
      "288    MSE loss =  2.6805968284606934\n",
      "289    MSE loss =  2.6805968284606934\n",
      "290    MSE loss =  2.6805968284606934\n",
      "291    MSE loss =  2.6805968284606934\n",
      "292    MSE loss =  2.6805968284606934\n",
      "293    MSE loss =  2.6805968284606934\n",
      "294    MSE loss =  2.6805968284606934\n",
      "295    MSE loss =  2.6805968284606934\n",
      "296    MSE loss =  2.6805968284606934\n",
      "297    MSE loss =  2.6805968284606934\n",
      "298    MSE loss =  2.6805968284606934\n",
      "299    MSE loss =  2.6805968284606934\n",
      "300    MSE loss =  2.6805968284606934\n",
      "301    MSE loss =  2.6805968284606934\n",
      "302    MSE loss =  2.6805968284606934\n",
      "303    MSE loss =  2.6805968284606934\n",
      "304    MSE loss =  2.6805968284606934\n",
      "305    MSE loss =  2.6805968284606934\n",
      "306    MSE loss =  2.6805968284606934\n",
      "307    MSE loss =  2.6805968284606934\n",
      "308    MSE loss =  2.6805968284606934\n",
      "309    MSE loss =  2.6805968284606934\n",
      "310    MSE loss =  2.6805968284606934\n",
      "311    MSE loss =  2.6805968284606934\n",
      "312    MSE loss =  2.6805968284606934\n",
      "313    MSE loss =  2.6805968284606934\n",
      "314    MSE loss =  2.6805968284606934\n",
      "315    MSE loss =  2.6805968284606934\n",
      "316    MSE loss =  2.6805968284606934\n",
      "317    MSE loss =  2.6805968284606934\n",
      "318    MSE loss =  2.6805968284606934\n",
      "319    MSE loss =  2.6805968284606934\n",
      "320    MSE loss =  2.6805968284606934\n",
      "321    MSE loss =  2.6805968284606934\n",
      "322    MSE loss =  2.6805968284606934\n",
      "323    MSE loss =  2.6805968284606934\n",
      "324    MSE loss =  2.6805968284606934\n",
      "325    MSE loss =  2.6805968284606934\n",
      "326    MSE loss =  2.6805968284606934\n",
      "327    MSE loss =  2.6805968284606934\n",
      "328    MSE loss =  2.6805968284606934\n",
      "329    MSE loss =  2.6805968284606934\n",
      "330    MSE loss =  2.6805968284606934\n",
      "331    MSE loss =  2.6805968284606934\n",
      "332    MSE loss =  2.6805968284606934\n",
      "333    MSE loss =  2.6805968284606934\n",
      "334    MSE loss =  2.6805968284606934\n",
      "335    MSE loss =  2.6805968284606934\n",
      "336    MSE loss =  2.6805968284606934\n",
      "337    MSE loss =  2.6805968284606934\n",
      "338    MSE loss =  2.6805968284606934\n",
      "339    MSE loss =  2.6805968284606934\n",
      "340    MSE loss =  2.6805968284606934\n",
      "341    MSE loss =  2.6805968284606934\n",
      "342    MSE loss =  2.6805968284606934\n",
      "343    MSE loss =  2.6805968284606934\n",
      "344    MSE loss =  2.6805968284606934\n",
      "345    MSE loss =  2.6805968284606934\n",
      "346    MSE loss =  2.6805968284606934\n",
      "347    MSE loss =  2.6805968284606934\n",
      "348    MSE loss =  2.6805968284606934\n",
      "349    MSE loss =  2.6805968284606934\n",
      "350    MSE loss =  2.6805968284606934\n",
      "351    MSE loss =  2.6805968284606934\n",
      "352    MSE loss =  2.6805968284606934\n",
      "353    MSE loss =  2.6805968284606934\n",
      "354    MSE loss =  2.6805968284606934\n",
      "355    MSE loss =  2.6805968284606934\n",
      "356    MSE loss =  2.6805968284606934\n",
      "357    MSE loss =  2.6805968284606934\n",
      "358    MSE loss =  2.6805968284606934\n",
      "359    MSE loss =  2.6805968284606934\n",
      "360    MSE loss =  2.6805968284606934\n",
      "361    MSE loss =  2.6805968284606934\n",
      "362    MSE loss =  2.6805968284606934\n",
      "363    MSE loss =  2.6805968284606934\n",
      "364    MSE loss =  2.6805968284606934\n",
      "365    MSE loss =  2.6805968284606934\n",
      "366    MSE loss =  2.6805968284606934\n",
      "367    MSE loss =  2.6805968284606934\n",
      "368    MSE loss =  2.6805968284606934\n",
      "369    MSE loss =  2.6805968284606934\n",
      "370    MSE loss =  2.6805968284606934\n",
      "371    MSE loss =  2.6805968284606934\n",
      "372    MSE loss =  2.6805968284606934\n",
      "373    MSE loss =  2.6805968284606934\n",
      "374    MSE loss =  2.6805968284606934\n",
      "375    MSE loss =  2.6805968284606934\n",
      "376    MSE loss =  2.6805968284606934\n",
      "377    MSE loss =  2.6805968284606934\n",
      "378    MSE loss =  2.6805968284606934\n",
      "379    MSE loss =  2.6805968284606934\n",
      "380    MSE loss =  2.6805968284606934\n",
      "381    MSE loss =  2.6805968284606934\n",
      "382    MSE loss =  2.6805968284606934\n",
      "383    MSE loss =  2.6805968284606934\n",
      "384    MSE loss =  2.6805968284606934\n",
      "385    MSE loss =  2.6805968284606934\n",
      "386    MSE loss =  2.6805968284606934\n",
      "387    MSE loss =  2.6805968284606934\n",
      "388    MSE loss =  2.6805968284606934\n",
      "389    MSE loss =  2.6805968284606934\n",
      "390    MSE loss =  2.6805968284606934\n",
      "391    MSE loss =  2.6805968284606934\n",
      "392    MSE loss =  2.6805968284606934\n",
      "393    MSE loss =  2.6805968284606934\n",
      "394    MSE loss =  2.6805968284606934\n",
      "395    MSE loss =  2.6805968284606934\n",
      "396    MSE loss =  2.6805968284606934\n",
      "397    MSE loss =  2.6805968284606934\n",
      "398    MSE loss =  2.6805968284606934\n",
      "399    MSE loss =  2.6805968284606934\n",
      "400    MSE loss =  2.6805968284606934\n",
      "401    MSE loss =  2.6805968284606934\n",
      "402    MSE loss =  2.6805968284606934\n",
      "403    MSE loss =  2.6805968284606934\n",
      "404    MSE loss =  2.6805968284606934\n",
      "405    MSE loss =  2.6805968284606934\n",
      "406    MSE loss =  2.6805968284606934\n",
      "407    MSE loss =  2.6805968284606934\n",
      "408    MSE loss =  2.6805968284606934\n",
      "409    MSE loss =  2.6805968284606934\n",
      "410    MSE loss =  2.6805968284606934\n",
      "411    MSE loss =  2.6805968284606934\n",
      "412    MSE loss =  2.6805968284606934\n",
      "413    MSE loss =  2.6805968284606934\n",
      "414    MSE loss =  2.6805968284606934\n",
      "415    MSE loss =  2.6805968284606934\n",
      "416    MSE loss =  2.6805968284606934\n",
      "417    MSE loss =  2.6805968284606934\n",
      "418    MSE loss =  2.6805968284606934\n",
      "419    MSE loss =  2.6805968284606934\n",
      "420    MSE loss =  2.6805968284606934\n",
      "421    MSE loss =  2.6805968284606934\n",
      "422    MSE loss =  2.6805968284606934\n",
      "423    MSE loss =  2.6805968284606934\n",
      "424    MSE loss =  2.6805968284606934\n",
      "425    MSE loss =  2.6805968284606934\n",
      "426    MSE loss =  2.6805968284606934\n",
      "427    MSE loss =  2.6805968284606934\n",
      "428    MSE loss =  2.6805968284606934\n",
      "429    MSE loss =  2.6805968284606934\n",
      "430    MSE loss =  2.6805968284606934\n",
      "431    MSE loss =  2.6805968284606934\n",
      "432    MSE loss =  2.6805968284606934\n",
      "433    MSE loss =  2.6805968284606934\n",
      "434    MSE loss =  2.6805968284606934\n",
      "435    MSE loss =  2.6805968284606934\n",
      "436    MSE loss =  2.6805968284606934\n",
      "437    MSE loss =  2.6805968284606934\n",
      "438    MSE loss =  2.6805968284606934\n",
      "439    MSE loss =  2.6805968284606934\n",
      "440    MSE loss =  2.6805968284606934\n",
      "441    MSE loss =  2.6805968284606934\n",
      "442    MSE loss =  2.6805968284606934\n",
      "443    MSE loss =  2.6805968284606934\n",
      "444    MSE loss =  2.6805968284606934\n",
      "445    MSE loss =  2.6805968284606934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446    MSE loss =  2.6805968284606934\n",
      "447    MSE loss =  2.6805968284606934\n",
      "448    MSE loss =  2.6805968284606934\n",
      "449    MSE loss =  2.6805968284606934\n",
      "450    MSE loss =  2.6805968284606934\n",
      "451    MSE loss =  2.6805968284606934\n",
      "452    MSE loss =  2.6805968284606934\n",
      "453    MSE loss =  2.6805968284606934\n",
      "454    MSE loss =  2.6805968284606934\n",
      "455    MSE loss =  2.6805968284606934\n",
      "456    MSE loss =  2.6805968284606934\n",
      "457    MSE loss =  2.6805968284606934\n",
      "458    MSE loss =  2.6805968284606934\n",
      "459    MSE loss =  2.6805968284606934\n",
      "460    MSE loss =  2.6805968284606934\n",
      "461    MSE loss =  2.6805968284606934\n",
      "462    MSE loss =  2.6805968284606934\n",
      "463    MSE loss =  2.6805968284606934\n",
      "464    MSE loss =  2.6805968284606934\n",
      "465    MSE loss =  2.6805968284606934\n",
      "466    MSE loss =  2.6805968284606934\n",
      "467    MSE loss =  2.6805968284606934\n",
      "468    MSE loss =  2.6805968284606934\n",
      "469    MSE loss =  2.6805968284606934\n",
      "470    MSE loss =  2.6805968284606934\n",
      "471    MSE loss =  2.6805968284606934\n",
      "472    MSE loss =  2.6805968284606934\n",
      "473    MSE loss =  2.6805968284606934\n",
      "474    MSE loss =  2.6805968284606934\n",
      "475    MSE loss =  2.6805968284606934\n",
      "476    MSE loss =  2.6805968284606934\n",
      "477    MSE loss =  2.6805968284606934\n",
      "478    MSE loss =  2.6805968284606934\n",
      "479    MSE loss =  2.6805968284606934\n",
      "480    MSE loss =  2.6805968284606934\n",
      "481    MSE loss =  2.6805968284606934\n",
      "482    MSE loss =  2.6805968284606934\n",
      "483    MSE loss =  2.6805968284606934\n",
      "484    MSE loss =  2.6805968284606934\n",
      "485    MSE loss =  2.6805968284606934\n",
      "486    MSE loss =  2.6805968284606934\n",
      "487    MSE loss =  2.6805968284606934\n",
      "488    MSE loss =  2.6805968284606934\n",
      "489    MSE loss =  2.6805968284606934\n",
      "490    MSE loss =  2.6805968284606934\n",
      "491    MSE loss =  2.6805968284606934\n",
      "492    MSE loss =  2.6805968284606934\n",
      "493    MSE loss =  2.6805968284606934\n",
      "494    MSE loss =  2.6805968284606934\n",
      "495    MSE loss =  2.6805968284606934\n",
      "496    MSE loss =  2.6805968284606934\n",
      "497    MSE loss =  2.6805968284606934\n",
      "498    MSE loss =  2.6805968284606934\n",
      "499    MSE loss =  2.6805968284606934\n",
      "500    MSE loss =  2.6805968284606934\n",
      "501    MSE loss =  2.6805968284606934\n",
      "502    MSE loss =  2.6805968284606934\n",
      "503    MSE loss =  2.6805968284606934\n",
      "504    MSE loss =  2.6805968284606934\n",
      "505    MSE loss =  2.6805968284606934\n",
      "506    MSE loss =  2.6805968284606934\n",
      "507    MSE loss =  2.6805968284606934\n",
      "508    MSE loss =  2.6805968284606934\n",
      "509    MSE loss =  2.6805968284606934\n",
      "510    MSE loss =  2.6805968284606934\n",
      "511    MSE loss =  2.6805968284606934\n",
      "512    MSE loss =  2.6805968284606934\n",
      "513    MSE loss =  2.6805968284606934\n",
      "514    MSE loss =  2.6805968284606934\n",
      "515    MSE loss =  2.6805968284606934\n",
      "516    MSE loss =  2.6805968284606934\n",
      "517    MSE loss =  2.6805968284606934\n",
      "518    MSE loss =  2.6805968284606934\n",
      "519    MSE loss =  2.6805968284606934\n",
      "520    MSE loss =  2.6805968284606934\n",
      "521    MSE loss =  2.6805968284606934\n",
      "522    MSE loss =  2.6805968284606934\n",
      "523    MSE loss =  2.6805968284606934\n",
      "524    MSE loss =  2.6805968284606934\n",
      "525    MSE loss =  2.6805968284606934\n",
      "526    MSE loss =  2.6805968284606934\n",
      "527    MSE loss =  2.6805968284606934\n",
      "528    MSE loss =  2.6805968284606934\n",
      "529    MSE loss =  2.6805968284606934\n",
      "530    MSE loss =  2.6805968284606934\n",
      "531    MSE loss =  2.6805968284606934\n",
      "532    MSE loss =  2.6805968284606934\n",
      "533    MSE loss =  2.6805968284606934\n",
      "534    MSE loss =  2.6805968284606934\n",
      "535    MSE loss =  2.6805968284606934\n",
      "536    MSE loss =  2.6805968284606934\n",
      "537    MSE loss =  2.6805968284606934\n",
      "538    MSE loss =  2.6805968284606934\n",
      "539    MSE loss =  2.6805968284606934\n",
      "540    MSE loss =  2.6805968284606934\n",
      "541    MSE loss =  2.6805968284606934\n",
      "542    MSE loss =  2.6805968284606934\n",
      "543    MSE loss =  2.6805968284606934\n",
      "544    MSE loss =  2.6805968284606934\n",
      "545    MSE loss =  2.6805968284606934\n",
      "546    MSE loss =  2.6805968284606934\n",
      "547    MSE loss =  2.6805968284606934\n",
      "548    MSE loss =  2.6805968284606934\n",
      "549    MSE loss =  2.6805968284606934\n",
      "550    MSE loss =  2.6805968284606934\n",
      "551    MSE loss =  2.6805968284606934\n",
      "552    MSE loss =  2.6805968284606934\n",
      "553    MSE loss =  2.6805968284606934\n",
      "554    MSE loss =  2.6805968284606934\n",
      "555    MSE loss =  2.6805968284606934\n",
      "556    MSE loss =  2.6805968284606934\n",
      "557    MSE loss =  2.6805968284606934\n",
      "558    MSE loss =  2.6805968284606934\n",
      "559    MSE loss =  2.6805968284606934\n",
      "560    MSE loss =  2.6805968284606934\n",
      "561    MSE loss =  2.6805968284606934\n",
      "562    MSE loss =  2.6805968284606934\n",
      "563    MSE loss =  2.6805968284606934\n",
      "564    MSE loss =  2.6805968284606934\n",
      "565    MSE loss =  2.6805968284606934\n",
      "566    MSE loss =  2.6805968284606934\n",
      "567    MSE loss =  2.6805968284606934\n",
      "568    MSE loss =  2.6805968284606934\n",
      "569    MSE loss =  2.6805968284606934\n",
      "570    MSE loss =  2.6805968284606934\n",
      "571    MSE loss =  2.6805968284606934\n",
      "572    MSE loss =  2.6805968284606934\n",
      "573    MSE loss =  2.6805968284606934\n",
      "574    MSE loss =  2.6805968284606934\n",
      "575    MSE loss =  2.6805968284606934\n",
      "576    MSE loss =  2.6805968284606934\n",
      "577    MSE loss =  2.6805968284606934\n",
      "578    MSE loss =  2.6805968284606934\n",
      "579    MSE loss =  2.6805968284606934\n",
      "580    MSE loss =  2.6805968284606934\n",
      "581    MSE loss =  2.6805968284606934\n",
      "582    MSE loss =  2.6805968284606934\n",
      "583    MSE loss =  2.6805968284606934\n",
      "584    MSE loss =  2.6805968284606934\n",
      "585    MSE loss =  2.6805968284606934\n",
      "586    MSE loss =  2.6805968284606934\n",
      "587    MSE loss =  2.6805968284606934\n",
      "588    MSE loss =  2.6805968284606934\n",
      "589    MSE loss =  2.6805968284606934\n",
      "590    MSE loss =  2.6805968284606934\n",
      "591    MSE loss =  2.6805968284606934\n",
      "592    MSE loss =  2.6805968284606934\n",
      "593    MSE loss =  2.6805968284606934\n",
      "594    MSE loss =  2.6805968284606934\n",
      "595    MSE loss =  2.6805968284606934\n",
      "596    MSE loss =  2.6805968284606934\n",
      "597    MSE loss =  2.6805968284606934\n",
      "598    MSE loss =  2.6805968284606934\n",
      "599    MSE loss =  2.6805968284606934\n",
      "600    MSE loss =  2.6805968284606934\n",
      "601    MSE loss =  2.6805968284606934\n",
      "602    MSE loss =  2.6805968284606934\n",
      "603    MSE loss =  2.6805968284606934\n",
      "604    MSE loss =  2.6805968284606934\n",
      "605    MSE loss =  2.6805968284606934\n",
      "606    MSE loss =  2.6805968284606934\n",
      "607    MSE loss =  2.6805968284606934\n",
      "608    MSE loss =  2.6805968284606934\n",
      "609    MSE loss =  2.6805968284606934\n",
      "610    MSE loss =  2.6805968284606934\n",
      "611    MSE loss =  2.6805968284606934\n",
      "612    MSE loss =  2.6805968284606934\n",
      "613    MSE loss =  2.6805968284606934\n",
      "614    MSE loss =  2.6805968284606934\n",
      "615    MSE loss =  2.6805968284606934\n",
      "616    MSE loss =  2.6805968284606934\n",
      "617    MSE loss =  2.6805968284606934\n",
      "618    MSE loss =  2.6805968284606934\n",
      "619    MSE loss =  2.6805968284606934\n",
      "620    MSE loss =  2.6805968284606934\n",
      "621    MSE loss =  2.6805968284606934\n",
      "622    MSE loss =  2.6805968284606934\n",
      "623    MSE loss =  2.6805968284606934\n",
      "624    MSE loss =  2.6805968284606934\n",
      "625    MSE loss =  2.6805968284606934\n",
      "626    MSE loss =  2.6805968284606934\n",
      "627    MSE loss =  2.6805968284606934\n",
      "628    MSE loss =  2.6805968284606934\n",
      "629    MSE loss =  2.6805968284606934\n",
      "630    MSE loss =  2.6805968284606934\n",
      "631    MSE loss =  2.6805968284606934\n",
      "632    MSE loss =  2.6805968284606934\n",
      "633    MSE loss =  2.6805968284606934\n",
      "634    MSE loss =  2.6805968284606934\n",
      "635    MSE loss =  2.6805968284606934\n",
      "636    MSE loss =  2.6805968284606934\n",
      "637    MSE loss =  2.6805968284606934\n",
      "638    MSE loss =  2.6805968284606934\n",
      "639    MSE loss =  2.6805968284606934\n",
      "640    MSE loss =  2.6805968284606934\n",
      "641    MSE loss =  2.6805968284606934\n",
      "642    MSE loss =  2.6805968284606934\n",
      "643    MSE loss =  2.6805968284606934\n",
      "644    MSE loss =  2.6805968284606934\n",
      "645    MSE loss =  2.6805968284606934\n",
      "646    MSE loss =  2.6805968284606934\n",
      "647    MSE loss =  2.6805968284606934\n",
      "648    MSE loss =  2.6805968284606934\n",
      "649    MSE loss =  2.6805968284606934\n",
      "650    MSE loss =  2.6805968284606934\n",
      "651    MSE loss =  2.6805968284606934\n",
      "652    MSE loss =  2.6805968284606934\n",
      "653    MSE loss =  2.6805968284606934\n",
      "654    MSE loss =  2.6805968284606934\n",
      "655    MSE loss =  2.6805968284606934\n",
      "656    MSE loss =  2.6805968284606934\n",
      "657    MSE loss =  2.6805968284606934\n",
      "658    MSE loss =  2.6805968284606934\n",
      "659    MSE loss =  2.6805968284606934\n",
      "660    MSE loss =  2.6805968284606934\n",
      "661    MSE loss =  2.6805968284606934\n",
      "662    MSE loss =  2.6805968284606934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "663    MSE loss =  2.6805968284606934\n",
      "664    MSE loss =  2.6805968284606934\n",
      "665    MSE loss =  2.6805968284606934\n",
      "666    MSE loss =  2.6805968284606934\n",
      "667    MSE loss =  2.6805968284606934\n",
      "668    MSE loss =  2.6805968284606934\n",
      "669    MSE loss =  2.6805968284606934\n",
      "670    MSE loss =  2.6805968284606934\n",
      "671    MSE loss =  2.6805968284606934\n",
      "672    MSE loss =  2.6805968284606934\n",
      "673    MSE loss =  2.6805968284606934\n",
      "674    MSE loss =  2.6805968284606934\n",
      "675    MSE loss =  2.6805968284606934\n",
      "676    MSE loss =  2.6805968284606934\n",
      "677    MSE loss =  2.6805968284606934\n",
      "678    MSE loss =  2.6805968284606934\n",
      "679    MSE loss =  2.6805968284606934\n",
      "680    MSE loss =  2.6805968284606934\n",
      "681    MSE loss =  2.6805968284606934\n",
      "682    MSE loss =  2.6805968284606934\n",
      "683    MSE loss =  2.6805968284606934\n",
      "684    MSE loss =  2.6805968284606934\n",
      "685    MSE loss =  2.6805968284606934\n",
      "686    MSE loss =  2.6805968284606934\n",
      "687    MSE loss =  2.6805968284606934\n",
      "688    MSE loss =  2.6805968284606934\n",
      "689    MSE loss =  2.6805968284606934\n",
      "690    MSE loss =  2.6805968284606934\n",
      "691    MSE loss =  2.6805968284606934\n",
      "692    MSE loss =  2.6805968284606934\n",
      "693    MSE loss =  2.6805968284606934\n",
      "694    MSE loss =  2.6805968284606934\n",
      "695    MSE loss =  2.6805968284606934\n",
      "696    MSE loss =  2.6805968284606934\n",
      "697    MSE loss =  2.6805968284606934\n",
      "698    MSE loss =  2.6805968284606934\n",
      "699    MSE loss =  2.6805968284606934\n",
      "700    MSE loss =  2.6805968284606934\n",
      "701    MSE loss =  2.6805968284606934\n",
      "702    MSE loss =  2.6805968284606934\n",
      "703    MSE loss =  2.6805968284606934\n",
      "704    MSE loss =  2.6805968284606934\n",
      "705    MSE loss =  2.6805968284606934\n",
      "706    MSE loss =  2.6805968284606934\n",
      "707    MSE loss =  2.6805968284606934\n",
      "708    MSE loss =  2.6805968284606934\n",
      "709    MSE loss =  2.6805968284606934\n",
      "710    MSE loss =  2.6805968284606934\n",
      "711    MSE loss =  2.6805968284606934\n",
      "712    MSE loss =  2.6805968284606934\n",
      "713    MSE loss =  2.6805968284606934\n",
      "714    MSE loss =  2.6805968284606934\n",
      "715    MSE loss =  2.6805968284606934\n",
      "716    MSE loss =  2.6805968284606934\n",
      "717    MSE loss =  2.6805968284606934\n",
      "718    MSE loss =  2.6805968284606934\n",
      "719    MSE loss =  2.6805968284606934\n",
      "720    MSE loss =  2.6805968284606934\n",
      "721    MSE loss =  2.6805968284606934\n",
      "722    MSE loss =  2.6805968284606934\n",
      "723    MSE loss =  2.6805968284606934\n",
      "724    MSE loss =  2.6805968284606934\n",
      "725    MSE loss =  2.6805968284606934\n",
      "726    MSE loss =  2.6805968284606934\n",
      "727    MSE loss =  2.6805968284606934\n",
      "728    MSE loss =  2.6805968284606934\n",
      "729    MSE loss =  2.6805968284606934\n",
      "730    MSE loss =  2.6805968284606934\n",
      "731    MSE loss =  2.6805968284606934\n",
      "732    MSE loss =  2.6805968284606934\n",
      "733    MSE loss =  2.6805968284606934\n",
      "734    MSE loss =  2.6805968284606934\n",
      "735    MSE loss =  2.6805968284606934\n",
      "736    MSE loss =  2.6805968284606934\n",
      "737    MSE loss =  2.6805968284606934\n",
      "738    MSE loss =  2.6805968284606934\n",
      "739    MSE loss =  2.6805968284606934\n",
      "740    MSE loss =  2.6805968284606934\n",
      "741    MSE loss =  2.6805968284606934\n",
      "742    MSE loss =  2.6805968284606934\n",
      "743    MSE loss =  2.6805968284606934\n",
      "744    MSE loss =  2.6805968284606934\n",
      "745    MSE loss =  2.6805968284606934\n",
      "746    MSE loss =  2.6805968284606934\n",
      "747    MSE loss =  2.6805968284606934\n",
      "748    MSE loss =  2.6805968284606934\n",
      "749    MSE loss =  2.6805968284606934\n",
      "750    MSE loss =  2.6805968284606934\n",
      "751    MSE loss =  2.6805968284606934\n",
      "752    MSE loss =  2.6805968284606934\n",
      "753    MSE loss =  2.6805968284606934\n",
      "754    MSE loss =  2.6805968284606934\n",
      "755    MSE loss =  2.6805968284606934\n",
      "756    MSE loss =  2.6805968284606934\n",
      "757    MSE loss =  2.6805968284606934\n",
      "758    MSE loss =  2.6805968284606934\n",
      "759    MSE loss =  2.6805968284606934\n",
      "760    MSE loss =  2.6805968284606934\n",
      "761    MSE loss =  2.6805968284606934\n",
      "762    MSE loss =  2.6805968284606934\n",
      "763    MSE loss =  2.6805968284606934\n",
      "764    MSE loss =  2.6805968284606934\n",
      "765    MSE loss =  2.6805968284606934\n",
      "766    MSE loss =  2.6805968284606934\n",
      "767    MSE loss =  2.6805968284606934\n",
      "768    MSE loss =  2.6805968284606934\n",
      "769    MSE loss =  2.6805968284606934\n",
      "770    MSE loss =  2.6805968284606934\n",
      "771    MSE loss =  2.6805968284606934\n",
      "772    MSE loss =  2.6805968284606934\n",
      "773    MSE loss =  2.6805968284606934\n",
      "774    MSE loss =  2.6805968284606934\n",
      "775    MSE loss =  2.6805968284606934\n",
      "776    MSE loss =  2.6805968284606934\n",
      "777    MSE loss =  2.6805968284606934\n",
      "778    MSE loss =  2.6805968284606934\n",
      "779    MSE loss =  2.6805968284606934\n",
      "780    MSE loss =  2.6805968284606934\n",
      "781    MSE loss =  2.6805968284606934\n",
      "782    MSE loss =  2.6805968284606934\n",
      "783    MSE loss =  2.6805968284606934\n",
      "784    MSE loss =  2.6805968284606934\n",
      "785    MSE loss =  2.6805968284606934\n",
      "786    MSE loss =  2.6805968284606934\n",
      "787    MSE loss =  2.6805968284606934\n",
      "788    MSE loss =  2.6805968284606934\n",
      "789    MSE loss =  2.6805968284606934\n",
      "790    MSE loss =  2.6805968284606934\n",
      "791    MSE loss =  2.6805968284606934\n",
      "792    MSE loss =  2.6805968284606934\n",
      "793    MSE loss =  2.6805968284606934\n",
      "794    MSE loss =  2.6805968284606934\n",
      "795    MSE loss =  2.6805968284606934\n",
      "796    MSE loss =  2.6805968284606934\n",
      "797    MSE loss =  2.6805968284606934\n",
      "798    MSE loss =  2.6805968284606934\n",
      "799    MSE loss =  2.6805968284606934\n",
      "800    MSE loss =  2.6805968284606934\n",
      "801    MSE loss =  2.6805968284606934\n",
      "802    MSE loss =  2.6805968284606934\n",
      "803    MSE loss =  2.6805968284606934\n",
      "804    MSE loss =  2.6805968284606934\n",
      "805    MSE loss =  2.6805968284606934\n",
      "806    MSE loss =  2.6805968284606934\n",
      "807    MSE loss =  2.6805968284606934\n",
      "808    MSE loss =  2.6805968284606934\n",
      "809    MSE loss =  2.6805968284606934\n",
      "810    MSE loss =  2.6805968284606934\n",
      "811    MSE loss =  2.6805968284606934\n",
      "812    MSE loss =  2.6805968284606934\n",
      "813    MSE loss =  2.6805968284606934\n",
      "814    MSE loss =  2.6805968284606934\n",
      "815    MSE loss =  2.6805968284606934\n",
      "816    MSE loss =  2.6805968284606934\n",
      "817    MSE loss =  2.6805968284606934\n",
      "818    MSE loss =  2.6805968284606934\n",
      "819    MSE loss =  2.6805968284606934\n",
      "820    MSE loss =  2.6805968284606934\n",
      "821    MSE loss =  2.6805968284606934\n",
      "822    MSE loss =  2.6805968284606934\n",
      "823    MSE loss =  2.6805968284606934\n",
      "824    MSE loss =  2.6805968284606934\n",
      "825    MSE loss =  2.6805968284606934\n",
      "826    MSE loss =  2.6805968284606934\n",
      "827    MSE loss =  2.6805968284606934\n",
      "828    MSE loss =  2.6805968284606934\n",
      "829    MSE loss =  2.6805968284606934\n",
      "830    MSE loss =  2.6805968284606934\n",
      "831    MSE loss =  2.6805968284606934\n",
      "832    MSE loss =  2.6805968284606934\n",
      "833    MSE loss =  2.6805968284606934\n",
      "834    MSE loss =  2.6805968284606934\n",
      "835    MSE loss =  2.6805968284606934\n",
      "836    MSE loss =  2.6805968284606934\n",
      "837    MSE loss =  2.6805968284606934\n",
      "838    MSE loss =  2.6805968284606934\n",
      "839    MSE loss =  2.6805968284606934\n",
      "840    MSE loss =  2.6805968284606934\n",
      "841    MSE loss =  2.6805968284606934\n",
      "842    MSE loss =  2.6805968284606934\n",
      "843    MSE loss =  2.6805968284606934\n",
      "844    MSE loss =  2.6805968284606934\n",
      "845    MSE loss =  2.6805968284606934\n",
      "846    MSE loss =  2.6805968284606934\n",
      "847    MSE loss =  2.6805968284606934\n",
      "848    MSE loss =  2.6805968284606934\n",
      "849    MSE loss =  2.6805968284606934\n",
      "850    MSE loss =  2.6805968284606934\n",
      "851    MSE loss =  2.6805968284606934\n",
      "852    MSE loss =  2.6805968284606934\n",
      "853    MSE loss =  2.6805968284606934\n",
      "854    MSE loss =  2.6805968284606934\n",
      "855    MSE loss =  2.6805968284606934\n",
      "856    MSE loss =  2.6805968284606934\n",
      "857    MSE loss =  2.6805968284606934\n",
      "858    MSE loss =  2.6805968284606934\n",
      "859    MSE loss =  2.6805968284606934\n",
      "860    MSE loss =  2.6805968284606934\n",
      "861    MSE loss =  2.6805968284606934\n",
      "862    MSE loss =  2.6805968284606934\n",
      "863    MSE loss =  2.6805968284606934\n",
      "864    MSE loss =  2.6805968284606934\n",
      "865    MSE loss =  2.6805968284606934\n",
      "866    MSE loss =  2.6805968284606934\n",
      "867    MSE loss =  2.6805968284606934\n",
      "868    MSE loss =  2.6805968284606934\n",
      "869    MSE loss =  2.6805968284606934\n",
      "870    MSE loss =  2.6805968284606934\n",
      "871    MSE loss =  2.6805968284606934\n",
      "872    MSE loss =  2.6805968284606934\n",
      "873    MSE loss =  2.6805968284606934\n",
      "874    MSE loss =  2.6805968284606934\n",
      "875    MSE loss =  2.6805968284606934\n",
      "876    MSE loss =  2.6805968284606934\n",
      "877    MSE loss =  2.6805968284606934\n",
      "878    MSE loss =  2.6805968284606934\n",
      "879    MSE loss =  2.6805968284606934\n",
      "880    MSE loss =  2.6805968284606934\n",
      "881    MSE loss =  2.6805968284606934\n",
      "882    MSE loss =  2.6805968284606934\n",
      "883    MSE loss =  2.6805968284606934\n",
      "884    MSE loss =  2.6805968284606934\n",
      "885    MSE loss =  2.6805968284606934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886    MSE loss =  2.6805968284606934\n",
      "887    MSE loss =  2.6805968284606934\n",
      "888    MSE loss =  2.6805968284606934\n",
      "889    MSE loss =  2.6805968284606934\n",
      "890    MSE loss =  2.6805968284606934\n",
      "891    MSE loss =  2.6805968284606934\n",
      "892    MSE loss =  2.6805968284606934\n",
      "893    MSE loss =  2.6805968284606934\n",
      "894    MSE loss =  2.6805968284606934\n",
      "895    MSE loss =  2.6805968284606934\n",
      "896    MSE loss =  2.6805968284606934\n",
      "897    MSE loss =  2.6805968284606934\n",
      "898    MSE loss =  2.6805968284606934\n",
      "899    MSE loss =  2.6805968284606934\n",
      "900    MSE loss =  2.6805968284606934\n",
      "901    MSE loss =  2.6805968284606934\n",
      "902    MSE loss =  2.6805968284606934\n",
      "903    MSE loss =  2.6805968284606934\n",
      "904    MSE loss =  2.6805968284606934\n",
      "905    MSE loss =  2.6805968284606934\n",
      "906    MSE loss =  2.6805968284606934\n",
      "907    MSE loss =  2.6805968284606934\n",
      "908    MSE loss =  2.6805968284606934\n",
      "909    MSE loss =  2.6805968284606934\n",
      "910    MSE loss =  2.6805968284606934\n",
      "911    MSE loss =  2.6805968284606934\n",
      "912    MSE loss =  2.6805968284606934\n",
      "913    MSE loss =  2.6805968284606934\n",
      "914    MSE loss =  2.6805968284606934\n",
      "915    MSE loss =  2.6805968284606934\n",
      "916    MSE loss =  2.6805968284606934\n",
      "917    MSE loss =  2.6805968284606934\n",
      "918    MSE loss =  2.6805968284606934\n",
      "919    MSE loss =  2.6805968284606934\n",
      "920    MSE loss =  2.6805968284606934\n",
      "921    MSE loss =  2.6805968284606934\n",
      "922    MSE loss =  2.6805968284606934\n",
      "923    MSE loss =  2.6805968284606934\n",
      "924    MSE loss =  2.6805968284606934\n",
      "925    MSE loss =  2.6805968284606934\n",
      "926    MSE loss =  2.6805968284606934\n",
      "927    MSE loss =  2.6805968284606934\n",
      "928    MSE loss =  2.6805968284606934\n",
      "929    MSE loss =  2.6805968284606934\n",
      "930    MSE loss =  2.6805968284606934\n",
      "931    MSE loss =  2.6805968284606934\n",
      "932    MSE loss =  2.6805968284606934\n",
      "933    MSE loss =  2.6805968284606934\n",
      "934    MSE loss =  2.6805968284606934\n",
      "935    MSE loss =  2.6805968284606934\n",
      "936    MSE loss =  2.6805968284606934\n",
      "937    MSE loss =  2.6805968284606934\n",
      "938    MSE loss =  2.6805968284606934\n",
      "939    MSE loss =  2.6805968284606934\n",
      "940    MSE loss =  2.6805968284606934\n",
      "941    MSE loss =  2.6805968284606934\n",
      "942    MSE loss =  2.6805968284606934\n",
      "943    MSE loss =  2.6805968284606934\n",
      "944    MSE loss =  2.6805968284606934\n",
      "945    MSE loss =  2.6805968284606934\n",
      "946    MSE loss =  2.6805968284606934\n",
      "947    MSE loss =  2.6805968284606934\n",
      "948    MSE loss =  2.6805968284606934\n",
      "949    MSE loss =  2.6805968284606934\n",
      "950    MSE loss =  2.6805968284606934\n",
      "951    MSE loss =  2.6805968284606934\n",
      "952    MSE loss =  2.6805968284606934\n",
      "953    MSE loss =  2.6805968284606934\n",
      "954    MSE loss =  2.6805968284606934\n",
      "955    MSE loss =  2.6805968284606934\n",
      "956    MSE loss =  2.6805968284606934\n",
      "957    MSE loss =  2.6805968284606934\n",
      "958    MSE loss =  2.6805968284606934\n",
      "959    MSE loss =  2.6805968284606934\n",
      "960    MSE loss =  2.6805968284606934\n",
      "961    MSE loss =  2.6805968284606934\n",
      "962    MSE loss =  2.6805968284606934\n",
      "963    MSE loss =  2.6805968284606934\n",
      "964    MSE loss =  2.6805968284606934\n",
      "965    MSE loss =  2.6805968284606934\n",
      "966    MSE loss =  2.6805968284606934\n",
      "967    MSE loss =  2.6805968284606934\n",
      "968    MSE loss =  2.6805968284606934\n",
      "969    MSE loss =  2.6805968284606934\n",
      "970    MSE loss =  2.6805968284606934\n",
      "971    MSE loss =  2.6805968284606934\n",
      "972    MSE loss =  2.6805968284606934\n",
      "973    MSE loss =  2.6805968284606934\n",
      "974    MSE loss =  2.6805968284606934\n",
      "975    MSE loss =  2.6805968284606934\n",
      "976    MSE loss =  2.6805968284606934\n",
      "977    MSE loss =  2.6805968284606934\n",
      "978    MSE loss =  2.6805968284606934\n",
      "979    MSE loss =  2.6805968284606934\n",
      "980    MSE loss =  2.6805968284606934\n",
      "981    MSE loss =  2.6805968284606934\n",
      "982    MSE loss =  2.6805968284606934\n",
      "983    MSE loss =  2.6805968284606934\n",
      "984    MSE loss =  2.6805968284606934\n",
      "985    MSE loss =  2.6805968284606934\n",
      "986    MSE loss =  2.6805968284606934\n",
      "987    MSE loss =  2.6805968284606934\n",
      "988    MSE loss =  2.6805968284606934\n",
      "989    MSE loss =  2.6805968284606934\n",
      "990    MSE loss =  2.6805968284606934\n",
      "991    MSE loss =  2.6805968284606934\n",
      "992    MSE loss =  2.6805968284606934\n",
      "993    MSE loss =  2.6805968284606934\n",
      "994    MSE loss =  2.6805968284606934\n",
      "995    MSE loss =  2.6805968284606934\n",
      "996    MSE loss =  2.6805968284606934\n",
      "997    MSE loss =  2.6805968284606934\n",
      "998    MSE loss =  2.6805968284606934\n",
      "999    MSE loss =  2.6805968284606934\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(lr = 1e-4,max_iter = 100, parameters = net.get_parameters())\n",
    "n=10**3\n",
    "N=output_data.shape[0]\n",
    "batch_size = 100\n",
    "for t in range(n):\n",
    "    acc_loss=0\n",
    "    for b in range(0, N, batch_size):\n",
    "        predictions = net.forward(input_data[b:b+batch_size])\n",
    "        l= loss.forward(predictions, output_data[b:b+batch_size].unsqueeze(-1))\n",
    "        acc_loss += l\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        net.backward(loss.backward())\n",
    "        \n",
    "        new_par = optimizer.step()\n",
    "        net.set_parameters(new_par)\n",
    "    \n",
    "    print(t, '   MSE loss = ' , acc_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "favorite-secret",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions after 1000 training steps: 99.3 %\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "for i in range(output_data.shape[0]):\n",
    "        x=input_data[i]\n",
    "        y=2*output_data[i]-1\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = net.forward(x.unsqueeze(0))\n",
    "        if abs(y_pred-output_data[i])<1:\n",
    "            correct+=1\n",
    "print('Correct predictions after '+str(n)+' training steps: '+str(correct/N*100)+' %')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "modules+sequential_notokyet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tirocinio",
   "language": "python",
   "name": "tirocinio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
