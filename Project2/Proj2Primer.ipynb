{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thermal-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "#torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "therapeutic-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward (self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward ( self , * gradwrtoutput ) :\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def param ( self ) :\n",
    "        return []   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imperial-class",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Losses(object):        \n",
    "    def function():\n",
    "        return NotImplementedError\n",
    "    def derivative():\n",
    "        NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comfortable-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizers(object):\n",
    "    def step():\n",
    "        return NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "found-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "asian-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, input_dim, out_dim, bias = True):\n",
    "        super().__init__()\n",
    "        std = 1/math.sqrt(input_dim)\n",
    "        self.weight = Parameter()\n",
    "        \n",
    "        self.weight.data = torch.rand(out_dim, input_dim)\n",
    "        self.weight.data = 2*std*self.weight.data - std\n",
    "        \n",
    "        self.with_bias = bias\n",
    "        if bias :\n",
    "            self.bias = Parameter()\n",
    "            self.bias = torch.rand(out_dim)\n",
    "            self.bias = 2*std*self.bias.data - std\n",
    "            \n",
    "        self.x = None\n",
    "              \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return self.weight.data.mv(x) + self.bias.data\n",
    "        \n",
    "    def backward(self, prev_grad):\n",
    "        \n",
    "        prev_grad = prev_grad.view(-1, 1)\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "        \n",
    "        if self.weight.grad is None:\n",
    "            self.weight.grad = torch.zeros_like(self.weight.data)\n",
    "        \n",
    "        self.weight.grad += prev_grad.view(-1, 1)*self.x.view(1, -1)\n",
    "        \n",
    "        if self.with_bias:\n",
    "            if self.bias.grad is None:\n",
    "                self.bias.grad = torch.zeros_like(self.bias.data)\n",
    "            self.bias.grad += prev_grad.view(-1)\n",
    "        \n",
    "        next_grad = prev_grad.view(1, -1)@self.weight.data\n",
    "        next_grad = next_grad.view(-1, 1)\n",
    "        return next_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "written-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLu(Module):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "    \n",
    "    def forward (self, x):\n",
    "        self.x = x\n",
    "        return x.tanh()\n",
    "        \n",
    "    def backward ( self, prev_grad) :\n",
    "        if self.x is None:\n",
    "            raise CallForwardFirst\n",
    "            \n",
    "        def d(x):\n",
    "            return 4 * (x.exp() + x.mul(-1).exp()).pow(-2)\n",
    "        \n",
    "        prev_grad = prev_grad.view(-1)\n",
    "        deriv = d(self.x)\n",
    "        return deriv*prev_grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "happy-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Losses):\n",
    "    def function(self, v, t):\n",
    "        return (v - t).pow(2).mean()\n",
    "    \n",
    "    def derivative(self, v, t):\n",
    "        return 2 * (v - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nervous-ghost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0944,  0.9078,  1.0506, -0.0373,  0.2254, -0.3821]),\n",
       " tensor([-0.0944,  0.9078,  1.0506, -0.0373,  0.2254, -0.3821],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#forward, comparing with torch\n",
    "linear = Linear(5, 6, True)\n",
    "builtin_linear = torch.nn.Linear(5, 6)\n",
    "linear.weight.data = builtin_linear.weight.data\n",
    "linear.bias.data = builtin_linear.bias.data\n",
    "\n",
    "x = torch.randn(5)\n",
    "b = torch.randn(5)\n",
    "y = torch.randn(6)\n",
    "linear.forward(x), builtin_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chief-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward, comparing with torch\n",
    "linear = Linear(5, 6, True)\n",
    "builtin_linear = torch.nn.Linear(5, 6)\n",
    "linear.weight.data = builtin_linear.weight.data\n",
    "linear.bias.data = builtin_linear.bias.data\n",
    "relu = ReLu()\n",
    "\n",
    "#building loss derivative\n",
    "builtin_output = builtin_linear(x)\n",
    "builtin_loss = torch.nn.MSELoss()(builtin_output, y)\n",
    "builtin_loss_derivative = torch.autograd.grad( builtin_loss, builtin_output,  )[0]#.detach().copy()\n",
    "der = builtin_loss_derivative.detach()\n",
    "der.requires_grad = False\n",
    "\n",
    "relu.forward(linear.forward(x))\n",
    "linear.backward((der))\n",
    "\n",
    "###\n",
    "builtin_loss = torch.nn.MSELoss()(builtin_linear(x), y)\n",
    "builtin_loss.backward()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pacific-beach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0666,  0.0258,  0.2986, -0.0188,  0.1002],\n",
       "         [-0.0459, -0.0178, -0.2056,  0.0130, -0.0690],\n",
       "         [-0.0734, -0.0285, -0.3291,  0.0207, -0.1104],\n",
       "         [ 0.0060,  0.0023,  0.0269, -0.0017,  0.0090],\n",
       "         [-0.1086, -0.0421, -0.4867,  0.0307, -0.1633],\n",
       "         [-0.2709, -0.1051, -1.2143,  0.0765, -0.4075]]),\n",
       " tensor([[ 0.0666,  0.0258,  0.2986, -0.0188,  0.1002],\n",
       "         [-0.0459, -0.0178, -0.2056,  0.0130, -0.0690],\n",
       "         [-0.0734, -0.0285, -0.3291,  0.0207, -0.1104],\n",
       "         [ 0.0060,  0.0023,  0.0269, -0.0017,  0.0090],\n",
       "         [-0.1086, -0.0421, -0.4867,  0.0307, -0.1633],\n",
       "         [-0.2709, -0.1051, -1.2143,  0.0765, -0.4075]]),\n",
       " tensor([[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight.grad, builtin_linear.weight.grad, linear.weight.grad == builtin_linear.weight.grad "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tirocinio",
   "language": "python",
   "name": "tirocinio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
