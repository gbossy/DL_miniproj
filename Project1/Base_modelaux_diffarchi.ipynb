{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import torch\n",
    "import various_data_functions\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generation\n",
    "N=10**3\n",
    "#train_input,train_target,train_classes,test_input,test_target,test_classes=prologue.generate_pair_sets(N)\n",
    "train_input,train_target,train_classes,test_input,test_target,test_classes=various_data_functions.data(N,True,False,nn.CrossEntropyLoss)\n",
    "#train_target=train_target.long()#.float for MSELoss, .long for CrossEntropy\n",
    "#train_input=train_input.float()\n",
    "#train_classes=train_classes.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base functions adapted from the practicals\n",
    "def train_model(model, train_input, train_target,train_classes, mini_batch_size, crit=nn.MSELoss, eta = 1e-3, nb_epochs = 500,print_=False):\n",
    "    criterion = crit()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = eta)\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "        acc_loss1 = 0\n",
    "        acc_loss2 = 0\n",
    "        acc_loss3 = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output,aux_output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            if crit==nn.MSELoss:\n",
    "                loss1 = criterion(output[:,1], train_target.narrow(0, b, mini_batch_size))\n",
    "                #print(torch.argmax(aux_output[:,0:9],dim=1))\n",
    "                #print(train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(torch.argmax(aux_output[:,0:9],dim=1), train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(torch.argmax(aux_output[:,10:19],dim=1), train_classes[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                print('|| loss1 req grad =', loss1.requires_grad, '|| loss2 req grad =',loss2.requires_grad,'|| loss3 req grad =', loss3.requires_grad)\n",
    "            elif crit==nn.CrossEntropyLoss:\n",
    "                loss1 = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "                #print(torch.argmax(aux_output[:,0:9],dim=1))\n",
    "                #print(train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(aux_output[:,:10], train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(aux_output[:,10:], train_classes[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + 0.1*(loss2 + loss3)\n",
    "                #print(loss1, loss2.requires_grad, loss3.requires_grad)\n",
    "            else:\n",
    "                print(\"Loss not implemented\")\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "            acc_loss1 = acc_loss1 + loss1.item()\n",
    "            acc_loss2 = acc_loss2 + loss2.item()\n",
    "            acc_loss3 = acc_loss3 + loss3.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if False:\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p -= eta * p.grad\n",
    "        if print_:\n",
    "            print(e, 'tot loss', acc_loss, 'loss1', acc_loss1, 'loss2', acc_loss2, 'loss3', acc_loss3)\n",
    "            \n",
    "def compute_nb_errors(model, input, target, mini_batch_size=100):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output , aux_output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k]!=predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors\n",
    "\n",
    "def run_many_times(model,crit=nn.MSELoss,mini_batch_size=100,n=10,print_=False,eta=1e-3,nb_epochs=500):\n",
    "    average_error=0\n",
    "    for i in range(n):\n",
    "        m=model()\n",
    "        train_model(m, train_input, train_target,train_classes,mini_batch_size,crit=crit,eta=eta,nb_epochs=nb_epochs)\n",
    "        nb_test_errors = compute_nb_errors(m, test_input, test_target, mini_batch_size)\n",
    "        print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "        average_error+=(100 * nb_test_errors) / test_input.size(0)\n",
    "    print(\"Average error: \"+str(average_error/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is it better to use groups or not?\n",
    "#Takes about 2 hours to run\n",
    "#about 22.5% error average without groups if we exclude outliers that get stuck and don't move\n",
    "#about 21.5% error average with groups if we exclude outliers that get stuck and don't move\n",
    "class NetGroups3200Aux20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 200, kernel_size=3,groups=2)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(3200, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        aux_output = F.softmax(self.fc1(x.view(-1, 3200)), dim=1)\n",
    "        x = F.relu(self.fc1(x.view(-1, 3200)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        aux_output = F.softmax(x, dim=1)\n",
    "        return output, aux_output\n",
    "    \n",
    "class NetGroups3200Aux20_20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 200, kernel_size=3,groups=2)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(3200, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        aux_output = F.softmax(self.fc1(x.view(-1, 3200)), dim=1)\n",
    "        x = F.relu(self.fc1(x.view(-1, 3200)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        aux_output = F.softmax(self.aux_linear(x), dim=1)\n",
    "        return output, aux_output\n",
    "\n",
    "class NetGroups1600_400Aux20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 100, kernel_size=3,groups=2)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(1600, 400)\n",
    "        self.fc2 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 1600)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        aux_output = F.softmax(x, dim=1)\n",
    "        output = F.softmax(self.fc3(x),dim=1)\n",
    "        return output, aux_output\n",
    "\n",
    "class NetGroups320_160_80_40Aux20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 20, kernel_size=3,groups=2)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(320, 160)\n",
    "        self.fc2 = nn.Linear(160, 80)\n",
    "        self.fc3 = nn.Linear(80,40)\n",
    "        self.fc4 = nn.Linear(40,20)\n",
    "        self.fc5 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 320)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        aux_output = F.softmax(x, dim=1)\n",
    "        output = F.softmax(self.fc5(x),dim=1)\n",
    "        return output, aux_output\n",
    "    \n",
    "class NetGroupsDoubleConvAux20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3,groups=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3,groups=2)\n",
    "        self.fc1 = nn.Linear(256, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        aux_output = F.softmax(self.fc1(x.view(-1, 256)), dim=1)\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        aux_output = F.softmax(x, dim=1)\n",
    "        return output, aux_output\n",
    "    \n",
    "class NetGroups320_80DoubleConvAux20(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 80, kernel_size=3,groups=2)\n",
    "        self.conv2 = nn.Conv2d(80, 320, kernel_size=3,groups=2)\n",
    "        self.fc1 = nn.Linear(1280, 320)\n",
    "        self.fc2 = nn.Linear(320, 80)\n",
    "        self.fc3 = nn.Linear(80, 20)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        aux_output = F.softmax(self.fc1(x.view(-1, 1280)), dim=1)\n",
    "        x = F.relu(self.fc1(x.view(-1, 1280)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = F.softmax(self.fc3(x), dim=1)\n",
    "        aux_output = F.softmax(x, dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 21.10% 211/1000\n",
      "test error Net 20.20% 202/1000\n",
      "test error Net 20.50% 205/1000\n",
      "test error Net 19.90% 199/1000\n",
      "test error Net 20.50% 205/1000\n",
      "test error Net 20.80% 208/1000\n",
      "test error Net 21.10% 211/1000\n",
      "test error Net 20.90% 209/1000\n",
      "test error Net 20.00% 200/1000\n",
      "test error Net 21.10% 211/1000\n",
      "Average error: 20.61\n"
     ]
    }
   ],
   "source": [
    "run_many_times(NetGroups3200Aux20,crit=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 19.30% 193/1000\n",
      "test error Net 20.60% 206/1000\n",
      "test error Net 21.10% 211/1000\n",
      "test error Net 20.20% 202/1000\n",
      "test error Net 20.50% 205/1000\n",
      "test error Net 20.50% 205/1000\n",
      "test error Net 20.70% 207/1000\n",
      "test error Net 20.80% 208/1000\n",
      "test error Net 21.40% 214/1000\n",
      "test error Net 20.60% 206/1000\n",
      "Average error: 20.57\n"
     ]
    }
   ],
   "source": [
    "#Same as before but with 20x20 linear layer before the auxiliary loss\n",
    "run_many_times(NetGroups3200Aux20_20,crit=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 22.50% 225/1000\n",
      "test error Net 23.70% 237/1000\n",
      "test error Net 22.60% 226/1000\n",
      "test error Net 22.40% 224/1000\n",
      "test error Net 23.00% 230/1000\n",
      "test error Net 22.80% 228/1000\n",
      "test error Net 22.70% 227/1000\n",
      "test error Net 22.40% 224/1000\n",
      "test error Net 24.20% 242/1000\n",
      "test error Net 22.30% 223/1000\n",
      "Average error: 22.860000000000003\n"
     ]
    }
   ],
   "source": [
    "#Same as the first but with 2 linear layers\n",
    "run_many_times(NetGroups1600_400Aux20,crit=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "Average error: 47.39999999999999\n"
     ]
    }
   ],
   "source": [
    "run_many_times(NetGroups320_160_80_40Aux20,crit=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 20.50% 205/1000\n",
      "test error Net 19.00% 190/1000\n",
      "test error Net 20.90% 209/1000\n",
      "test error Net 20.00% 200/1000\n",
      "test error Net 18.70% 187/1000\n",
      "test error Net 20.30% 203/1000\n",
      "test error Net 20.40% 204/1000\n",
      "test error Net 19.20% 192/1000\n",
      "test error Net 19.60% 196/1000\n",
      "test error Net 21.80% 218/1000\n",
      "Average error: 20.04\n"
     ]
    }
   ],
   "source": [
    "#This time with eta=1e-2 and 2000 epochs as the network is deeper\n",
    "run_many_times(NetGroups320_160_80_40Aux20,crit=nn.CrossEntropyLoss,eta=1e-2,nb_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 31.80% 318/1000\n",
      "test error Net 46.80% 468/1000\n",
      "test error Net 39.60% 396/1000\n",
      "test error Net 45.40% 454/1000\n",
      "test error Net 43.70% 437/1000\n",
      "test error Net 33.80% 338/1000\n",
      "test error Net 43.90% 439/1000\n",
      "test error Net 41.50% 415/1000\n",
      "test error Net 43.50% 435/1000\n",
      "test error Net 36.40% 364/1000\n",
      "Average error: 40.64\n"
     ]
    }
   ],
   "source": [
    "#Trying with 2 convolutional layers\n",
    "run_many_times(NetGroupsDoubleConvAux20,crit=nn.CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 19.90% 199/1000\n",
      "test error Net 21.60% 216/1000\n",
      "test error Net 17.20% 172/1000\n",
      "test error Net 18.60% 186/1000\n",
      "test error Net 19.20% 192/1000\n",
      "test error Net 20.60% 206/1000\n",
      "test error Net 19.90% 199/1000\n",
      "test error Net 18.80% 188/1000\n",
      "test error Net 20.60% 206/1000\n",
      "test error Net 19.70% 197/1000\n",
      "Average error: 19.610000000000003\n"
     ]
    }
   ],
   "source": [
    "#This time with more epochs\n",
    "run_many_times(NetGroupsDoubleConvAux20,crit=nn.CrossEntropyLoss,nb_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 22.50% 225/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "Average error: 44.90999999999999\n"
     ]
    }
   ],
   "source": [
    "run_many_times(NetGroups320_80DoubleConvAux20,crit=nn.CrossEntropyLoss,nb_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 18.00% 180/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 17.80% 178/1000\n",
      "test error Net 47.40% 474/1000\n",
      "Average error: 41.49999999999999\n"
     ]
    }
   ],
   "source": [
    "#Try with a different eta\n",
    "run_many_times(NetGroups320_80DoubleConvAux20,crit=nn.CrossEntropyLoss,eta=1e-2,nb_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 52.60% 526/1000\n",
      "test error Net 52.60% 526/1000\n",
      "test error Net 47.40% 474/1000\n",
      "test error Net 52.60% 526/1000\n"
     ]
    }
   ],
   "source": [
    "#Again with smaller eta this time\n",
    "run_many_times(NetGroups320_80DoubleConvAux20,crit=nn.CrossEntropyLoss,eta=1e-4,nb_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_many_times(NetGroups320_80DoubleConvAux20,crit=nn.CrossEntropyLoss,eta=1e-1,nb_epochs=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
