{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch\n",
    "import various_data_functions\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from models import *\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "N=10**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base functions adapted from the practicals\n",
    "def train_model(model, train_input, train_target,train_classes, mini_batch_size, test_input=None, test_target=None, crit=nn.CrossEntropyLoss, eta = 1e-3, nb_epochs = 50,print_=False, store_loss = False, aux_factor=1, store_error=False, checkpoint_name=None):\n",
    "    #Initializing the loss, the optimizer, and the stored loss and errors for the plots\n",
    "    criterion = crit()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    stored_loss = []\n",
    "    stored_error = []\n",
    "    \n",
    "    #Retrieving data if a similar model has already been trained or partially trained\n",
    "    nb_epochs_finished = 0\n",
    "    if checkpoint_name!=None:\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_name)\n",
    "            nb_epochs_finished = checkpoint['nb_epochs_finished']\n",
    "            model.load_state_dict(checkpoint['model_state'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "            if print_:\n",
    "                print(f'Checkpoint loaded with {nb_epochs_finished} epochs finished.')\n",
    "            stored_loss=checkpoint['stored_loss']\n",
    "            if len(stored_loss)>nb_epochs*3:\n",
    "                stored_loss=stored_loss[0:nb_epochs*3]\n",
    "            stored_error=checkpoint['stored_error']\n",
    "            if len(stored_error)>nb_epochs:\n",
    "                stored_error=stored_error[0:nb_epochs]\n",
    "        except FileNotFoundError:\n",
    "            if print_:\n",
    "                print('Starting from scratch.')\n",
    "        except:\n",
    "            print('Error when loading the checkpoint.')\n",
    "            exit(1)\n",
    "    \n",
    "    #Training the network if the checkpoint wasn't fully trained\n",
    "    for e in range(nb_epochs_finished,nb_epochs):\n",
    "        \n",
    "        #Initializing accumulated losses\n",
    "        #loss1 is the loss over the output (identifying which number is greater)\n",
    "        #loss2 and loss3 are the auxiliary losses trying to identify the numbers in the 2 input pictures\n",
    "        acc_loss = 0\n",
    "        acc_loss1 = 0\n",
    "        acc_loss2 = 0\n",
    "        acc_loss3 = 0\n",
    "        \n",
    "        #permuting the samples\n",
    "        permuted_index = torch.randperm(train_input.size()[0])\n",
    "        train_input_shuffled = train_input[permuted_index]\n",
    "        train_target_shuffled = train_target[permuted_index]\n",
    "        train_classes_shuffled = train_classes[permuted_index]\n",
    "        \n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            \n",
    "            #forward pass\n",
    "            output,aux_output = model(train_input_shuffled.narrow(0, b, mini_batch_size))\n",
    "            if crit==nn.MSELoss:\n",
    "                \n",
    "                #computing the different losses\n",
    "                loss1 = criterion(output[:,1], train_target_shuffled.narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(torch.argmax(aux_output[:,0:9],dim=1), train_classes_shuffled[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(torch.argmax(aux_output[:,10:19],dim=1), train_classes_shuffled[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                print('|| loss1 req grad =', loss1.requires_grad, '|| loss2 req grad =',loss2.requires_grad,'|| loss3 req grad =', loss3.requires_grad)\n",
    "            elif crit==nn.CrossEntropyLoss:\n",
    "                loss1 = criterion(output, train_target_shuffled.narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(aux_output[:,:10], train_classes_shuffled[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(aux_output[:,10:], train_classes_shuffled[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + aux_factor*(loss2 + loss3)\n",
    "            else:\n",
    "                print(\"Loss not implemented\")\n",
    "                \n",
    "            #Update the accumulated losses\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "            acc_loss1 = acc_loss1 + loss1.item()\n",
    "            acc_loss2 = acc_loss2 + loss2.item()\n",
    "            acc_loss3 = acc_loss3 + loss3.item()\n",
    "            \n",
    "            #zero the gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            #backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            #optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "        #update the stored losses and error if needed\n",
    "        if store_loss:\n",
    "            stored_loss += [[acc_loss1], [acc_loss2], [acc_loss3]]\n",
    "        if store_error:\n",
    "            stored_error +=[compute_nb_errors(model, test_input, test_target, mini_batch_size)]\n",
    "            \n",
    "        #print the different losses if needed\n",
    "        if print_:\n",
    "            print(e, 'tot loss', acc_loss, 'loss1', acc_loss1, 'loss2', acc_loss2, 'loss3', acc_loss3)\n",
    "            \n",
    "        #save the checkpoint for later if needed\n",
    "        if checkpoint_name!=None:\n",
    "                checkpoint = {'nb_epochs_finished': e + 1,'model_state': model.state_dict(),'optimizer_state': optimizer.state_dict(),'stored_loss':stored_loss,'stored_error':stored_error}\n",
    "                torch.save(checkpoint, checkpoint_name)\n",
    "        \n",
    "    #return the stored quantities  \n",
    "    return torch.tensor(stored_loss),torch.tensor(stored_error)\n",
    "\n",
    "#error computing function\n",
    "def compute_nb_errors(model, input, target, mini_batch_size=100):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output , aux_output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k]!=predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors\n",
    "\n",
    "#this function lets us do multiple runs more easily\n",
    "def run_many_times(model,crit=nn.CrossEntropyLoss,mini_batch_size=100,n=10,print_=True,eta=1e-3,nb_epochs=25,aux_factor=0,shuffle=True, store_error=False,checkpoint_name=None):\n",
    "    average_error=0\n",
    "    losses=torch.empty(0,nb_epochs,3)\n",
    "    errors=torch.empty(0,nb_epochs)\n",
    "    for i in range(n):\n",
    "        m=model()\n",
    "        train_input,train_target,train_classes,test_input,test_target,test_classes=various_data_functions.data(N,True,False,nn.CrossEntropyLoss,shuffle=shuffle)\n",
    "        if checkpoint_name!=None:\n",
    "            checkpoint_name_spec=checkpoint_name+'try_'+str(i)+'.pth'\n",
    "        else:\n",
    "            checkpoint_name_spec=None\n",
    "        new_losses,new_errors=train_model(m, train_input, train_target,train_classes,mini_batch_size,test_input=test_input, test_target=test_target,crit=crit,eta=eta,nb_epochs=nb_epochs,aux_factor=aux_factor,store_loss=True,store_error=store_error,checkpoint_name=checkpoint_name_spec)\n",
    "        new_losses=new_losses.view(1,nb_epochs, 3)\n",
    "        if store_error:\n",
    "            new_errors= new_errors.view(1,nb_epochs)\n",
    "            errors = torch.cat((errors,new_errors),0)\n",
    "        if print_:\n",
    "            losses = torch.cat((losses, new_losses), 0)\n",
    "            nb_test_errors = compute_nb_errors(m, test_input, test_target, mini_batch_size)\n",
    "            print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "            average_error+=(100 * nb_test_errors) / test_input.size(0)\n",
    "    if print_:\n",
    "        print(\"Average error: \"+str(average_error/n))\n",
    "        avg_losses=torch.sum(losses,0)/n\n",
    "        mod=int(torch.floor(torch.Tensor([nb_epochs/25])))\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True)\n",
    "        x_lab=torch.arange(nb_epochs)\n",
    "        x_labels=x_lab[x_lab%mod==0].detach().numpy()\n",
    "        ax0.errorbar(x_labels, avg_losses[x_lab%mod==0,0].detach().numpy(), yerr=torch.std(losses[:,:,0],0)[x_lab%mod==0].detach().numpy(), fmt='-o')\n",
    "        ax0.set_title('evolution of the cross entropy loss')\n",
    "        ax1.errorbar(x_labels, avg_losses[x_lab%mod==0,1].detach().numpy(), yerr=torch.std(losses[:,:,1],0)[x_lab%mod==0].detach().numpy(), fmt='o')\n",
    "        ax1.errorbar(x_labels, avg_losses[x_lab%mod==0,2].detach().numpy(), yerr=torch.std(losses[:,:,2],0)[x_lab%mod==0].detach().numpy(), fmt='o')\n",
    "        ax1.set_title('evolution of the auxiliary losses')\n",
    "        plt.show()\n",
    "    if store_error:\n",
    "        return errors\n",
    "\n",
    "#this function returns the number of parameters of a model\n",
    "def n_params(model):\n",
    "    n = 0\n",
    "    for params in model.parameters():\n",
    "        n += params.numel()\n",
    "    return n\n",
    "\n",
    "#this function uses the previous ones to build a big plot with several models\n",
    "def big_error_plot(models,model_names,n=50,nb_epochs=100,eta=1e-3,div=25,name=\"big_error_plot.png\",aux_factor=1):\n",
    "    x_lab=torch.arange(nb_epochs)\n",
    "    mod=int(torch.floor(torch.Tensor([nb_epochs/div])))\n",
    "    x_labels=x_lab[x_lab%mod==0].detach().numpy()\n",
    "    for i in range(len(models)):\n",
    "        print(\"Starting model \" + model_names[i])\n",
    "        errors=run_many_times(models[i],crit=nn.CrossEntropyLoss,mini_batch_size=10,n=n,print_=False,eta=eta,nb_epochs=nb_epochs,aux_factor=aux_factor,shuffle=True, store_error=True,checkpoint_name='checkpoints/'+model_names[i]+'_'+str(aux_factor).replace('.','')+'_')\n",
    "        #sns.relplot(x=\"timepoint\", y=\"signal\", col=\"region\",hue=\"event\", style=\"event\", kind=\"line\",)\n",
    "        #plt.errorbar(x_labels,(errors.mean(dim=0)[x_lab%mod==0]/10).detach().numpy(),yerr=(torch.std(errors,dim=0)[x_lab%mod==0]/10).detach().numpy(),fmt='o')\n",
    "        mean=(errors.mean(dim=0)[x_lab%mod==0]/10).detach().numpy()\n",
    "        std=(torch.std(errors,dim=0)[x_lab%mod==0]/10).detach().numpy()\n",
    "        plt.plot(x_labels,mean)\n",
    "        plt.fill_between(x_labels,mean-std,mean+std,alpha=0.5)\n",
    "        print('Mean number of errors after '+str(nb_epochs)+' epochs of training: '+str(errors.mean(dim=0)[-1])+' out of 1000 test samples with a standard deviation of '+str(torch.std(errors,dim=0)[-1]))\n",
    "    plt.ylabel('Accuracy[%]')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.legend(model_names)\n",
    "    plt.savefig(name,bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpuit images are 14*14 \n",
    "shuffle = True\n",
    "train_input,train_target,train_classes,test_input,test_target,test_classes=various_data_functions.data(N,True,False,nn.CrossEntropyLoss,shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3328)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after = (14-3+1)/3\n",
    "int(after), int(2*after**2*104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C1L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images \n",
    "        self.conv1 = nn.Conv2d(1, 104, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(4**2*104, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*104, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=3, stride=3))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=3, stride=3))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*104)\n",
    "        x2 = x2.view(-1, 4**2*104)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C1L2WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape\n",
    "aux_output[4,1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = (14-3+1)/3\n",
    "int(after), int(2*after**2*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C1L3WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images\n",
    "        self.aux_linear = nn.Linear(4**2*16, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*16, 128)\n",
    "        self.fc2 = nn.Linear(128, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=3, stride=3))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=3, stride=3))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*16)\n",
    "        x2 = x2.view(-1, 4**2*16)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = F.softmax(self.fc3(x),dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C1L3WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 320)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = (14-3+1)/3\n",
    "int(after), int(2*after**2*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C1L5WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(4**2*10, 20)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*10, 160)\n",
    "        self.fc2 = nn.Linear(160, 80)\n",
    "        self.fc3 = nn.Linear(80,40)\n",
    "        self.fc4 = nn.Linear(40,20)\n",
    "        self.fc5 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=3, stride=3))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=3, stride=3))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*10)\n",
    "        x2 = x2.view(-1, 4**2*10)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x.view(-1, 4**2*10)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        output = F.softmax(self.fc5(x),dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C1L3WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 960)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = 4\n",
    "int(after), int(2*2**2*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C2L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images \n",
    "        self.conv1 = nn.Conv2d(1, 24, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(24, 120, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(2**2*120, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*2**2*120, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=2, stride=2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv2(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv2(x2), kernel_size=2, stride=2))\n",
    "\n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 2**2*120)\n",
    "        x2 = x2.view(-1, 2**2*120)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x.view(-1, 2*2**2*120)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C2L2WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = 4\n",
    "int(after), int(2*2**2*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C2L3WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(16, 64, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(2**2*64, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*2**2*64, 60)\n",
    "        self.fc2 = nn.Linear(60, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=2, stride=2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv2(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv2(x2), kernel_size=2, stride=2))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 2**2*64)\n",
    "        x2 = x2.view(-1, 2**2*64)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = F.softmax(self.fc3(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C2L3WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = 4\n",
    "int(after), int(2*4**2*44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C3L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 11, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(11, 22, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(22, 44, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(4**2*44, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*44, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution\n",
    "        x1 = F.relu(self.conv1(picture1))\n",
    "        x2 = F.relu(self.conv1(picture2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(self.conv2(x1))\n",
    "        x2 = F.relu(self.conv2(x2))\n",
    "        #third convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv3(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv3(x2), kernel_size=2, stride=2))\n",
    "        \n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*44)\n",
    "        x2 = x2.view(-1, 4**2*44)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C3L2WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "##########################################################################################################################\n",
    "#Not done for those below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C4L2WS_bis(nn.Module):#bis got changed\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(6, 12, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(12, 24, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(24, 48, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(864, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        x1_1 = F.relu(self.conv1(picture1))\n",
    "        x1_2 = F.relu(self.conv2(x1_1))\n",
    "        x1_3 = F.relu(self.conv3(x1_2))\n",
    "        x1 = F.relu(F.max_pool2d(self.conv4(x1_3), kernel_size=2, stride=2))\n",
    "        x2_1 = F.relu(self.conv1(picture2))\n",
    "        x2_2 = F.relu(self.conv2(x2_1))\n",
    "        x2_3 = F.relu(self.conv3(x2_2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv4(x2_3), kernel_size=2, stride=2))\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "\n",
    "        aux_output = F.softmax(self.fc1(x.view(-1, 864)), dim=1)\n",
    "        x = F.relu(self.fc1(x.view(-1, 864)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        aux_output = F.softmax(x, dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C4L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=2)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(1152, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        x1_1 = F.relu(self.conv1(picture1))\n",
    "        x1_2 = F.relu(F.max_pool2d(self.conv2(x1_1), kernel_size=2, stride=2))\n",
    "        x1_3 = F.relu(self.conv3(x1_2))\n",
    "        x1 = F.relu(self.conv4(x1_3))\n",
    "        x2_1 = F.relu(self.conv1(picture2))\n",
    "        x2_2 = F.relu(F.max_pool2d(self.conv2(x2_1), kernel_size=2, stride=2))\n",
    "        x2_3 = F.relu(self.conv3(x2_2))\n",
    "        x2 = F.relu(self.conv4(x2_3))\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "\n",
    "        aux_output = F.softmax(self.fc1(x.view(-1, 1152)), dim=1)\n",
    "        x = F.relu(self.fc1(x.view(-1, 1152)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        aux_output = F.softmax(x, dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Net3328Aux20WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Net512_128Aux20WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Net320_160_80_40Aux20WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=NetDoubleConvAux20WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Net512_60DoubleConvAux20WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Net1408TripleConvAux20WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Net864QuadConvAux20WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Net1152QuadConvAux20WS_bis()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To test the function\n",
    "models=[Net3328Aux20WS,Net512_128Aux20WS]\n",
    "model_names=['Net3328Aux20WS','Net512_128Aux20WS']\n",
    "big_error_plot(models,model_names,n=3,nb_epochs=10,div=10,name=\"TestPlot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[C1L2WS,C2L2WS,C2L3WS,C3L2WS,C4L2WS]\n",
    "model_names=['C1L2WS','C2L2WS','C2L3WS','C3L2WS','C4L2WS']\n",
    "big_error_plot(models,model_names,n=10,nb_epochs=20,name='big_plot_WS_aux=1.png',div=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as before with aux_factor=0.1\n",
    "models=[C1L2WS,C2L2WS,C2L3WS,C3L2WS,C4L2WS]\n",
    "model_names=['C1L2WS','C2L2WS','C2L3WS','C3L2WS','C4L2WS']\n",
    "big_error_plot(models,model_names,n=10,nb_epochs=20,aux_factor=0.1,name='big_plot_WS_aux=01.png',div=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as before with aux_factor=1\n",
    "models=[C1L2WS,C2L2WS,C2L3WS,C3L2WS,C4L2WS]\n",
    "model_names=['C1L2WS','C2L2WS','C2L3WS','C3L2WS','C4L2WS']\n",
    "big_error_plot(models,model_names,n=10,nb_epochs=20,aux_factor=1,name='big_plot_WS_aux=0.png',div=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tirocinio",
   "language": "python",
   "name": "tirocinio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
