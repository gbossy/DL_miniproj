{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch\n",
    "import various_data_functions\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from models import *\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "N=10**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base functions adapted from the practicals\n",
    "def train_model(model, train_input, train_target,train_classes, mini_batch_size, test_input=None, test_target=None, crit=nn.CrossEntropyLoss, eta = 1e-3, nb_epochs = 50,print_=False, store_loss = False, aux_factor=1, store_error=False, checkpoint_name=None):\n",
    "    #Initializing the loss, the optimizer, and the stored loss and errors for the plots\n",
    "    criterion = crit()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    stored_loss = []\n",
    "    stored_error = []\n",
    "    \n",
    "    #Retrieving data if a similar model has already been trained or partially trained\n",
    "    nb_epochs_finished = 0\n",
    "    if checkpoint_name!=None:\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_name)\n",
    "            nb_epochs_finished = checkpoint['nb_epochs_finished']\n",
    "            model.load_state_dict(checkpoint['model_state'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "            if print_:\n",
    "                print(f'Checkpoint loaded with {nb_epochs_finished} epochs finished.')\n",
    "            stored_loss=checkpoint['stored_loss']\n",
    "            if len(stored_loss)>nb_epochs*3:\n",
    "                stored_loss=stored_loss[0:nb_epochs*3]\n",
    "            stored_error=checkpoint['stored_error']\n",
    "            if len(stored_error)>nb_epochs:\n",
    "                stored_error=stored_error[0:nb_epochs]\n",
    "        except FileNotFoundError:\n",
    "            if print_:\n",
    "                print('Starting from scratch.')\n",
    "        except:\n",
    "            print('Error when loading the checkpoint.')\n",
    "            exit(1)\n",
    "    \n",
    "    #Training the network if the checkpoint wasn't fully trained\n",
    "    for e in range(nb_epochs_finished,nb_epochs):\n",
    "        \n",
    "        #Initializing accumulated losses\n",
    "        #loss1 is the loss over the output (identifying which number is greater)\n",
    "        #loss2 and loss3 are the auxiliary losses trying to identify the numbers in the 2 input pictures\n",
    "        acc_loss = 0\n",
    "        acc_loss1 = 0\n",
    "        acc_loss2 = 0\n",
    "        acc_loss3 = 0\n",
    "        \n",
    "        #permuting the samples\n",
    "        permuted_index = torch.randperm(train_input.size()[0])\n",
    "        train_input_shuffled = train_input[permuted_index]\n",
    "        train_target_shuffled = train_target[permuted_index]\n",
    "        train_classes_shuffled = train_classes[permuted_index]\n",
    "        \n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            \n",
    "            #forward pass\n",
    "            output,aux_output = model(train_input_shuffled.narrow(0, b, mini_batch_size))\n",
    "            if crit==nn.MSELoss:\n",
    "                \n",
    "                #computing the different losses\n",
    "                loss1 = criterion(output[:,1], train_target_shuffled.narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(torch.argmax(aux_output[:,0:9],dim=1), train_classes_shuffled[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(torch.argmax(aux_output[:,10:19],dim=1), train_classes_shuffled[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                print('|| loss1 req grad =', loss1.requires_grad, '|| loss2 req grad =',loss2.requires_grad,'|| loss3 req grad =', loss3.requires_grad)\n",
    "            elif crit==nn.CrossEntropyLoss:\n",
    "                loss1 = criterion(output, train_target_shuffled.narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(aux_output[:,0,:], train_classes_shuffled[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(aux_output[:,1,:], train_classes_shuffled[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + aux_factor*(loss2 + loss3)\n",
    "            else:\n",
    "                print(\"Loss not implemented\")\n",
    "                \n",
    "            #Update the accumulated losses\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "            acc_loss1 = acc_loss1 + loss1.item()\n",
    "            acc_loss2 = acc_loss2 + loss2.item()\n",
    "            acc_loss3 = acc_loss3 + loss3.item()\n",
    "            \n",
    "            #zero the gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            #backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            #optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "        #update the stored losses and error if needed\n",
    "        if store_loss:\n",
    "            stored_loss += [[acc_loss1], [acc_loss2], [acc_loss3]]\n",
    "        if store_error:\n",
    "            stored_error +=[compute_nb_errors(model, test_input, test_target, mini_batch_size)]\n",
    "            \n",
    "        #print the different losses if needed\n",
    "        if print_:\n",
    "            print(e, 'tot loss', acc_loss, 'loss1', acc_loss1, 'loss2', acc_loss2, 'loss3', acc_loss3)\n",
    "            \n",
    "        #save the checkpoint for later if needed\n",
    "        if checkpoint_name!=None:\n",
    "                checkpoint = {'nb_epochs_finished': e + 1,'model_state': model.state_dict(),'optimizer_state': optimizer.state_dict(),'stored_loss':stored_loss,'stored_error':stored_error}\n",
    "                torch.save(checkpoint, checkpoint_name)\n",
    "        \n",
    "    #return the stored quantities  \n",
    "    return torch.tensor(stored_loss),torch.tensor(stored_error)\n",
    "\n",
    "#error computing function\n",
    "def compute_nb_errors(model, input, target, mini_batch_size=100):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output , aux_output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k]!=predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors\n",
    "\n",
    "#this function lets us do multiple runs more easily\n",
    "def run_many_times(model,crit=nn.CrossEntropyLoss,mini_batch_size=100,n=10,print_=True,eta=1e-3,nb_epochs=25,aux_factor=0,shuffle=True, store_error=False,checkpoint_name=None):\n",
    "    average_error=0\n",
    "    losses=torch.empty(0,nb_epochs,3)\n",
    "    errors=torch.empty(0,nb_epochs)\n",
    "    for i in range(n):\n",
    "        m=model()\n",
    "        train_input,train_target,train_classes,test_input,test_target,test_classes=various_data_functions.data(N,True,False,nn.CrossEntropyLoss,shuffle=shuffle)\n",
    "        if checkpoint_name!=None:\n",
    "            checkpoint_name_spec=checkpoint_name+'try_'+str(i)+'.pth'\n",
    "        else:\n",
    "            checkpoint_name_spec=None\n",
    "        new_losses,new_errors=train_model(m, train_input, train_target,train_classes,mini_batch_size,test_input=test_input, test_target=test_target,crit=crit,eta=eta,nb_epochs=nb_epochs,aux_factor=aux_factor,store_loss=True,store_error=store_error,checkpoint_name=checkpoint_name_spec)\n",
    "        new_losses=new_losses.view(1,nb_epochs, 3)\n",
    "        if store_error:\n",
    "            new_errors= new_errors.view(1,nb_epochs)\n",
    "            errors = torch.cat((errors,new_errors),0)\n",
    "        if print_:\n",
    "            losses = torch.cat((losses, new_losses), 0)\n",
    "            nb_test_errors = compute_nb_errors(m, test_input, test_target, mini_batch_size)\n",
    "            print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "            average_error+=(100 * nb_test_errors) / test_input.size(0)\n",
    "    if print_:\n",
    "        print(\"Average error: \"+str(average_error/n))\n",
    "        avg_losses=torch.sum(losses,0)/n\n",
    "        mod=int(torch.floor(torch.Tensor([nb_epochs/20])))\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True)\n",
    "        x_lab=torch.arange(nb_epochs)\n",
    "        x_labels=x_lab[x_lab%mod==0].detach().numpy()\n",
    "        ax0.errorbar(x_labels, avg_losses[x_lab%mod==0,0].detach().numpy(), yerr=torch.std(losses[:,:,0],0)[x_lab%mod==0].detach().numpy(), fmt='-o')\n",
    "        ax0.set_title('evolution of the cross entropy loss')\n",
    "        ax1.errorbar(x_labels, avg_losses[x_lab%mod==0,1].detach().numpy(), yerr=torch.std(losses[:,:,1],0)[x_lab%mod==0].detach().numpy(), fmt='o')\n",
    "        ax1.errorbar(x_labels, avg_losses[x_lab%mod==0,2].detach().numpy(), yerr=torch.std(losses[:,:,2],0)[x_lab%mod==0].detach().numpy(), fmt='o')\n",
    "        ax1.set_title('evolution of the auxiliary losses')\n",
    "        plt.show()\n",
    "    if store_error:\n",
    "        return errors\n",
    "\n",
    "#this function returns the number of parameters of a model\n",
    "def n_params(model):\n",
    "    n = 0\n",
    "    for params in model.parameters():\n",
    "        n += params.numel()\n",
    "    return n\n",
    "\n",
    "#this function uses the previous ones to build a big plot with several models\n",
    "def big_error_plot(models,model_names,n=50,nb_epochs=100,eta=1e-3,div=25,name=\"big_error_plot.png\",aux_factor=1):\n",
    "    x_lab=torch.arange(nb_epochs)\n",
    "    mod=int(torch.floor(torch.Tensor([nb_epochs/div])))\n",
    "    x_labels=x_lab[x_lab%mod==0].detach().numpy()\n",
    "    for i in range(len(models)):\n",
    "        print(\"Starting model \" + model_names[i])\n",
    "        errors=run_many_times(models[i],crit=nn.CrossEntropyLoss,mini_batch_size=10,n=n,print_=False,eta=eta,nb_epochs=nb_epochs,aux_factor=aux_factor,shuffle=True, store_error=True,checkpoint_name='checkpoints/'+model_names[i]+'_'+str(aux_factor).replace('.','')+'_')\n",
    "        #sns.relplot(x=\"timepoint\", y=\"signal\", col=\"region\",hue=\"event\", style=\"event\", kind=\"line\",)\n",
    "        #plt.errorbar(x_labels,(errors.mean(dim=0)[x_lab%mod==0]/10).detach().numpy(),yerr=(torch.std(errors,dim=0)[x_lab%mod==0]/10).detach().numpy(),fmt='o')\n",
    "        mean=(errors.mean(dim=0)[x_lab%mod==0]/10).detach().numpy()\n",
    "        std=(torch.std(errors,dim=0)[x_lab%mod==0]/10).detach().numpy()\n",
    "        plt.plot(x_labels,mean)\n",
    "        plt.fill_between(x_labels,mean-std,mean+std,alpha=0.3)\n",
    "        print('Mean number of errors after '+str(nb_epochs)+' epochs of training: '+str(errors.mean(dim=0)[-1])+' out of 1000 test samples with a standard deviation of '+str(torch.std(errors,dim=0)[-1]))\n",
    "    plt.ylabel('Accuracy[%]')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.legend(model_names)\n",
    "    plt.savefig(name,bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpuit images are 14*14 \n",
    "shuffle = True\n",
    "train_input,train_target,train_classes,test_input,test_target,test_classes=various_data_functions.data(N,True,False,nn.CrossEntropyLoss,shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3328)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after = (14-3+1)/3\n",
    "int(after), int(2*after**2*104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C1L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images \n",
    "        self.conv1 = nn.Conv2d(1, 104, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(4**2*104, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*104, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=3, stride=3))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=3, stride=3))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*104)\n",
    "        x2 = x2.view(-1, 4**2*104)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2136, 0.0655, 0.0487, 0.0874, 0.1247, 0.1386, 0.0703, 0.0691,\n",
      "          0.1024, 0.0796],\n",
      "         [0.2391, 0.0783, 0.0562, 0.0794, 0.0984, 0.0942, 0.0889, 0.0363,\n",
      "          0.1474, 0.0818]],\n",
      "\n",
      "        [[0.2090, 0.0776, 0.0441, 0.1097, 0.0626, 0.1001, 0.0587, 0.0818,\n",
      "          0.0808, 0.1758],\n",
      "         [0.2650, 0.0688, 0.0397, 0.0826, 0.1024, 0.1186, 0.0866, 0.0904,\n",
      "          0.0725, 0.0735]],\n",
      "\n",
      "        [[0.1782, 0.0608, 0.0653, 0.1084, 0.0754, 0.1060, 0.1156, 0.0581,\n",
      "          0.1131, 0.1190],\n",
      "         [0.2446, 0.0477, 0.0228, 0.0938, 0.1026, 0.1299, 0.0760, 0.0710,\n",
      "          0.1091, 0.1024]],\n",
      "\n",
      "        [[0.1642, 0.0374, 0.0504, 0.0681, 0.0769, 0.1244, 0.1096, 0.1107,\n",
      "          0.1002, 0.1581],\n",
      "         [0.1748, 0.0615, 0.0414, 0.1057, 0.0976, 0.1053, 0.0678, 0.1024,\n",
      "          0.1133, 0.1302]],\n",
      "\n",
      "        [[0.1977, 0.0396, 0.0406, 0.0711, 0.0977, 0.1292, 0.1177, 0.0673,\n",
      "          0.0713, 0.1678],\n",
      "         [0.2418, 0.0484, 0.0404, 0.0707, 0.0924, 0.1593, 0.0663, 0.0569,\n",
      "          0.1003, 0.1235]]], grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C1L2WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "print(aux_output)\n",
    "output.shape, aux_output.shape\n",
    "aux_output[4,1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = (14-3+1)/3\n",
    "int(after), int(2*after**2*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C1L3WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images\n",
    "        self.aux_linear = nn.Linear(4**2*16, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*16, 128)\n",
    "        self.fc2 = nn.Linear(128, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=3, stride=3))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=3, stride=3))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*16)\n",
    "        x2 = x2.view(-1, 4**2*16)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = F.softmax(self.fc3(x),dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C1L3WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 320)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = (14-3+1)/3\n",
    "int(after), int(2*after**2*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C1L5WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(4**2*10, 20)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*10, 160)\n",
    "        self.fc2 = nn.Linear(160, 80)\n",
    "        self.fc3 = nn.Linear(80,40)\n",
    "        self.fc4 = nn.Linear(40,20)\n",
    "        self.fc5 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=3, stride=3))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=3, stride=3))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*10)\n",
    "        x2 = x2.view(-1, 4**2*10)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x.view(-1, 4**2*10)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        output = F.softmax(self.fc5(x),dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C1L3WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 960)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = 4\n",
    "int(after), int(2*2**2*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C2L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images \n",
    "        self.conv1 = nn.Conv2d(1, 24, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(24, 120, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(2**2*120, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*2**2*120, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=2, stride=2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv2(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv2(x2), kernel_size=2, stride=2))\n",
    "\n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 2**2*120)\n",
    "        x2 = x2.view(-1, 2**2*120)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x.view(-1, 2*2**2*120)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C2L2WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = 4\n",
    "int(after), int(2*2**2*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C2L3WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(16, 64, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(2**2*64, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*2**2*64, 60)\n",
    "        self.fc2 = nn.Linear(60, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=2, stride=2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv2(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv2(x2), kernel_size=2, stride=2))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 2**2*64)\n",
    "        x2 = x2.view(-1, 2**2*64)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = F.softmax(self.fc3(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C2L3WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1408)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = 4\n",
    "int(after), int(2*4**2*44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C3L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 11, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(11, 22, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(22, 44, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(4**2*44, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*44, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution\n",
    "        x1 = F.relu(self.conv1(picture1))\n",
    "        x2 = F.relu(self.conv1(picture2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(self.conv2(x1))\n",
    "        x2 = F.relu(self.conv2(x2))\n",
    "        #third convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv3(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv3(x2), kernel_size=2, stride=2))\n",
    "        \n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*44)\n",
    "        x2 = x2.view(-1, 4**2*44)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C3L2WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "##########################################################################################################################\n",
    "#NOW done for those below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C4L2WS_bis(nn.Module):#bis got changed\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(6, 12, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(12, 24, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(24, 48, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(3**2*48, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(864, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution        \n",
    "        x1 = F.relu(self.conv1(picture1))\n",
    "        x2 = F.relu(self.conv1(picture2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(self.conv2(x1))\n",
    "        x2 = F.relu(self.conv2(x2))\n",
    "        #third convolution\n",
    "        x1 = F.relu(self.conv3(x1))\n",
    "        x2 = F.relu(self.conv3(x2))\n",
    "        #fourth convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv4(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv4(x2), kernel_size=2, stride=2))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 3**2*48)\n",
    "        x2 = x2.view(-1, 3**2*48)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C4L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=2)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(3**2*64, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(1152, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution        \n",
    "        x1 = F.relu(self.conv1(picture1))\n",
    "        x2 = F.relu(self.conv1(picture2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv2(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv2(x2), kernel_size=2, stride=2))\n",
    "        #third convolution\n",
    "        x1 = F.relu(self.conv3(x1))\n",
    "        x2 = F.relu(self.conv3(x2))\n",
    "        #fourth convolution\n",
    "        x1 = F.relu(self.conv4(x1))\n",
    "        x2 = F.relu(self.conv4(x2))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 3**2*64)\n",
    "        x2 = x2.view(-1, 3**2*64)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84312\n"
     ]
    }
   ],
   "source": [
    "m=C1L2WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71016\n"
     ]
    }
   ],
   "source": [
    "m=C1L3WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68862\n"
     ]
    }
   ],
   "source": [
    "m=C1L5WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50352\n"
     ]
    }
   ],
   "source": [
    "m=C2L2WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44052\n"
     ]
    }
   ],
   "source": [
    "m=C2L3WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46338\n"
     ]
    }
   ],
   "source": [
    "m=C3L2WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35424\n"
     ]
    }
   ],
   "source": [
    "m=C4L2WS_bis()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40456\n"
     ]
    }
   ],
   "source": [
    "m=C4L2WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 14.80% 148/1000\n",
      "test error Net 12.10% 121/1000\n",
      "test error Net 15.60% 156/1000\n",
      "Average error: 14.166666666666666\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=C2L2WS\n",
    "run_many_times(model,crit=nn.CrossEntropyLoss,mini_batch_size=10,n=3,print_=True,eta=1e-3,nb_epochs=20,aux_factor=1,shuffle=True, store_error=False,checkpoint_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Net3328Aux20WS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-a2ccaa383b9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#To test the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mNet3328Aux20WS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNet512_128Aux20WS\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Net3328Aux20WS'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Net512_128Aux20WS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbig_error_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"TestPlot.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Net3328Aux20WS' is not defined"
     ]
    }
   ],
   "source": [
    "#To test the function\n",
    "models=[Net3328Aux20WS,Net512_128Aux20WS]\n",
    "model_names=['Net3328Aux20WS','Net512_128Aux20WS']\n",
    "big_error_plot(models,model_names,n=3,nb_epochs=10,div=10,name=\"TestPlot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[C1L2WS,C2L2WS,C2L3WS,C3L2WS,C4L2WS]\n",
    "model_names=['C1L2WS','C2L2WS','C2L3WS','C3L2WS','C4L2WS']\n",
    "big_error_plot(models,model_names,n=20,nb_epochs=20,name='big_plot_WS_aux=1.png',div=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as before with aux_factor=0.1\n",
    "models=[C1L2WS,C2L2WS,C2L3WS,C3L2WS,C4L2WS]\n",
    "model_names=['C1L2WS','C2L2WS','C2L3WS','C3L2WS','C4L2WS']\n",
    "big_error_plot(models,model_names,n=10,nb_epochs=20,aux_factor=0.1,name='big_plot_WS_aux=01.png',div=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as before with aux_factor=1\n",
    "models=[C1L2WS,C2L2WS,C2L3WS,C3L2WS,C4L2WS]\n",
    "model_names=['C1L2WS','C2L2WS','C2L3WS','C3L2WS','C4L2WS']\n",
    "big_error_plot(models,model_names,n=10,nb_epochs=20,aux_factor=1,name='big_plot_WS_aux=0.png',div=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tirocinio",
   "language": "python",
   "name": "tirocinio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
