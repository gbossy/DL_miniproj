{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import torch\n",
    "import various_data_functions\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from models import *\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "N=10**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base functions adapted from the practicals\n",
    "def train_model(model, train_input, train_target,train_classes, mini_batch_size, test_input=None, test_target=None, crit=nn.CrossEntropyLoss, eta = 1e-3, nb_epochs = 50,print_=False, store_loss = False, aux_factor=1, store_error=False, checkpoint_name=None):\n",
    "    #Initializing the loss, the optimizer, and the stored loss and errors for the plots\n",
    "    criterion = crit()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "    stored_loss = []\n",
    "    stored_error = []\n",
    "    \n",
    "    #Retrieving data if a similar model has already been trained or partially trained\n",
    "    nb_epochs_finished = 0\n",
    "    if checkpoint_name!=None:\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_name)\n",
    "            nb_epochs_finished = checkpoint['nb_epochs_finished']\n",
    "            model.load_state_dict(checkpoint['model_state'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "            if print_:\n",
    "                print(f'Checkpoint loaded with {nb_epochs_finished} epochs finished.')\n",
    "            stored_loss=checkpoint['stored_loss']\n",
    "            if len(stored_loss)>nb_epochs*3:\n",
    "                stored_loss=stored_loss[0:nb_epochs*3]\n",
    "            stored_error=checkpoint['stored_error']\n",
    "            if len(stored_error)>nb_epochs:\n",
    "                stored_error=stored_error[0:nb_epochs]\n",
    "        except FileNotFoundError:\n",
    "            if print_:\n",
    "                print('Starting from scratch.')\n",
    "        except:\n",
    "            print('Error when loading the checkpoint.')\n",
    "            exit(1)\n",
    "    \n",
    "    #Training the network if the checkpoint wasn't fully trained\n",
    "    for e in range(nb_epochs_finished,nb_epochs):\n",
    "        \n",
    "        #Initializing accumulated losses\n",
    "        #loss1 is the loss over the output (identifying which number is greater)\n",
    "        #loss2 and loss3 are the auxiliary losses trying to identify the numbers in the 2 input pictures\n",
    "        acc_loss = 0\n",
    "        acc_loss1 = 0\n",
    "        acc_loss2 = 0\n",
    "        acc_loss3 = 0\n",
    "        \n",
    "        #permuting the samples\n",
    "        permuted_index = torch.randperm(train_input.size()[0])\n",
    "        train_input_shuffled = train_input[permuted_index]\n",
    "        train_target_shuffled = train_target[permuted_index]\n",
    "        train_classes_shuffled = train_classes[permuted_index]\n",
    "        \n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            \n",
    "            #forward pass\n",
    "            output,aux_output = model(train_input_shuffled.narrow(0, b, mini_batch_size))\n",
    "            if crit==nn.MSELoss:\n",
    "                \n",
    "                #computing the different losses\n",
    "                loss1 = criterion(output[:,1], train_target_shuffled.narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(torch.argmax(aux_output[:,0:9],dim=1), train_classes_shuffled[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(torch.argmax(aux_output[:,10:19],dim=1), train_classes_shuffled[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                print('|| loss1 req grad =', loss1.requires_grad, '|| loss2 req grad =',loss2.requires_grad,'|| loss3 req grad =', loss3.requires_grad)\n",
    "            elif crit==nn.CrossEntropyLoss:\n",
    "                loss1 = criterion(output, train_target_shuffled.narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(aux_output[:,0,:], train_classes_shuffled[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(aux_output[:,1,:], train_classes_shuffled[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + aux_factor*(loss2 + loss3)\n",
    "            else:\n",
    "                print(\"Loss not implemented\")\n",
    "                \n",
    "            #Update the accumulated losses\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "            acc_loss1 = acc_loss1 + loss1.item()\n",
    "            acc_loss2 = acc_loss2 + loss2.item()\n",
    "            acc_loss3 = acc_loss3 + loss3.item()\n",
    "            \n",
    "            #zero the gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            #backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            #optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "        #update the stored losses and error if needed\n",
    "        if store_loss:\n",
    "            stored_loss += [[acc_loss1], [acc_loss2], [acc_loss3]]\n",
    "        if store_error:\n",
    "            stored_error +=[compute_nb_errors(model, test_input, test_target, mini_batch_size)]\n",
    "            \n",
    "        #print the different losses if needed\n",
    "        if print_:\n",
    "            print(e, 'tot loss', acc_loss, 'loss1', acc_loss1, 'loss2', acc_loss2, 'loss3', acc_loss3)\n",
    "            \n",
    "        #save the checkpoint for later if needed\n",
    "        if checkpoint_name!=None:\n",
    "                checkpoint = {'nb_epochs_finished': e + 1,'model_state': model.state_dict(),'optimizer_state': optimizer.state_dict(),'stored_loss':stored_loss,'stored_error':stored_error}\n",
    "                torch.save(checkpoint, checkpoint_name)\n",
    "        \n",
    "    #return the stored quantities  \n",
    "    return torch.tensor(stored_loss),torch.tensor(stored_error)\n",
    "\n",
    "#error computing function\n",
    "def compute_nb_errors(model, input, target, mini_batch_size=100):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output , aux_output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k]!=predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors\n",
    "\n",
    "#this function lets us do multiple runs more easily\n",
    "def run_many_times(model,crit=nn.CrossEntropyLoss,mini_batch_size=100,n=10,print_=True,eta=1e-3,nb_epochs=25,aux_factor=0,shuffle=True, store_error=False,checkpoint_name=None):\n",
    "    average_error=0\n",
    "    losses=torch.empty(0,nb_epochs,3)\n",
    "    errors=torch.empty(0,nb_epochs)\n",
    "    for i in range(n):\n",
    "        m=model()\n",
    "        train_input,train_target,train_classes,test_input,test_target,test_classes=various_data_functions.data(N,True,False,nn.CrossEntropyLoss,shuffle=shuffle)\n",
    "        if checkpoint_name!=None:\n",
    "            checkpoint_name_spec=checkpoint_name+'try_'+str(i)+'.pth'\n",
    "        else:\n",
    "            checkpoint_name_spec=None\n",
    "        new_losses,new_errors=train_model(m, train_input, train_target,train_classes,mini_batch_size,test_input=test_input, test_target=test_target,crit=crit,eta=eta,nb_epochs=nb_epochs,aux_factor=aux_factor,store_loss=True,store_error=store_error,checkpoint_name=checkpoint_name_spec)\n",
    "        new_losses=new_losses.view(1,nb_epochs, 3)\n",
    "        if store_error:\n",
    "            new_errors= new_errors.view(1,nb_epochs)\n",
    "            errors = torch.cat((errors,new_errors),0)\n",
    "        if print_:\n",
    "            losses = torch.cat((losses, new_losses), 0)\n",
    "            nb_test_errors = compute_nb_errors(m, test_input, test_target, mini_batch_size)\n",
    "            print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "            average_error+=(100 * nb_test_errors) / test_input.size(0)\n",
    "    if print_:\n",
    "        print(\"Average error: \"+str(average_error/n))\n",
    "        avg_losses=torch.sum(losses,0)/n\n",
    "        mod=int(torch.floor(torch.Tensor([nb_epochs/20])))\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True)\n",
    "        x_lab=torch.arange(nb_epochs)\n",
    "        x_labels=x_lab[x_lab%mod==0].detach().numpy()\n",
    "        ax0.errorbar(x_labels, avg_losses[x_lab%mod==0,0].detach().numpy(), yerr=torch.std(losses[:,:,0],0)[x_lab%mod==0].detach().numpy(), fmt='-o')\n",
    "        ax0.set_title('evolution of the cross entropy loss')\n",
    "        ax1.errorbar(x_labels, avg_losses[x_lab%mod==0,1].detach().numpy(), yerr=torch.std(losses[:,:,1],0)[x_lab%mod==0].detach().numpy(), fmt='o')\n",
    "        ax1.errorbar(x_labels, avg_losses[x_lab%mod==0,2].detach().numpy(), yerr=torch.std(losses[:,:,2],0)[x_lab%mod==0].detach().numpy(), fmt='o')\n",
    "        ax1.set_title('evolution of the auxiliary losses')\n",
    "        plt.show()\n",
    "    if store_error:\n",
    "        return errors\n",
    "\n",
    "#this function returns the number of parameters of a model\n",
    "def n_params(model):\n",
    "    n = 0\n",
    "    for params in model.parameters():\n",
    "        n += params.numel()\n",
    "    return n\n",
    "\n",
    "#this function uses the previous ones to build a big plot with several models\n",
    "def big_error_plot(models,model_names,n=50,nb_epochs=100,eta=1e-3,div=25,name=\"big_error_plot.png\",aux_factor=1):\n",
    "    x_lab=torch.arange(nb_epochs)\n",
    "    mod=int(torch.floor(torch.Tensor([nb_epochs/div])))\n",
    "    x_labels=x_lab[x_lab%mod==0].detach().numpy()\n",
    "    for i in range(len(models)):\n",
    "        print(\"Starting model \" + model_names[i])\n",
    "        errors=run_many_times(models[i],crit=nn.CrossEntropyLoss,mini_batch_size=10,n=n,print_=False,eta=eta,nb_epochs=nb_epochs,aux_factor=aux_factor,shuffle=True, store_error=True,checkpoint_name='checkpoints/'+model_names[i]+'_'+str(aux_factor).replace('.','')+'_')\n",
    "        #sns.relplot(x=\"timepoint\", y=\"signal\", col=\"region\",hue=\"event\", style=\"event\", kind=\"line\",)\n",
    "        #plt.errorbar(x_labels,(errors.mean(dim=0)[x_lab%mod==0]/10).detach().numpy(),yerr=(torch.std(errors,dim=0)[x_lab%mod==0]/10).detach().numpy(),fmt='o')\n",
    "        mean=(errors.mean(dim=0)[x_lab%mod==0]/10).detach().numpy()\n",
    "        std=(torch.std(errors,dim=0)[x_lab%mod==0]/10).detach().numpy()\n",
    "        plt.plot(x_labels,mean)\n",
    "        plt.fill_between(x_labels,mean-std,mean+std,alpha=0.3)\n",
    "        print('Mean number of errors after '+str(nb_epochs)+' epochs of training: '+str(errors.mean(dim=0)[-1])+' out of 1000 test samples with a standard deviation of '+str(torch.std(errors,dim=0)[-1]))\n",
    "    plt.ylabel('Accuracy[%]')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.legend(model_names)\n",
    "    plt.savefig(name,bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpuit images are 14*14 \n",
    "shuffle = True\n",
    "train_input,train_target,train_classes,test_input,test_target,test_classes=various_data_functions.data(N,True,False,nn.CrossEntropyLoss,shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3328)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after = (14-3+1)/3\n",
    "int(after), int(2*after**2*104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C1L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images \n",
    "        self.conv1 = nn.Conv2d(1, 104, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(4**2*104, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*104, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=3, stride=3))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=3, stride=3))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*104)\n",
    "        x2 = x2.view(-1, 4**2*104)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0698, 0.0731, 0.1361, 0.0938, 0.0797, 0.1770, 0.1022, 0.0635,\n",
      "          0.1380, 0.0667],\n",
      "         [0.0844, 0.0536, 0.1123, 0.1097, 0.0854, 0.1795, 0.0989, 0.0756,\n",
      "          0.1353, 0.0655]],\n",
      "\n",
      "        [[0.0677, 0.0425, 0.0889, 0.0932, 0.0734, 0.2114, 0.1310, 0.0528,\n",
      "          0.1775, 0.0617],\n",
      "         [0.0825, 0.0699, 0.1638, 0.0956, 0.0639, 0.2206, 0.0880, 0.0504,\n",
      "          0.0697, 0.0954]],\n",
      "\n",
      "        [[0.0932, 0.0573, 0.1230, 0.1190, 0.0661, 0.1911, 0.0926, 0.0730,\n",
      "          0.0925, 0.0922],\n",
      "         [0.0885, 0.0453, 0.1253, 0.1192, 0.0364, 0.2807, 0.1315, 0.0359,\n",
      "          0.0754, 0.0618]],\n",
      "\n",
      "        [[0.0797, 0.0473, 0.1750, 0.1024, 0.0406, 0.2283, 0.1168, 0.0572,\n",
      "          0.0719, 0.0808],\n",
      "         [0.0937, 0.0557, 0.1163, 0.1096, 0.0815, 0.1789, 0.0925, 0.0561,\n",
      "          0.1359, 0.0798]],\n",
      "\n",
      "        [[0.0771, 0.0420, 0.1707, 0.1327, 0.0571, 0.2132, 0.0994, 0.0508,\n",
      "          0.0943, 0.0626],\n",
      "         [0.0569, 0.0431, 0.1285, 0.1271, 0.0660, 0.2283, 0.1159, 0.0498,\n",
      "          0.1073, 0.0771]]], grad_fn=<CatBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C1L2WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "print(aux_output)\n",
    "output.shape, aux_output.shape\n",
    "aux_output[4,1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = (14-3+1)/3\n",
    "int(after), int(2*after**2*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C1L3WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images\n",
    "        self.aux_linear = nn.Linear(4**2*16, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*16, 128)\n",
    "        self.fc2 = nn.Linear(128, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=3, stride=3))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=3, stride=3))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*16)\n",
    "        x2 = x2.view(-1, 4**2*16)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = F.softmax(self.fc3(x),dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C1L3WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 320)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = (14-3+1)/3\n",
    "int(after), int(2*after**2*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C1L5WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(4**2*10, 20)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*10, 160)\n",
    "        self.fc2 = nn.Linear(160, 80)\n",
    "        self.fc3 = nn.Linear(80,40)\n",
    "        self.fc4 = nn.Linear(40,20)\n",
    "        self.fc5 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=3, stride=3))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=3, stride=3))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*10)\n",
    "        x2 = x2.view(-1, 4**2*10)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x.view(-1, 4**2*10)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        output = F.softmax(self.fc5(x),dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C1L3WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 960)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = 4\n",
    "int(after), int(2*2**2*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C2L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images \n",
    "        self.conv1 = nn.Conv2d(1, 24, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(24, 120, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(2**2*120, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*2**2*120, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=2, stride=2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv2(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv2(x2), kernel_size=2, stride=2))\n",
    "\n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 2**2*120)\n",
    "        x2 = x2.view(-1, 2**2*120)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x.view(-1, 2*2**2*120)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C2L2WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = 4\n",
    "int(after), int(2*2**2*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C2L3WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(16, 64, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(2**2*64, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*2**2*64, 60)\n",
    "        self.fc2 = nn.Linear(60, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv1(picture1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv1(picture2), kernel_size=2, stride=2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv2(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv2(x2), kernel_size=2, stride=2))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 2**2*64)\n",
    "        x2 = x2.view(-1, 2**2*64)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        output = F.softmax(self.fc3(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C2L3WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1408)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################################################\n",
    "#check the sizes\n",
    "after = 4\n",
    "int(after), int(2*4**2*44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C3L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 11, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(11, 22, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(22, 44, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(4**2*44, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(2*4**2*44, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution\n",
    "        x1 = F.relu(self.conv1(picture1))\n",
    "        x2 = F.relu(self.conv1(picture2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(self.conv2(x1))\n",
    "        x2 = F.relu(self.conv2(x2))\n",
    "        #third convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv3(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv3(x2), kernel_size=2, stride=2))\n",
    "        \n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 4**2*44)\n",
    "        x2 = x2.view(-1, 4**2*44)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]), torch.Size([5, 2, 10]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checks\n",
    "model = C3L2WS()\n",
    "x = train_input[0:5]\n",
    "output, aux_output = model(x)\n",
    "output.shape, aux_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################\n",
    "##########################################################################################################################\n",
    "#NOW done for those below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C4L2WS_bis(nn.Module):#bis got changed\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(6, 12, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(12, 24, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(24, 48, kernel_size=3)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(3**2*48, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(864, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution        \n",
    "        x1 = F.relu(self.conv1(picture1))\n",
    "        x2 = F.relu(self.conv1(picture2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(self.conv2(x1))\n",
    "        x2 = F.relu(self.conv2(x2))\n",
    "        #third convolution\n",
    "        x1 = F.relu(self.conv3(x1))\n",
    "        x2 = F.relu(self.conv3(x2))\n",
    "        #fourth convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv4(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv4(x2), kernel_size=2, stride=2))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 3**2*48)\n",
    "        x2 = x2.view(-1, 3**2*48)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C4L2WS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #convolutions. To be run in parallel on the 2 images\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=2)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        \n",
    "        #linear layer for the auxiliary losses. To be run in parallel on the 2 images \n",
    "        self.aux_linear = nn.Linear(3**2*64, 10)\n",
    "        \n",
    "        #The outputs of the convolutions will be merged in a single vector\n",
    "        #Then, the following linear layers for the real loss will be used.\n",
    "        self.fc1 = nn.Linear(1152, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #crop the images in 2\n",
    "        picture1 =x.narrow(1, 0, 1)\n",
    "        picture2 =x.narrow(1, 1, 1)\n",
    "        \n",
    "        #computing the convolutions+relu in parallel on the two pictures.\n",
    "        #first convolution        \n",
    "        x1 = F.relu(self.conv1(picture1))\n",
    "        x2 = F.relu(self.conv1(picture2))\n",
    "        #second convolution\n",
    "        x1 = F.relu(F.max_pool2d(self.conv2(x1), kernel_size=2, stride=2))\n",
    "        x2 = F.relu(F.max_pool2d(self.conv2(x2), kernel_size=2, stride=2))\n",
    "        #third convolution\n",
    "        x1 = F.relu(self.conv3(x1))\n",
    "        x2 = F.relu(self.conv3(x2))\n",
    "        #fourth convolution\n",
    "        x1 = F.relu(self.conv4(x1))\n",
    "        x2 = F.relu(self.conv4(x2))\n",
    "        \n",
    "        #Reshaping to make 2 vectors\n",
    "        x1 = x1.view(-1, 3**2*64)\n",
    "        x2 = x2.view(-1, 3**2*64)\n",
    "        \n",
    "        #computing the linear+softmax to make the auxiliary output\n",
    "        a1 = F.softmax(self.aux_linear(x1), dim = 1).unsqueeze(1)\n",
    "        a2 = F.softmax(self.aux_linear(x2), dim = 1).unsqueeze(1)\n",
    "        #merging the vector to return the final auxiliary output\n",
    "        aux_output = torch.cat((a1, a2), 1)\n",
    "        \n",
    "        #computing the linear layers for the final output\n",
    "        x = torch.cat((x1,x2), 1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        return output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84312\n"
     ]
    }
   ],
   "source": [
    "m=C1L2WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71016\n"
     ]
    }
   ],
   "source": [
    "m=C1L3WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68862\n"
     ]
    }
   ],
   "source": [
    "m=C1L5WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50352\n"
     ]
    }
   ],
   "source": [
    "m=C2L2WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44052\n"
     ]
    }
   ],
   "source": [
    "m=C2L3WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46338\n"
     ]
    }
   ],
   "source": [
    "m=C3L2WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31094\n"
     ]
    }
   ],
   "source": [
    "m=C4L2WS_bis()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34686\n"
     ]
    }
   ],
   "source": [
    "m=C4L2WS()\n",
    "print(n_params(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 15.70% 157/1000\n",
      "test error Net 16.60% 166/1000\n",
      "test error Net 15.40% 154/1000\n",
      "Average error: 15.899999999999999\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=C2L2WS\n",
    "run_many_times(model,crit=nn.CrossEntropyLoss,mini_batch_size=10,n=3,print_=True,eta=1e-3,nb_epochs=20,aux_factor=1,shuffle=True, store_error=False,checkpoint_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To test the function\n",
    "models=[Net3328Aux20WS,Net512_128Aux20WS]\n",
    "model_names=['Net3328Aux20WS','Net512_128Aux20WS']\n",
    "big_error_plot(models,model_names,n=3,nb_epochs=10,div=10,name=\"TestPlot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model C1L2WS\n"
     ]
    }
   ],
   "source": [
    "models=[C1L2WS,C2L2WS,C2L3WS,C3L2WS,C4L2WS]\n",
    "model_names=['C1L2WS','C2L2WS','C2L3WS','C3L2WS','C4L2WS']\n",
    "big_error_plot(models,model_names,n=20,nb_epochs=20,name='big_plot_WS_aux=1.png',div=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as before with aux_factor=0.1\n",
    "models=[C1L2WS,C2L2WS,C2L3WS,C3L2WS,C4L2WS]\n",
    "model_names=['C1L2WS','C2L2WS','C2L3WS','C3L2WS','C4L2WS']\n",
    "big_error_plot(models,model_names,n=10,nb_epochs=20,aux_factor=0.1,name='big_plot_WS_aux=01.png',div=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as before with aux_factor=1\n",
    "models=[C1L2WS,C2L2WS,C2L3WS,C3L2WS,C4L2WS]\n",
    "model_names=['C1L2WS','C2L2WS','C2L3WS','C3L2WS','C4L2WS']\n",
    "big_error_plot(models,model_names,n=10,nb_epochs=20,aux_factor=1,name='big_plot_WS_aux=0.png',div=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
