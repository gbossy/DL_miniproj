{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generation\n",
    "N=10**3\n",
    "train_input,train_target,train_classes,test_input,test_target,test_classes=prologue.generate_pair_sets(N)\n",
    "train_target=train_target.long()#.float for MSELoss, .long for CrossEntropy\n",
    "train_input=train_input.float()\n",
    "train_classes=train_classes.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base functions adapted from the practicals\n",
    "def train_model(model, train_input, train_target,train_classes, mini_batch_size, crit=nn.MSELoss, eta = 1e-3, nb_epochs = 250,print_=False):\n",
    "    criterion = crit()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = eta)\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "        acc_loss1 = 0\n",
    "        acc_loss2 = 0\n",
    "        acc_loss3 = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output,aux_output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            if crit==nn.MSELoss:\n",
    "                loss1 = criterion(output[:,1], train_target.narrow(0, b, mini_batch_size))\n",
    "                #print(torch.argmax(aux_output[:,0:9],dim=1))\n",
    "                #print(train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(torch.argmax(aux_output[:,0:9],dim=1), train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(torch.argmax(aux_output[:,10:19],dim=1), train_classes[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                print('|| loss1 req grad =', loss1.requires_grad, '|| loss2 req grad =',loss2.requires_grad,'|| loss3 req grad =', loss3.requires_grad)\n",
    "            elif crit==nn.CrossEntropyLoss:\n",
    "                loss1 = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "                #print(torch.argmax(aux_output[:,0:9],dim=1))\n",
    "                #print(train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(aux_output[:,:10], train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(aux_output[:,10:], train_classes[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 #+ 0.1*(loss2 + loss3)\n",
    "                #print(loss1, loss2.requires_grad, loss3.requires_grad)\n",
    "            else:\n",
    "                print(\"Loss not implemented\")\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "            acc_loss1 = acc_loss1 + loss1.item()\n",
    "            acc_loss2 = acc_loss2 + loss2.item()\n",
    "            acc_loss3 = acc_loss3 + loss3.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if False:\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p -= eta * p.grad\n",
    "\n",
    "        print(e, 'tot loss', acc_loss, 'loss1', acc_loss1, 'loss2', acc_loss2, 'loss3', acc_loss3)\n",
    "            \n",
    "def compute_nb_errors(model, input, target, mini_batch_size=100):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output , aux_output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k]!=predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors\n",
    "\n",
    "def run_many_times(model,crit=nn.MSELoss,mini_batch_size=100,n=10,print_=False):\n",
    "    average_error=0\n",
    "    for i in range(n):\n",
    "        m=model()\n",
    "        train_model(m, train_input, train_target,train_classes,mini_batch_size,crit=crit)\n",
    "        nb_test_errors = compute_nb_errors(m, test_input, test_target, mini_batch_size)\n",
    "        print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "        average_error+=(100 * nb_test_errors) / test_input.size(0)\n",
    "    print(\"Average error: \"+str(average_error/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is it better to use groups or not?\n",
    "#Takes about 2 hours to run\n",
    "#about 22.5% error average without groups if we exclude outliers that get stuck and don't move\n",
    "#about 21.5% error average with groups if we exclude outliers that get stuck and don't move\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 100, kernel_size=3,groups=2)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(1600, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        aux_output = F.softmax(self.fc1(x.view(-1, 1600)), dim=1)\n",
    "        x = F.relu(self.fc1(x.view(-1, 1600)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        aux_output = F.softmax(x, dim=1)\n",
    "        #print(x)\n",
    "        return output, aux_output\n",
    "    \n",
    "    def last_hiddes(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 1600)))\n",
    "        return x\n",
    "\n",
    "class NetGroups(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, groups=2)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(512, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 512)))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        #print(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Net()\n",
    "mini_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# If you try to run the train using MSELoss like you were doing, you will get that loss2 and loss3 \n",
    "# requires_grad = False\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "0 tot loss 355.62580490112305 loss1 4.455808699131012 loss2 243.2999973297119 loss3 107.86999988555908\n"
     ]
    }
   ],
   "source": [
    "train_target = train_target.float()\n",
    "train_classes = train_classes.float()\n",
    "crit = nn.MSELoss\n",
    "\n",
    "train_model(m, train_input, train_target, train_classes,mini_batch_size, crit, nb_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "# This is because you used a sclar target for the 10 classes, using argmax before calling the loss\n",
    "# Since argmax is a non differentiable function it sets the req. grad. of the result to False\n",
    "# Example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, True, False)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(5., requires_grad = True)\n",
    "b = torch.argmax(a)\n",
    "a.dtype, a.requires_grad, b.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "# If you don't want to use hot label embedding you need to use the Cross Entropy instead. \n",
    "# I wrote some code in the training function and it seems to be working\n",
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tot loss 7.083635568618774 loss1 7.083635568618774 loss2 23.50584363937378 loss3 23.36737632751465\n",
      "1 tot loss 6.5514044761657715 loss1 6.5514044761657715 loss2 23.662835121154785 loss3 23.397489309310913\n",
      "2 tot loss 6.548925280570984 loss1 6.548925280570984 loss2 23.726492643356323 loss3 23.351585626602173\n",
      "3 tot loss 6.506751835346222 loss1 6.506751835346222 loss2 23.75475788116455 loss3 23.20123839378357\n",
      "4 tot loss 6.376391887664795 loss1 6.376391887664795 loss2 23.70777940750122 loss3 23.31951332092285\n",
      "5 tot loss 6.26804780960083 loss1 6.26804780960083 loss2 23.748292684555054 loss3 23.332770109176636\n",
      "6 tot loss 6.0114635825157166 loss1 6.0114635825157166 loss2 23.88239622116089 loss3 23.285412073135376\n",
      "7 tot loss 5.968484044075012 loss1 5.968484044075012 loss2 23.83894991874695 loss3 23.31095242500305\n",
      "8 tot loss 5.975208044052124 loss1 5.975208044052124 loss2 23.698276042938232 loss3 23.47767210006714\n",
      "9 tot loss 5.887198984622955 loss1 5.887198984622955 loss2 23.792410612106323 loss3 23.408872842788696\n",
      "10 tot loss 5.818183660507202 loss1 5.818183660507202 loss2 23.912766218185425 loss3 23.28335213661194\n",
      "11 tot loss 5.717447996139526 loss1 5.717447996139526 loss2 23.865644216537476 loss3 23.249180555343628\n",
      "12 tot loss 5.770739853382111 loss1 5.770739853382111 loss2 23.880776405334473 loss3 23.27290987968445\n",
      "13 tot loss 5.836733937263489 loss1 5.836733937263489 loss2 23.796602725982666 loss3 23.328974962234497\n",
      "14 tot loss 5.671996504068375 loss1 5.671996504068375 loss2 23.76946997642517 loss3 23.317390203475952\n",
      "15 tot loss 5.534943580627441 loss1 5.534943580627441 loss2 23.833256721496582 loss3 23.304950952529907\n",
      "16 tot loss 5.862990736961365 loss1 5.862990736961365 loss2 23.836573600769043 loss3 23.263898611068726\n",
      "17 tot loss 5.522710412740707 loss1 5.522710412740707 loss2 23.86649465560913 loss3 23.25887942314148\n",
      "18 tot loss 5.747807502746582 loss1 5.747807502746582 loss2 23.877207279205322 loss3 23.27431845664978\n",
      "19 tot loss 6.132329523563385 loss1 6.132329523563385 loss2 23.95064377784729 loss3 23.190651178359985\n",
      "20 tot loss 5.484556317329407 loss1 5.484556317329407 loss2 23.818523645401 loss3 23.297709941864014\n",
      "21 tot loss 5.617119312286377 loss1 5.617119312286377 loss2 23.72809338569641 loss3 23.321261167526245\n",
      "22 tot loss 5.375796765089035 loss1 5.375796765089035 loss2 23.671329736709595 loss3 23.332298040390015\n",
      "23 tot loss 5.5097209215164185 loss1 5.5097209215164185 loss2 23.58414125442505 loss3 23.42321801185608\n",
      "24 tot loss 5.463131755590439 loss1 5.463131755590439 loss2 23.63533854484558 loss3 23.417184352874756\n",
      "25 tot loss 5.681851297616959 loss1 5.681851297616959 loss2 23.740718126296997 loss3 23.30943465232849\n",
      "26 tot loss 5.250855416059494 loss1 5.250855416059494 loss2 23.608804941177368 loss3 23.419442415237427\n",
      "27 tot loss 5.298988372087479 loss1 5.298988372087479 loss2 23.560394525527954 loss3 23.42173194885254\n",
      "28 tot loss 5.3747788071632385 loss1 5.3747788071632385 loss2 23.560790300369263 loss3 23.401265144348145\n",
      "29 tot loss 5.424619197845459 loss1 5.424619197845459 loss2 23.516749620437622 loss3 23.443240642547607\n",
      "30 tot loss 5.34205961227417 loss1 5.34205961227417 loss2 23.516952753067017 loss3 23.402481079101562\n",
      "31 tot loss 5.138027727603912 loss1 5.138027727603912 loss2 23.54960560798645 loss3 23.340226650238037\n",
      "32 tot loss 5.063206881284714 loss1 5.063206881284714 loss2 23.507275104522705 loss3 23.389853715896606\n",
      "33 tot loss 5.11168697476387 loss1 5.11168697476387 loss2 23.55649161338806 loss3 23.381996870040894\n",
      "34 tot loss 5.101875424385071 loss1 5.101875424385071 loss2 23.656005382537842 loss3 23.32791304588318\n",
      "35 tot loss 5.199523508548737 loss1 5.199523508548737 loss2 23.625297784805298 loss3 23.35396385192871\n",
      "36 tot loss 5.337526381015778 loss1 5.337526381015778 loss2 23.507968425750732 loss3 23.403257608413696\n",
      "37 tot loss 5.031222015619278 loss1 5.031222015619278 loss2 23.54548692703247 loss3 23.359689235687256\n",
      "38 tot loss 5.050979524850845 loss1 5.050979524850845 loss2 23.59199047088623 loss3 23.29280400276184\n",
      "39 tot loss 5.01492041349411 loss1 5.01492041349411 loss2 23.57344365119934 loss3 23.33400535583496\n",
      "40 tot loss 5.109204530715942 loss1 5.109204530715942 loss2 23.615586280822754 loss3 23.265214204788208\n",
      "41 tot loss 4.922139137983322 loss1 4.922139137983322 loss2 23.581525325775146 loss3 23.290011644363403\n",
      "42 tot loss 4.821945250034332 loss1 4.821945250034332 loss2 23.54842448234558 loss3 23.345930576324463\n",
      "43 tot loss 5.071501165628433 loss1 5.071501165628433 loss2 23.52414631843567 loss3 23.3412823677063\n",
      "44 tot loss 4.989354163408279 loss1 4.989354163408279 loss2 23.550054788589478 loss3 23.337239265441895\n",
      "45 tot loss 4.81417191028595 loss1 4.81417191028595 loss2 23.625993967056274 loss3 23.262921810150146\n",
      "46 tot loss 4.714239835739136 loss1 4.714239835739136 loss2 23.527859687805176 loss3 23.345200300216675\n",
      "47 tot loss 4.870841592550278 loss1 4.870841592550278 loss2 23.520463705062866 loss3 23.347381353378296\n",
      "48 tot loss 4.838493138551712 loss1 4.838493138551712 loss2 23.611696243286133 loss3 23.33892822265625\n",
      "49 tot loss 4.834801197052002 loss1 4.834801197052002 loss2 23.474143981933594 loss3 23.375590801239014\n",
      "50 tot loss 4.866670101881027 loss1 4.866670101881027 loss2 23.522714853286743 loss3 23.349164962768555\n",
      "51 tot loss 4.867286652326584 loss1 4.867286652326584 loss2 23.559669494628906 loss3 23.29603624343872\n",
      "52 tot loss 4.889095455408096 loss1 4.889095455408096 loss2 23.43284821510315 loss3 23.4019672870636\n",
      "53 tot loss 4.888128936290741 loss1 4.888128936290741 loss2 23.552072048187256 loss3 23.363502979278564\n",
      "54 tot loss 4.833520799875259 loss1 4.833520799875259 loss2 23.547138929367065 loss3 23.40744113922119\n",
      "55 tot loss 4.923960208892822 loss1 4.923960208892822 loss2 23.56635808944702 loss3 23.364471673965454\n",
      "56 tot loss 4.572298914194107 loss1 4.572298914194107 loss2 23.531566381454468 loss3 23.355408906936646\n",
      "57 tot loss 4.8482431173324585 loss1 4.8482431173324585 loss2 23.530675649642944 loss3 23.345738887786865\n",
      "58 tot loss 4.901133596897125 loss1 4.901133596897125 loss2 23.629191160202026 loss3 23.286773681640625\n",
      "59 tot loss 4.577176779508591 loss1 4.577176779508591 loss2 23.59003758430481 loss3 23.321200847625732\n",
      "60 tot loss 4.601182788610458 loss1 4.601182788610458 loss2 23.616557121276855 loss3 23.29292130470276\n",
      "61 tot loss 4.592912048101425 loss1 4.592912048101425 loss2 23.52515959739685 loss3 23.364120960235596\n",
      "62 tot loss 4.628192037343979 loss1 4.628192037343979 loss2 23.49439311027527 loss3 23.39379906654358\n",
      "63 tot loss 4.664581269025803 loss1 4.664581269025803 loss2 23.542638778686523 loss3 23.39190173149109\n",
      "64 tot loss 4.491991579532623 loss1 4.491991579532623 loss2 23.440723657608032 loss3 23.453871488571167\n",
      "65 tot loss 4.77123436331749 loss1 4.77123436331749 loss2 23.585191249847412 loss3 23.367111444473267\n",
      "66 tot loss 4.407559752464294 loss1 4.407559752464294 loss2 23.51604413986206 loss3 23.413992404937744\n",
      "67 tot loss 4.7396528124809265 loss1 4.7396528124809265 loss2 23.501084566116333 loss3 23.38214874267578\n",
      "68 tot loss 4.547340631484985 loss1 4.547340631484985 loss2 23.60312533378601 loss3 23.335228443145752\n",
      "69 tot loss 4.750466912984848 loss1 4.750466912984848 loss2 23.49702763557434 loss3 23.411691904067993\n",
      "70 tot loss 4.676489502191544 loss1 4.676489502191544 loss2 23.542805433273315 loss3 23.370437622070312\n",
      "71 tot loss 4.64688241481781 loss1 4.64688241481781 loss2 23.52231740951538 loss3 23.3843731880188\n",
      "72 tot loss 4.724353313446045 loss1 4.724353313446045 loss2 23.58268451690674 loss3 23.355695247650146\n",
      "73 tot loss 4.412421613931656 loss1 4.412421613931656 loss2 23.64116334915161 loss3 23.28970456123352\n",
      "74 tot loss 4.569411873817444 loss1 4.569411873817444 loss2 23.558431148529053 loss3 23.318262815475464\n",
      "75 tot loss 4.4133341908454895 loss1 4.4133341908454895 loss2 23.537391185760498 loss3 23.34050726890564\n",
      "76 tot loss 4.365880012512207 loss1 4.365880012512207 loss2 23.564077615737915 loss3 23.319315433502197\n",
      "77 tot loss 4.482138454914093 loss1 4.482138454914093 loss2 23.525537967681885 loss3 23.2993803024292\n",
      "78 tot loss 4.621700286865234 loss1 4.621700286865234 loss2 23.628793001174927 loss3 23.29387068748474\n",
      "79 tot loss 4.585048824548721 loss1 4.585048824548721 loss2 23.572837591171265 loss3 23.311352491378784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 tot loss 4.432866990566254 loss1 4.432866990566254 loss2 23.47458815574646 loss3 23.32164168357849\n",
      "81 tot loss 4.276668727397919 loss1 4.276668727397919 loss2 23.566107273101807 loss3 23.324901819229126\n",
      "82 tot loss 4.64102166891098 loss1 4.64102166891098 loss2 23.529914140701294 loss3 23.309309482574463\n",
      "83 tot loss 4.436985820531845 loss1 4.436985820531845 loss2 23.55938458442688 loss3 23.336621284484863\n",
      "84 tot loss 4.5342302322387695 loss1 4.5342302322387695 loss2 23.64218258857727 loss3 23.26150369644165\n",
      "85 tot loss 4.42035984992981 loss1 4.42035984992981 loss2 23.59520387649536 loss3 23.237106323242188\n",
      "86 tot loss 4.414541035890579 loss1 4.414541035890579 loss2 23.523736715316772 loss3 23.278398990631104\n",
      "87 tot loss 4.315794736146927 loss1 4.315794736146927 loss2 23.567427396774292 loss3 23.26348853111267\n",
      "88 tot loss 4.471444934606552 loss1 4.471444934606552 loss2 23.536428928375244 loss3 23.321644067764282\n",
      "89 tot loss 4.3768496215343475 loss1 4.3768496215343475 loss2 23.571139097213745 loss3 23.343209266662598\n",
      "90 tot loss 4.333393543958664 loss1 4.333393543958664 loss2 23.558363437652588 loss3 23.345686197280884\n",
      "91 tot loss 4.108390539884567 loss1 4.108390539884567 loss2 23.546385288238525 loss3 23.3099946975708\n",
      "92 tot loss 4.087852716445923 loss1 4.087852716445923 loss2 23.560467004776 loss3 23.29345965385437\n",
      "93 tot loss 4.66539466381073 loss1 4.66539466381073 loss2 23.539596796035767 loss3 23.27055263519287\n",
      "94 tot loss 4.086523413658142 loss1 4.086523413658142 loss2 23.633256435394287 loss3 23.237809896469116\n",
      "95 tot loss 4.40271982550621 loss1 4.40271982550621 loss2 23.535062074661255 loss3 23.29451298713684\n",
      "96 tot loss 4.737304240465164 loss1 4.737304240465164 loss2 23.605122327804565 loss3 23.30376696586609\n",
      "97 tot loss 4.222440958023071 loss1 4.222440958023071 loss2 23.599295139312744 loss3 23.3065083026886\n",
      "98 tot loss 4.346641004085541 loss1 4.346641004085541 loss2 23.55464744567871 loss3 23.380038022994995\n",
      "99 tot loss 4.389391541481018 loss1 4.389391541481018 loss2 23.537060499191284 loss3 23.342618227005005\n",
      "100 tot loss 4.151697754859924 loss1 4.151697754859924 loss2 23.63678216934204 loss3 23.288819313049316\n",
      "101 tot loss 4.446141183376312 loss1 4.446141183376312 loss2 23.617162704467773 loss3 23.261396646499634\n",
      "102 tot loss 4.263746649026871 loss1 4.263746649026871 loss2 23.597468376159668 loss3 23.28458046913147\n",
      "103 tot loss 4.1462900042533875 loss1 4.1462900042533875 loss2 23.595341682434082 loss3 23.24804663658142\n",
      "104 tot loss 4.500595360994339 loss1 4.500595360994339 loss2 23.64387559890747 loss3 23.238242864608765\n",
      "105 tot loss 4.176448971033096 loss1 4.176448971033096 loss2 23.585277795791626 loss3 23.221933603286743\n",
      "106 tot loss 4.142892509698868 loss1 4.142892509698868 loss2 23.64161205291748 loss3 23.215036869049072\n",
      "107 tot loss 4.170226097106934 loss1 4.170226097106934 loss2 23.596715688705444 loss3 23.254387378692627\n",
      "108 tot loss 4.104901909828186 loss1 4.104901909828186 loss2 23.59672522544861 loss3 23.32464361190796\n",
      "109 tot loss 4.095531195402145 loss1 4.095531195402145 loss2 23.58219075202942 loss3 23.300411462783813\n",
      "110 tot loss 4.009259730577469 loss1 4.009259730577469 loss2 23.65815806388855 loss3 23.257988214492798\n",
      "111 tot loss 3.9548314213752747 loss1 3.9548314213752747 loss2 23.634317636489868 loss3 23.24914050102234\n",
      "112 tot loss 4.442271202802658 loss1 4.442271202802658 loss2 23.58645534515381 loss3 23.27302885055542\n",
      "113 tot loss 4.189525842666626 loss1 4.189525842666626 loss2 23.678674459457397 loss3 23.23446273803711\n",
      "114 tot loss 4.190687954425812 loss1 4.190687954425812 loss2 23.666045427322388 loss3 23.254729986190796\n",
      "115 tot loss 3.9529890418052673 loss1 3.9529890418052673 loss2 23.625369548797607 loss3 23.260054349899292\n",
      "116 tot loss 4.37156417965889 loss1 4.37156417965889 loss2 23.6832172870636 loss3 23.24087882041931\n",
      "117 tot loss 4.20513978600502 loss1 4.20513978600502 loss2 23.660967111587524 loss3 23.266493558883667\n",
      "118 tot loss 4.127451092004776 loss1 4.127451092004776 loss2 23.62794852256775 loss3 23.335843801498413\n",
      "119 tot loss 4.209297269582748 loss1 4.209297269582748 loss2 23.664016485214233 loss3 23.296151876449585\n",
      "120 tot loss 4.08244401216507 loss1 4.08244401216507 loss2 23.616949796676636 loss3 23.28969717025757\n",
      "121 tot loss 3.9875345826148987 loss1 3.9875345826148987 loss2 23.638484716415405 loss3 23.297715425491333\n",
      "122 tot loss 4.243438303470612 loss1 4.243438303470612 loss2 23.63684606552124 loss3 23.32389521598816\n",
      "123 tot loss 4.190163642168045 loss1 4.190163642168045 loss2 23.6421856880188 loss3 23.309108018875122\n",
      "124 tot loss 3.923222005367279 loss1 3.923222005367279 loss2 23.597222089767456 loss3 23.28586769104004\n",
      "125 tot loss 4.190154999494553 loss1 4.190154999494553 loss2 23.62072515487671 loss3 23.284865379333496\n",
      "126 tot loss 3.9523784816265106 loss1 3.9523784816265106 loss2 23.665064811706543 loss3 23.25759220123291\n",
      "127 tot loss 4.006471812725067 loss1 4.006471812725067 loss2 23.63267207145691 loss3 23.261114835739136\n",
      "128 tot loss 3.8131197690963745 loss1 3.8131197690963745 loss2 23.61589527130127 loss3 23.27971363067627\n",
      "129 tot loss 3.8424116671085358 loss1 3.8424116671085358 loss2 23.62347388267517 loss3 23.298028707504272\n",
      "130 tot loss 4.181211978197098 loss1 4.181211978197098 loss2 23.636131048202515 loss3 23.296570301055908\n",
      "131 tot loss 4.0586996376514435 loss1 4.0586996376514435 loss2 23.624307870864868 loss3 23.306110858917236\n",
      "132 tot loss 3.794191747903824 loss1 3.794191747903824 loss2 23.62217950820923 loss3 23.282211780548096\n",
      "133 tot loss 3.9069707691669464 loss1 3.9069707691669464 loss2 23.61793851852417 loss3 23.28108811378479\n",
      "134 tot loss 3.7689218521118164 loss1 3.7689218521118164 loss2 23.651487112045288 loss3 23.26491069793701\n",
      "135 tot loss 3.771053522825241 loss1 3.771053522825241 loss2 23.626116275787354 loss3 23.250937938690186\n",
      "136 tot loss 3.769577741622925 loss1 3.769577741622925 loss2 23.639402866363525 loss3 23.277965545654297\n",
      "137 tot loss 3.828300505876541 loss1 3.828300505876541 loss2 23.6026873588562 loss3 23.276216745376587\n",
      "138 tot loss 3.7769932746887207 loss1 3.7769932746887207 loss2 23.63056778907776 loss3 23.29296374320984\n",
      "139 tot loss 3.805533617734909 loss1 3.805533617734909 loss2 23.629550218582153 loss3 23.274877309799194\n",
      "140 tot loss 4.0378884971141815 loss1 4.0378884971141815 loss2 23.64142370223999 loss3 23.293630838394165\n",
      "141 tot loss 4.001656144857407 loss1 4.001656144857407 loss2 23.61500358581543 loss3 23.304823637008667\n",
      "142 tot loss 3.7378157675266266 loss1 3.7378157675266266 loss2 23.64983081817627 loss3 23.260138273239136\n",
      "143 tot loss 3.8031749427318573 loss1 3.8031749427318573 loss2 23.662928342819214 loss3 23.237012147903442\n",
      "144 tot loss 4.274014502763748 loss1 4.274014502763748 loss2 23.649000883102417 loss3 23.236289978027344\n",
      "145 tot loss 3.8548524379730225 loss1 3.8548524379730225 loss2 23.662994146347046 loss3 23.273311853408813\n",
      "146 tot loss 3.9141556322574615 loss1 3.9141556322574615 loss2 23.661514043807983 loss3 23.253400325775146\n",
      "147 tot loss 4.211356341838837 loss1 4.211356341838837 loss2 23.65235424041748 loss3 23.290518045425415\n",
      "148 tot loss 3.7405702769756317 loss1 3.7405702769756317 loss2 23.634077310562134 loss3 23.296922206878662\n",
      "149 tot loss 3.740392655134201 loss1 3.740392655134201 loss2 23.670129537582397 loss3 23.275956869125366\n",
      "150 tot loss 3.7038735151290894 loss1 3.7038735151290894 loss2 23.64954900741577 loss3 23.27485489845276\n",
      "151 tot loss 3.717823028564453 loss1 3.717823028564453 loss2 23.66070556640625 loss3 23.269588708877563\n",
      "152 tot loss 3.6969085335731506 loss1 3.6969085335731506 loss2 23.64783239364624 loss3 23.272802352905273\n",
      "153 tot loss 3.679989755153656 loss1 3.679989755153656 loss2 23.65782070159912 loss3 23.271111488342285\n",
      "154 tot loss 3.6719478368759155 loss1 3.6719478368759155 loss2 23.651071786880493 loss3 23.27323317527771\n",
      "155 tot loss 3.6607396602630615 loss1 3.6607396602630615 loss2 23.66140866279602 loss3 23.274738550186157\n",
      "156 tot loss 3.6609491109848022 loss1 3.6609491109848022 loss2 23.662816524505615 loss3 23.275653839111328\n",
      "157 tot loss 3.655146896839142 loss1 3.655146896839142 loss2 23.670602321624756 loss3 23.271634578704834\n",
      "158 tot loss 3.652653992176056 loss1 3.652653992176056 loss2 23.676187992095947 loss3 23.269381284713745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 tot loss 3.6513370871543884 loss1 3.6513370871543884 loss2 23.67773723602295 loss3 23.267898082733154\n",
      "160 tot loss 3.6502725780010223 loss1 3.6502725780010223 loss2 23.677562713623047 loss3 23.27153730392456\n",
      "161 tot loss 3.6490380465984344 loss1 3.6490380465984344 loss2 23.67766523361206 loss3 23.2720844745636\n",
      "162 tot loss 3.649061918258667 loss1 3.649061918258667 loss2 23.674086809158325 loss3 23.278908014297485\n",
      "163 tot loss 3.6484929025173187 loss1 3.6484929025173187 loss2 23.670122146606445 loss3 23.28499436378479\n",
      "164 tot loss 3.649860829114914 loss1 3.649860829114914 loss2 23.669405221939087 loss3 23.28862953186035\n",
      "165 tot loss 3.65177184343338 loss1 3.65177184343338 loss2 23.662638425827026 loss3 23.293423414230347\n",
      "166 tot loss 3.659593552350998 loss1 3.659593552350998 loss2 23.65524935722351 loss3 23.289854049682617\n",
      "167 tot loss 3.6511134803295135 loss1 3.6511134803295135 loss2 23.667009592056274 loss3 23.292081594467163\n",
      "168 tot loss 4.109481602907181 loss1 4.109481602907181 loss2 23.622810125350952 loss3 23.316379070281982\n",
      "169 tot loss 3.9245492219924927 loss1 3.9245492219924927 loss2 23.676523208618164 loss3 23.299870252609253\n",
      "170 tot loss 3.7225203216075897 loss1 3.7225203216075897 loss2 23.642584562301636 loss3 23.29431986808777\n",
      "171 tot loss 4.159044504165649 loss1 4.159044504165649 loss2 23.63227653503418 loss3 23.27110266685486\n",
      "172 tot loss 3.725937247276306 loss1 3.725937247276306 loss2 23.664445161819458 loss3 23.292209148406982\n",
      "173 tot loss 3.679861605167389 loss1 3.679861605167389 loss2 23.652796268463135 loss3 23.29048442840576\n",
      "174 tot loss 3.6840436160564423 loss1 3.6840436160564423 loss2 23.667494297027588 loss3 23.300275325775146\n",
      "175 tot loss 3.631860613822937 loss1 3.631860613822937 loss2 23.66445565223694 loss3 23.297166347503662\n",
      "176 tot loss 3.625850111246109 loss1 3.625850111246109 loss2 23.66662073135376 loss3 23.31036376953125\n",
      "177 tot loss 3.62322798371315 loss1 3.62322798371315 loss2 23.666622161865234 loss3 23.316250801086426\n",
      "178 tot loss 3.616939902305603 loss1 3.616939902305603 loss2 23.667829036712646 loss3 23.318496227264404\n",
      "179 tot loss 3.610053777694702 loss1 3.610053777694702 loss2 23.668192386627197 loss3 23.320955514907837\n",
      "180 tot loss 3.60673126578331 loss1 3.60673126578331 loss2 23.66352915763855 loss3 23.32636070251465\n",
      "181 tot loss 3.603796362876892 loss1 3.603796362876892 loss2 23.659859895706177 loss3 23.326539039611816\n",
      "182 tot loss 3.602446585893631 loss1 3.602446585893631 loss2 23.657552003860474 loss3 23.327335357666016\n",
      "183 tot loss 3.6015489995479584 loss1 3.6015489995479584 loss2 23.655613660812378 loss3 23.328339099884033\n",
      "184 tot loss 3.600755989551544 loss1 3.600755989551544 loss2 23.65438723564148 loss3 23.32863759994507\n",
      "185 tot loss 3.6000308096408844 loss1 3.6000308096408844 loss2 23.653754949569702 loss3 23.328941583633423\n",
      "186 tot loss 3.599465310573578 loss1 3.599465310573578 loss2 23.652074098587036 loss3 23.329167127609253\n",
      "187 tot loss 3.598738580942154 loss1 3.598738580942154 loss2 23.6519672870636 loss3 23.330023765563965\n",
      "188 tot loss 3.598355621099472 loss1 3.598355621099472 loss2 23.65114188194275 loss3 23.331262588500977\n",
      "189 tot loss 3.5978733599185944 loss1 3.5978733599185944 loss2 23.649937629699707 loss3 23.333019495010376\n",
      "190 tot loss 3.5973647832870483 loss1 3.5973647832870483 loss2 23.649104118347168 loss3 23.33384108543396\n",
      "191 tot loss 3.5968671441078186 loss1 3.5968671441078186 loss2 23.648347854614258 loss3 23.335413217544556\n",
      "192 tot loss 3.59655037522316 loss1 3.59655037522316 loss2 23.648284673690796 loss3 23.33615016937256\n",
      "193 tot loss 3.595919758081436 loss1 3.595919758081436 loss2 23.64738440513611 loss3 23.337533712387085\n",
      "194 tot loss 3.5956816375255585 loss1 3.5956816375255585 loss2 23.64729881286621 loss3 23.338885068893433\n",
      "195 tot loss 3.595176190137863 loss1 3.595176190137863 loss2 23.64675807952881 loss3 23.34053325653076\n",
      "196 tot loss 3.5948742032051086 loss1 3.5948742032051086 loss2 23.646185159683228 loss3 23.343082904815674\n",
      "197 tot loss 3.5945419669151306 loss1 3.5945419669151306 loss2 23.64625120162964 loss3 23.344669818878174\n",
      "198 tot loss 3.594111442565918 loss1 3.594111442565918 loss2 23.646771907806396 loss3 23.346850633621216\n",
      "199 tot loss 3.592025190591812 loss1 3.592025190591812 loss2 23.646833181381226 loss3 23.350445985794067\n",
      "200 tot loss 3.589176744222641 loss1 3.589176744222641 loss2 23.64626121520996 loss3 23.35400700569153\n",
      "201 tot loss 3.587686240673065 loss1 3.587686240673065 loss2 23.64508080482483 loss3 23.357996225357056\n",
      "202 tot loss 3.5865062475204468 loss1 3.5865062475204468 loss2 23.643537521362305 loss3 23.360945224761963\n",
      "203 tot loss 3.5888523757457733 loss1 3.5888523757457733 loss2 23.643085479736328 loss3 23.36272168159485\n",
      "204 tot loss 3.6061303913593292 loss1 3.6061303913593292 loss2 23.63180136680603 loss3 23.372373342514038\n",
      "205 tot loss 3.9129271507263184 loss1 3.9129271507263184 loss2 23.65666675567627 loss3 23.357940196990967\n",
      "206 tot loss 3.9405543208122253 loss1 3.9405543208122253 loss2 23.62211537361145 loss3 23.369457483291626\n",
      "207 tot loss 3.617633134126663 loss1 3.617633134126663 loss2 23.665773391723633 loss3 23.35747718811035\n",
      "208 tot loss 3.907847911119461 loss1 3.907847911119461 loss2 23.66145968437195 loss3 23.367502689361572\n",
      "209 tot loss 3.578591614961624 loss1 3.578591614961624 loss2 23.611494064331055 loss3 23.392898559570312\n",
      "210 tot loss 3.5747332870960236 loss1 3.5747332870960236 loss2 23.62313222885132 loss3 23.39414143562317\n",
      "211 tot loss 3.5676391422748566 loss1 3.5676391422748566 loss2 23.62140464782715 loss3 23.412805557250977\n",
      "212 tot loss 3.5667748749256134 loss1 3.5667748749256134 loss2 23.6165828704834 loss3 23.424107789993286\n",
      "213 tot loss 3.558503180742264 loss1 3.558503180742264 loss2 23.61624503135681 loss3 23.427581787109375\n",
      "214 tot loss 3.55392986536026 loss1 3.55392986536026 loss2 23.61867928504944 loss3 23.425041437149048\n",
      "215 tot loss 3.548174351453781 loss1 3.548174351453781 loss2 23.616857767105103 loss3 23.42536425590515\n",
      "216 tot loss 3.5441372096538544 loss1 3.5441372096538544 loss2 23.615410804748535 loss3 23.426960468292236\n",
      "217 tot loss 3.539787620306015 loss1 3.539787620306015 loss2 23.614187955856323 loss3 23.42948341369629\n",
      "218 tot loss 3.538050413131714 loss1 3.538050413131714 loss2 23.616649627685547 loss3 23.428849935531616\n",
      "219 tot loss 3.535722076892853 loss1 3.535722076892853 loss2 23.612103700637817 loss3 23.43306589126587\n",
      "220 tot loss 3.534227341413498 loss1 3.534227341413498 loss2 23.61420702934265 loss3 23.432040452957153\n",
      "221 tot loss 3.533143401145935 loss1 3.533143401145935 loss2 23.611777544021606 loss3 23.434202194213867\n",
      "222 tot loss 3.531798839569092 loss1 3.531798839569092 loss2 23.612695693969727 loss3 23.433339595794678\n",
      "223 tot loss 3.5312561094760895 loss1 3.5312561094760895 loss2 23.61211585998535 loss3 23.433415174484253\n",
      "224 tot loss 3.530210554599762 loss1 3.530210554599762 loss2 23.612253665924072 loss3 23.432958602905273\n",
      "225 tot loss 3.5298427045345306 loss1 3.5298427045345306 loss2 23.611395835876465 loss3 23.433671236038208\n",
      "226 tot loss 3.5290546119213104 loss1 3.5290546119213104 loss2 23.611863136291504 loss3 23.432966470718384\n",
      "227 tot loss 3.5286753177642822 loss1 3.5286753177642822 loss2 23.61086654663086 loss3 23.433984518051147\n",
      "228 tot loss 3.527970105409622 loss1 3.527970105409622 loss2 23.61078953742981 loss3 23.433913230895996\n",
      "229 tot loss 3.5277286171913147 loss1 3.5277286171913147 loss2 23.609694480895996 loss3 23.43497395515442\n",
      "230 tot loss 3.5273404121398926 loss1 3.5273404121398926 loss2 23.609663009643555 loss3 23.435194730758667\n",
      "231 tot loss 3.526853233575821 loss1 3.526853233575821 loss2 23.607383966445923 loss3 23.43714666366577\n",
      "232 tot loss 3.526368200778961 loss1 3.526368200778961 loss2 23.607167959213257 loss3 23.4364333152771\n",
      "233 tot loss 3.5261307060718536 loss1 3.5261307060718536 loss2 23.607446670532227 loss3 23.437326192855835\n",
      "234 tot loss 3.525712162256241 loss1 3.525712162256241 loss2 23.605963706970215 loss3 23.437048196792603\n",
      "235 tot loss 3.525494009256363 loss1 3.525494009256363 loss2 23.606648683547974 loss3 23.437764406204224\n",
      "236 tot loss 3.5252471268177032 loss1 3.5252471268177032 loss2 23.607625007629395 loss3 23.43817973136902\n",
      "237 tot loss 3.524876147508621 loss1 3.524876147508621 loss2 23.605082273483276 loss3 23.439208507537842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238 tot loss 3.524706870317459 loss1 3.524706870317459 loss2 23.603856086730957 loss3 23.439411878585815\n",
      "239 tot loss 3.524441570043564 loss1 3.524441570043564 loss2 23.6042218208313 loss3 23.438949584960938\n",
      "240 tot loss 3.524189442396164 loss1 3.524189442396164 loss2 23.60575008392334 loss3 23.439419507980347\n",
      "241 tot loss 3.5239596366882324 loss1 3.5239596366882324 loss2 23.60478162765503 loss3 23.43895983695984\n",
      "242 tot loss 3.523828446865082 loss1 3.523828446865082 loss2 23.60513925552368 loss3 23.43990421295166\n",
      "243 tot loss 3.5236441493034363 loss1 3.5236441493034363 loss2 23.60492777824402 loss3 23.439867734909058\n",
      "244 tot loss 3.5234424769878387 loss1 3.5234424769878387 loss2 23.60517954826355 loss3 23.4414701461792\n",
      "245 tot loss 3.523239940404892 loss1 3.523239940404892 loss2 23.603865385055542 loss3 23.441389083862305\n",
      "246 tot loss 3.5230972170829773 loss1 3.5230972170829773 loss2 23.603363752365112 loss3 23.441514015197754\n",
      "247 tot loss 3.522908538579941 loss1 3.522908538579941 loss2 23.603280305862427 loss3 23.44154953956604\n",
      "248 tot loss 3.522773027420044 loss1 3.522773027420044 loss2 23.60311985015869 loss3 23.441462516784668\n",
      "249 tot loss 3.5226141214370728 loss1 3.5226141214370728 loss2 23.604817152023315 loss3 23.442031621932983\n"
     ]
    }
   ],
   "source": [
    "train_target = train_target.long()\n",
    "train_classes = train_classes.long()\n",
    "crit = nn.CrossEntropyLoss\n",
    "\n",
    "train_model(m, train_input, train_target, train_classes,mini_batch_size, crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 19.00% 190/1000\n"
     ]
    }
   ],
   "source": [
    "nb_test_errors = compute_nb_errors(m, test_input, test_target, mini_batch_size = 100)\n",
    "print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([20, 512])\n",
      "torch.Size([20])\n",
      "torch.Size([2, 20])\n",
      "torch.Size([2])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "for p in m.parameters():\n",
    "    print(p.shape)\n",
    "    num+=1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
