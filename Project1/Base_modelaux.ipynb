{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generation\n",
    "N=10**3\n",
    "train_input,train_target,train_classes,test_input,test_target,test_classes=prologue.generate_pair_sets(N)\n",
    "train_target=train_target.long()#.float for MSELoss, .long for CrossEntropy\n",
    "train_input=train_input.float()\n",
    "train_classes=train_classes.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base functions adapted from the practicals\n",
    "def train_model(model, train_input, train_target,train_classes, mini_batch_size, crit=nn.MSELoss, eta = 1e-3, nb_epochs = 200,print_=False):\n",
    "    criterion = crit()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = eta)\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "        acc_loss1 = 0\n",
    "        acc_loss2 = 0\n",
    "        acc_loss3 = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output,aux_output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            if crit==nn.MSELoss:\n",
    "                loss1 = criterion(output[:,1], train_target.narrow(0, b, mini_batch_size))\n",
    "                #print(torch.argmax(aux_output[:,0:9],dim=1))\n",
    "                #print(train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(torch.argmax(aux_output[:,0:9],dim=1), train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(torch.argmax(aux_output[:,10:19],dim=1), train_classes[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                print('|| loss1 req grad =', loss1.requires_grad, '|| loss2 req grad =',loss2.requires_grad,'|| loss3 req grad =', loss3.requires_grad)\n",
    "            elif crit==nn.CrossEntropyLoss:\n",
    "                loss1 = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "                #print(torch.argmax(aux_output[:,0:9],dim=1))\n",
    "                #print(train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(aux_output[:,:10], train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(aux_output[:,10:], train_classes[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                #print(loss1, loss2.requires_grad, loss3.requires_grad)\n",
    "            else:\n",
    "                print(\"Loss not implemented\")\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "            acc_loss1 = acc_loss1 + loss1.item()\n",
    "            acc_loss2 = acc_loss2 + loss2.item()\n",
    "            acc_loss3 = acc_loss3 + loss3.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if False:\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p -= eta * p.grad\n",
    "\n",
    "        print(e, 'tot loss', acc_loss, 'loss1', acc_loss1, 'loss2', acc_loss2, 'loss3', acc_loss3)\n",
    "            \n",
    "def compute_nb_errors(model, input, target, mini_batch_size=100):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output , aux_output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k]!=predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors\n",
    "\n",
    "def run_many_times(model,crit=nn.MSELoss,mini_batch_size=100,n=10,print_=False):\n",
    "    average_error=0\n",
    "    for i in range(n):\n",
    "        m=model()\n",
    "        train_model(m, train_input, train_target,train_classes,mini_batch_size,crit=crit)\n",
    "        nb_test_errors = compute_nb_errors(m, test_input, test_target, mini_batch_size)\n",
    "        print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "        average_error+=(100 * nb_test_errors) / test_input.size(0)\n",
    "    print(\"Average error: \"+str(average_error/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is it better to use groups or not?\n",
    "#Takes about 2 hours to run\n",
    "#about 22.5% error average without groups if we exclude outliers that get stuck and don't move\n",
    "#about 21.5% error average with groups if we exclude outliers that get stuck and don't move\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(512, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        aux_output = F.softmax(self.fc1(x.view(-1, 512)), dim=1)\n",
    "        x = F.relu(self.fc1(x.view(-1, 512)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        aux_output = F.softmax(self.aux_linear(x), dim=1)\n",
    "        #print(x)\n",
    "        return output, aux_output\n",
    "    \n",
    "    def last_hiddes(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 512)))\n",
    "        return x\n",
    "\n",
    "class NetGroups(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, groups=2)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(512, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 512)))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        #print(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Net()\n",
    "mini_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# If you try to run the train using MSELoss like you were doing, you will get that loss2 and loss3 \n",
    "# requires_grad = False\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "0 tot loss 275.68832206726074 loss1 5.4483237862586975 loss2 138.38000011444092 loss3 131.85999870300293\n"
     ]
    }
   ],
   "source": [
    "train_target = train_target.float()\n",
    "train_classes = train_classes.float()\n",
    "crit = nn.MSELoss\n",
    "\n",
    "train_model(m, train_input, train_target, train_classes,mini_batch_size, crit, nb_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "# This is because you used a sclar target for the 10 classes, using argmax before calling the loss\n",
    "# Since argmax is a non differentiable function it sets the req. grad. of the result to False\n",
    "# Example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, True, False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(5., requires_grad = True)\n",
    "b = torch.argmax(a)\n",
    "a.dtype, a.requires_grad, b.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "# If you don't want to use hot label embedding you need to use the Cross Entropy instead. \n",
    "# I wrote some code in the training function and it seems to be working\n",
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tot loss 54.7694149017334 loss1 8.514983236789703 loss2 22.990002632141113 loss3 23.26442861557007\n",
      "1 tot loss 54.179779052734375 loss1 8.403752028942108 loss2 22.62832283973694 loss3 23.147704601287842\n",
      "2 tot loss 53.54970359802246 loss1 7.923763692378998 loss2 22.581851720809937 loss3 23.044088125228882\n",
      "3 tot loss 53.291667461395264 loss1 7.8056557178497314 loss2 22.58093237876892 loss3 22.905078649520874\n",
      "4 tot loss 52.84849691390991 loss1 7.528869867324829 loss2 22.470181465148926 loss3 22.849446773529053\n",
      "5 tot loss 52.62556171417236 loss1 7.361152231693268 loss2 22.444522380828857 loss3 22.819887399673462\n",
      "6 tot loss 52.23776149749756 loss1 7.003297507762909 loss2 22.42754077911377 loss3 22.806923389434814\n",
      "7 tot loss 52.029207706451416 loss1 6.863841116428375 loss2 22.410935163497925 loss3 22.75443124771118\n",
      "8 tot loss 51.788204193115234 loss1 6.646120131015778 loss2 22.404370307922363 loss3 22.73771333694458\n",
      "9 tot loss 51.44192457199097 loss1 6.365801751613617 loss2 22.371846437454224 loss3 22.70427703857422\n",
      "10 tot loss 51.233999252319336 loss1 6.2211538553237915 loss2 22.347930192947388 loss3 22.664915323257446\n",
      "11 tot loss 51.068198680877686 loss1 6.113330125808716 loss2 22.32975149154663 loss3 22.625117540359497\n",
      "12 tot loss 50.98439407348633 loss1 6.082536220550537 loss2 22.305227518081665 loss3 22.596630811691284\n",
      "13 tot loss 50.79057502746582 loss1 5.9362282156944275 loss2 22.30779767036438 loss3 22.546549320220947\n",
      "14 tot loss 50.838446617126465 loss1 5.978357434272766 loss2 22.31255531311035 loss3 22.547534465789795\n",
      "15 tot loss 50.53312301635742 loss1 5.748663008213043 loss2 22.28630018234253 loss3 22.498159646987915\n",
      "16 tot loss 50.400447368621826 loss1 5.632155776023865 loss2 22.287281036376953 loss3 22.481010675430298\n",
      "17 tot loss 50.418835163116455 loss1 5.673117786645889 loss2 22.27839732170105 loss3 22.46731996536255\n",
      "18 tot loss 50.250609397888184 loss1 5.518391668796539 loss2 22.26405692100525 loss3 22.4681613445282\n",
      "19 tot loss 50.29350566864014 loss1 5.588776886463165 loss2 22.259344339370728 loss3 22.44538426399231\n",
      "20 tot loss 50.22551250457764 loss1 5.556201010942459 loss2 22.249648332595825 loss3 22.419663906097412\n",
      "21 tot loss 50.15667200088501 loss1 5.510204046964645 loss2 22.238657474517822 loss3 22.407811403274536\n",
      "22 tot loss 50.03413200378418 loss1 5.409320652484894 loss2 22.242307662963867 loss3 22.382503509521484\n",
      "23 tot loss 49.937429904937744 loss1 5.345589876174927 loss2 22.23667073249817 loss3 22.355169773101807\n",
      "24 tot loss 49.84390640258789 loss1 5.259388625621796 loss2 22.233845949172974 loss3 22.350671768188477\n",
      "25 tot loss 49.79565668106079 loss1 5.22417426109314 loss2 22.229477882385254 loss3 22.342005014419556\n",
      "26 tot loss 49.804545402526855 loss1 5.239142894744873 loss2 22.22986602783203 loss3 22.33553695678711\n",
      "27 tot loss 49.68697547912598 loss1 5.140003800392151 loss2 22.227969646453857 loss3 22.3190016746521\n",
      "28 tot loss 49.832181453704834 loss1 5.294990420341492 loss2 22.220834255218506 loss3 22.31635618209839\n",
      "29 tot loss 49.65834617614746 loss1 5.129866391420364 loss2 22.22556471824646 loss3 22.30291485786438\n",
      "30 tot loss 49.87301301956177 loss1 5.356498450040817 loss2 22.219561338424683 loss3 22.296952962875366\n",
      "31 tot loss 49.73130798339844 loss1 5.2185841500759125 loss2 22.217063665390015 loss3 22.2956600189209\n",
      "32 tot loss 49.604265213012695 loss1 5.111983925104141 loss2 22.217411041259766 loss3 22.274871349334717\n",
      "33 tot loss 49.66674280166626 loss1 5.177531152963638 loss2 22.21414065361023 loss3 22.275070905685425\n",
      "34 tot loss 49.50715255737305 loss1 5.0258095264434814 loss2 22.215733766555786 loss3 22.26560950279236\n",
      "35 tot loss 49.470473766326904 loss1 4.998416632413864 loss2 22.21941375732422 loss3 22.252643585205078\n",
      "36 tot loss 49.54917287826538 loss1 5.079557925462723 loss2 22.2157039642334 loss3 22.253911018371582\n",
      "37 tot loss 49.44047164916992 loss1 4.997865378856659 loss2 22.21484661102295 loss3 22.227759838104248\n",
      "38 tot loss 49.46907997131348 loss1 5.015584319829941 loss2 22.20276927947998 loss3 22.25072717666626\n",
      "39 tot loss 49.47316312789917 loss1 5.047877788543701 loss2 22.203522443771362 loss3 22.22176194190979\n",
      "40 tot loss 49.51337146759033 loss1 5.085608690977097 loss2 22.194885969161987 loss3 22.232876539230347\n",
      "41 tot loss 49.38648223876953 loss1 4.989036679267883 loss2 22.189594268798828 loss3 22.207852602005005\n",
      "42 tot loss 49.31522560119629 loss1 4.930353045463562 loss2 22.181731700897217 loss3 22.203141450881958\n",
      "43 tot loss 49.299123764038086 loss1 4.952032297849655 loss2 22.183043718338013 loss3 22.16404700279236\n",
      "44 tot loss 49.23549032211304 loss1 4.914541959762573 loss2 22.172114849090576 loss3 22.148833513259888\n",
      "45 tot loss 49.174012660980225 loss1 4.884161174297333 loss2 22.17181921005249 loss3 22.11803102493286\n",
      "46 tot loss 49.086204528808594 loss1 4.812116771936417 loss2 22.164544820785522 loss3 22.109543323516846\n",
      "47 tot loss 49.0203537940979 loss1 4.81013149023056 loss2 22.168082237243652 loss3 22.042140007019043\n",
      "48 tot loss 48.9428277015686 loss1 4.802514582872391 loss2 22.165551900863647 loss3 21.974761724472046\n",
      "49 tot loss 49.01099348068237 loss1 4.947065442800522 loss2 22.167317867279053 loss3 21.896610260009766\n",
      "50 tot loss 48.899630069732666 loss1 4.911382406949997 loss2 22.17388343811035 loss3 21.814363718032837\n",
      "51 tot loss 48.73559522628784 loss1 4.803481429815292 loss2 22.17544960975647 loss3 21.756664276123047\n",
      "52 tot loss 48.7021918296814 loss1 4.805476397275925 loss2 22.178488969802856 loss3 21.718225479125977\n",
      "53 tot loss 48.743624687194824 loss1 4.863694250583649 loss2 22.18025541305542 loss3 21.6996750831604\n",
      "54 tot loss 48.89360761642456 loss1 5.0180312395095825 loss2 22.181979656219482 loss3 21.69359588623047\n",
      "55 tot loss 48.72045564651489 loss1 4.890059918165207 loss2 22.186161518096924 loss3 21.644234657287598\n",
      "56 tot loss 48.58913326263428 loss1 4.7861752808094025 loss2 22.185038805007935 loss3 21.617919921875\n",
      "57 tot loss 48.62360143661499 loss1 4.839668840169907 loss2 22.187593460083008 loss3 21.5963397026062\n",
      "58 tot loss 48.66978931427002 loss1 4.88163822889328 loss2 22.186949968338013 loss3 21.60120129585266\n",
      "59 tot loss 48.51655387878418 loss1 4.753965616226196 loss2 22.18638825416565 loss3 21.576200246810913\n",
      "60 tot loss 48.438047885894775 loss1 4.693306893110275 loss2 22.183154344558716 loss3 21.56158709526062\n",
      "61 tot loss 48.38915729522705 loss1 4.660553872585297 loss2 22.183501482009888 loss3 21.545101404190063\n",
      "62 tot loss 48.32208061218262 loss1 4.600071042776108 loss2 22.1868097782135 loss3 21.53519916534424\n",
      "63 tot loss 48.65030288696289 loss1 4.913774728775024 loss2 22.194133520126343 loss3 21.542393684387207\n",
      "64 tot loss 48.518171310424805 loss1 4.816602259874344 loss2 22.19199252128601 loss3 21.50957751274109\n",
      "65 tot loss 48.42998790740967 loss1 4.734200716018677 loss2 22.189786434173584 loss3 21.506000518798828\n",
      "66 tot loss 48.273860454559326 loss1 4.595993757247925 loss2 22.1881844997406 loss3 21.48968243598938\n",
      "67 tot loss 48.47834396362305 loss1 4.79839152097702 loss2 22.198169469833374 loss3 21.481781721115112\n",
      "68 tot loss 48.474562644958496 loss1 4.799558788537979 loss2 22.204750776290894 loss3 21.470252752304077\n",
      "69 tot loss 48.39169645309448 loss1 4.744316756725311 loss2 22.188085794448853 loss3 21.45929479598999\n",
      "70 tot loss 48.217435359954834 loss1 4.592171162366867 loss2 22.185123443603516 loss3 21.440141201019287\n",
      "71 tot loss 48.39388990402222 loss1 4.759181976318359 loss2 22.199311017990112 loss3 21.435397148132324\n",
      "72 tot loss 48.406299114227295 loss1 4.790776014328003 loss2 22.210009336471558 loss3 21.405513525009155\n",
      "73 tot loss 48.315536975860596 loss1 4.713451325893402 loss2 22.21021842956543 loss3 21.391867876052856\n",
      "74 tot loss 48.101452350616455 loss1 4.5287531316280365 loss2 22.18694829940796 loss3 21.385751008987427\n",
      "75 tot loss 48.07525062561035 loss1 4.511558264493942 loss2 22.185484409332275 loss3 21.37820792198181\n",
      "76 tot loss 48.05943202972412 loss1 4.506429612636566 loss2 22.1864275932312 loss3 21.366575956344604\n",
      "77 tot loss 48.236223220825195 loss1 4.680295884609222 loss2 22.20048499107361 loss3 21.35544204711914\n",
      "78 tot loss 48.17374801635742 loss1 4.632698029279709 loss2 22.199864149093628 loss3 21.341185092926025\n",
      "79 tot loss 48.18503713607788 loss1 4.643486320972443 loss2 22.18950343132019 loss3 21.352046489715576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 tot loss 48.09533357620239 loss1 4.565479606389999 loss2 22.184742212295532 loss3 21.34511160850525\n",
      "81 tot loss 48.107683181762695 loss1 4.576798588037491 loss2 22.186275959014893 loss3 21.344608545303345\n",
      "82 tot loss 48.07877731323242 loss1 4.556395351886749 loss2 22.185813426971436 loss3 21.336567878723145\n",
      "83 tot loss 47.97042751312256 loss1 4.465198248624802 loss2 22.18732738494873 loss3 21.317902088165283\n",
      "84 tot loss 48.04565382003784 loss1 4.543907970190048 loss2 22.20192575454712 loss3 21.299819469451904\n",
      "85 tot loss 47.92909479141235 loss1 4.438454806804657 loss2 22.199656009674072 loss3 21.29098391532898\n",
      "86 tot loss 48.27120113372803 loss1 4.77182000875473 loss2 22.209484100341797 loss3 21.289896726608276\n",
      "87 tot loss 48.10299205780029 loss1 4.611643373966217 loss2 22.18812870979309 loss3 21.303220748901367\n",
      "88 tot loss 47.94213151931763 loss1 4.470024019479752 loss2 22.18841004371643 loss3 21.283697366714478\n",
      "89 tot loss 48.14700222015381 loss1 4.673428565263748 loss2 22.207097053527832 loss3 21.266476154327393\n",
      "90 tot loss 48.02164125442505 loss1 4.563793838024139 loss2 22.202394008636475 loss3 21.25545358657837\n",
      "91 tot loss 48.03863859176636 loss1 4.59778505563736 loss2 22.20790982246399 loss3 21.23294425010681\n",
      "92 tot loss 47.885640144348145 loss1 4.440565377473831 loss2 22.196794748306274 loss3 21.248281002044678\n",
      "93 tot loss 47.89192581176758 loss1 4.448022246360779 loss2 22.199525356292725 loss3 21.244378089904785\n",
      "94 tot loss 47.80771064758301 loss1 4.3995358645915985 loss2 22.204898357391357 loss3 21.20327663421631\n",
      "95 tot loss 47.98819589614868 loss1 4.583360016345978 loss2 22.203547477722168 loss3 21.20128893852234\n",
      "96 tot loss 47.82096719741821 loss1 4.4328329265117645 loss2 22.20644450187683 loss3 21.18169069290161\n",
      "97 tot loss 47.90325450897217 loss1 4.5234295129776 loss2 22.207287311553955 loss3 21.172537565231323\n",
      "98 tot loss 47.87336540222168 loss1 4.5030403435230255 loss2 22.206372022628784 loss3 21.16395378112793\n",
      "99 tot loss 48.03976631164551 loss1 4.644061416387558 loss2 22.1983802318573 loss3 21.197325468063354\n",
      "100 tot loss 47.96536636352539 loss1 4.580513596534729 loss2 22.200692415237427 loss3 21.184159994125366\n",
      "101 tot loss 47.820374488830566 loss1 4.458731144666672 loss2 22.201692581176758 loss3 21.159950017929077\n",
      "102 tot loss 47.731616497039795 loss1 4.383541226387024 loss2 22.21137547492981 loss3 21.13669991493225\n",
      "103 tot loss 47.80729579925537 loss1 4.459486812353134 loss2 22.218411684036255 loss3 21.129397869110107\n",
      "104 tot loss 47.90262269973755 loss1 4.558994561433792 loss2 22.220847606658936 loss3 21.122780799865723\n",
      "105 tot loss 47.663888454437256 loss1 4.340695440769196 loss2 22.213210344314575 loss3 21.10998272895813\n",
      "106 tot loss 47.73962450027466 loss1 4.39846608042717 loss2 22.2063148021698 loss3 21.134844541549683\n",
      "107 tot loss 47.68737030029297 loss1 4.358280032873154 loss2 22.214714765548706 loss3 21.114375591278076\n",
      "108 tot loss 47.75588130950928 loss1 4.443147033452988 loss2 22.21752691268921 loss3 21.095207452774048\n",
      "109 tot loss 47.73841571807861 loss1 4.4318117797374725 loss2 22.21634864807129 loss3 21.09025502204895\n",
      "110 tot loss 47.631096839904785 loss1 4.314911127090454 loss2 22.213537454605103 loss3 21.102647304534912\n",
      "111 tot loss 47.63704824447632 loss1 4.32261449098587 loss2 22.218239307403564 loss3 21.096195220947266\n",
      "112 tot loss 47.70185327529907 loss1 4.398391932249069 loss2 22.220216751098633 loss3 21.083244800567627\n",
      "113 tot loss 47.901702880859375 loss1 4.596146643161774 loss2 22.216198921203613 loss3 21.089357376098633\n",
      "114 tot loss 47.71367788314819 loss1 4.407974362373352 loss2 22.21735382080078 loss3 21.08834934234619\n",
      "115 tot loss 47.58729267120361 loss1 4.292341530323029 loss2 22.222740173339844 loss3 21.072210788726807\n",
      "116 tot loss 47.69551992416382 loss1 4.412040501832962 loss2 22.22609543800354 loss3 21.05738377571106\n",
      "117 tot loss 47.85020637512207 loss1 4.564658582210541 loss2 22.217639207839966 loss3 21.067909240722656\n",
      "118 tot loss 47.58016061782837 loss1 4.297868221998215 loss2 22.222763776779175 loss3 21.059528589248657\n",
      "119 tot loss 47.673357009887695 loss1 4.40389010310173 loss2 22.22509765625 loss3 21.044369220733643\n",
      "120 tot loss 47.58194875717163 loss1 4.31671866774559 loss2 22.226375579833984 loss3 21.038854360580444\n",
      "121 tot loss 47.66869783401489 loss1 4.388364672660828 loss2 22.217551469802856 loss3 21.06278157234192\n",
      "122 tot loss 47.68986225128174 loss1 4.42338952422142 loss2 22.232619762420654 loss3 21.03385329246521\n",
      "123 tot loss 47.59934139251709 loss1 4.3422103226184845 loss2 22.226978540420532 loss3 21.030152559280396\n",
      "124 tot loss 47.493650913238525 loss1 4.251946777105331 loss2 22.226125478744507 loss3 21.015578508377075\n",
      "125 tot loss 47.47843074798584 loss1 4.245287597179413 loss2 22.227959156036377 loss3 21.00518488883972\n",
      "126 tot loss 47.47887945175171 loss1 4.2498659789562225 loss2 22.228699684143066 loss3 21.000314474105835\n",
      "127 tot loss 47.47828912734985 loss1 4.256306737661362 loss2 22.230214595794678 loss3 20.991767644882202\n",
      "128 tot loss 47.46240568161011 loss1 4.2451730370521545 loss2 22.229970932006836 loss3 20.9872624874115\n",
      "129 tot loss 47.45244646072388 loss1 4.239836782217026 loss2 22.22349739074707 loss3 20.98911213874817\n",
      "130 tot loss 47.42138910293579 loss1 4.209503263235092 loss2 22.227070808410645 loss3 20.98481512069702\n",
      "131 tot loss 47.64055871963501 loss1 4.422505021095276 loss2 22.231281757354736 loss3 20.98677086830139\n",
      "132 tot loss 47.515872955322266 loss1 4.309434533119202 loss2 22.22456669807434 loss3 20.981870889663696\n",
      "133 tot loss 47.421775341033936 loss1 4.224789947271347 loss2 22.229281663894653 loss3 20.967703104019165\n",
      "134 tot loss 47.55997562408447 loss1 4.367429673671722 loss2 22.232905864715576 loss3 20.95964026451111\n",
      "135 tot loss 47.49433374404907 loss1 4.308757454156876 loss2 22.22822332382202 loss3 20.957353115081787\n",
      "136 tot loss 47.45673227310181 loss1 4.275788962841034 loss2 22.234206438064575 loss3 20.94673776626587\n",
      "137 tot loss 47.453279972076416 loss1 4.276296406984329 loss2 22.23125195503235 loss3 20.945731163024902\n",
      "138 tot loss 47.3469295501709 loss1 4.173607110977173 loss2 22.22507095336914 loss3 20.9482524394989\n",
      "139 tot loss 47.39773750305176 loss1 4.219382852315903 loss2 22.22699213027954 loss3 20.951362371444702\n",
      "140 tot loss 47.464402198791504 loss1 4.292578160762787 loss2 22.228901863098145 loss3 20.942922592163086\n",
      "141 tot loss 47.49242544174194 loss1 4.324791342020035 loss2 22.235106468200684 loss3 20.932527780532837\n",
      "142 tot loss 47.354201793670654 loss1 4.190188497304916 loss2 22.224531650543213 loss3 20.939481496810913\n",
      "143 tot loss 47.334909439086914 loss1 4.170713365077972 loss2 22.227513551712036 loss3 20.93668270111084\n",
      "144 tot loss 47.28163003921509 loss1 4.125072717666626 loss2 22.227277517318726 loss3 20.929280281066895\n",
      "145 tot loss 47.34683179855347 loss1 4.1847215592861176 loss2 22.228231191635132 loss3 20.93387794494629\n",
      "146 tot loss 47.32198095321655 loss1 4.169194966554642 loss2 22.22846293449402 loss3 20.924322843551636\n",
      "147 tot loss 47.344369888305664 loss1 4.193073779344559 loss2 22.23224425315857 loss3 20.91905164718628\n",
      "148 tot loss 47.24589967727661 loss1 4.098613530397415 loss2 22.225887775421143 loss3 20.92139768600464\n",
      "149 tot loss 47.34317445755005 loss1 4.1871297955513 loss2 22.22868275642395 loss3 20.927361965179443\n",
      "150 tot loss 47.36031532287598 loss1 4.206062406301498 loss2 22.2262864112854 loss3 20.927965879440308\n",
      "151 tot loss 47.285176277160645 loss1 4.135289281606674 loss2 22.22810959815979 loss3 20.92177724838257\n",
      "152 tot loss 47.28029823303223 loss1 4.13587960600853 loss2 22.231391668319702 loss3 20.913026571273804\n",
      "153 tot loss 47.408318519592285 loss1 4.265046566724777 loss2 22.231197834014893 loss3 20.912074327468872\n",
      "154 tot loss 47.53175067901611 loss1 4.3841432929039 loss2 22.235019207000732 loss3 20.912588834762573\n",
      "155 tot loss 47.259692668914795 loss1 4.116519719362259 loss2 22.227269411087036 loss3 20.915903329849243\n",
      "156 tot loss 47.294437885284424 loss1 4.148664057254791 loss2 22.22783064842224 loss3 20.917943239212036\n",
      "157 tot loss 47.219547748565674 loss1 4.082550525665283 loss2 22.226022720336914 loss3 20.910974264144897\n",
      "158 tot loss 47.20783090591431 loss1 4.075519382953644 loss2 22.228724241256714 loss3 20.903587579727173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 tot loss 47.16296148300171 loss1 4.035487353801727 loss2 22.225329160690308 loss3 20.90214490890503\n",
      "160 tot loss 47.17938470840454 loss1 4.052687704563141 loss2 22.227431058883667 loss3 20.899266242980957\n",
      "161 tot loss 47.18211793899536 loss1 4.058861404657364 loss2 22.225942134857178 loss3 20.89731478691101\n",
      "162 tot loss 47.15498876571655 loss1 4.036004066467285 loss2 22.22473978996277 loss3 20.8942449092865\n",
      "163 tot loss 47.128056049346924 loss1 4.009431838989258 loss2 22.224837064743042 loss3 20.893786907196045\n",
      "164 tot loss 47.11588764190674 loss1 4.000841826200485 loss2 22.224166870117188 loss3 20.890878200531006\n",
      "165 tot loss 47.10334777832031 loss1 3.9911212027072906 loss2 22.223888635635376 loss3 20.8883376121521\n",
      "166 tot loss 47.096848487854004 loss1 3.98691788315773 loss2 22.224177837371826 loss3 20.88575267791748\n",
      "167 tot loss 47.08337640762329 loss1 3.977357655763626 loss2 22.221866369247437 loss3 20.884153842926025\n",
      "168 tot loss 47.07742977142334 loss1 3.9735409915447235 loss2 22.222037315368652 loss3 20.8818519115448\n",
      "169 tot loss 47.066551208496094 loss1 3.965445041656494 loss2 22.220407009124756 loss3 20.880698204040527\n",
      "170 tot loss 47.06084203720093 loss1 3.96101114153862 loss2 22.220552444458008 loss3 20.879278421401978\n",
      "171 tot loss 47.05194807052612 loss1 3.957495301961899 loss2 22.21871519088745 loss3 20.875736951828003\n",
      "172 tot loss 47.047142028808594 loss1 3.9542373418807983 loss2 22.2192223072052 loss3 20.873682975769043\n",
      "173 tot loss 47.044583797454834 loss1 3.9537502825260162 loss2 22.218398809432983 loss3 20.872435569763184\n",
      "174 tot loss 47.0442328453064 loss1 3.954240471124649 loss2 22.218550205230713 loss3 20.871442079544067\n",
      "175 tot loss 47.029905796051025 loss1 3.945727825164795 loss2 22.217605352401733 loss3 20.866571187973022\n",
      "176 tot loss 47.03658151626587 loss1 3.952226221561432 loss2 22.21707034111023 loss3 20.867284536361694\n",
      "177 tot loss 47.01578903198242 loss1 3.937105894088745 loss2 22.21720051765442 loss3 20.861483097076416\n",
      "178 tot loss 47.00019454956055 loss1 3.9288633167743683 loss2 22.217034101486206 loss3 20.854297637939453\n",
      "179 tot loss 46.99312973022461 loss1 3.9223702549934387 loss2 22.216248273849487 loss3 20.854511499404907\n",
      "180 tot loss 46.98654127120972 loss1 3.918809801340103 loss2 22.216720819473267 loss3 20.85101056098938\n",
      "181 tot loss 46.987555503845215 loss1 3.921951651573181 loss2 22.218195915222168 loss3 20.8474063873291\n",
      "182 tot loss 47.09811353683472 loss1 4.036029666662216 loss2 22.220803022384644 loss3 20.841280698776245\n",
      "183 tot loss 47.41950511932373 loss1 4.337642997503281 loss2 22.224018096923828 loss3 20.857844591140747\n",
      "184 tot loss 46.96748161315918 loss1 3.9092978835105896 loss2 22.212243795394897 loss3 20.84593963623047\n",
      "185 tot loss 46.95716619491577 loss1 3.902872681617737 loss2 22.213552713394165 loss3 20.840741872787476\n",
      "186 tot loss 46.967283725738525 loss1 3.915150076150894 loss2 22.214158535003662 loss3 20.837975025177002\n",
      "187 tot loss 46.951287269592285 loss1 3.900747001171112 loss2 22.214343786239624 loss3 20.836196184158325\n",
      "188 tot loss 46.97386312484741 loss1 3.9246454536914825 loss2 22.215848445892334 loss3 20.83336853981018\n",
      "189 tot loss 46.9441032409668 loss1 3.8960534930229187 loss2 22.215579986572266 loss3 20.832469940185547\n",
      "190 tot loss 46.93800401687622 loss1 3.8917032182216644 loss2 22.21558904647827 loss3 20.83071255683899\n",
      "191 tot loss 46.924893856048584 loss1 3.8791645169258118 loss2 22.215507745742798 loss3 20.83022165298462\n",
      "192 tot loss 46.9351487159729 loss1 3.89114910364151 loss2 22.21719193458557 loss3 20.826807022094727\n",
      "193 tot loss 47.78202486038208 loss1 4.716936618089676 loss2 22.226077795028687 loss3 20.839011430740356\n",
      "194 tot loss 46.992798805236816 loss1 3.942517638206482 loss2 22.216991662979126 loss3 20.83328866958618\n",
      "195 tot loss 46.95728063583374 loss1 3.9147267043590546 loss2 22.21596336364746 loss3 20.82659101486206\n",
      "196 tot loss 46.95739412307739 loss1 3.9166798293590546 loss2 22.216609954833984 loss3 20.824103593826294\n",
      "197 tot loss 47.285895347595215 loss1 4.238893955945969 loss2 22.22009015083313 loss3 20.82691192626953\n",
      "198 tot loss 46.96630096435547 loss1 3.9253652691841125 loss2 22.21449303627014 loss3 20.82644248008728\n",
      "199 tot loss 46.91060543060303 loss1 3.8736779391765594 loss2 22.214717388153076 loss3 20.82221007347107\n"
     ]
    }
   ],
   "source": [
    "train_target = train_target.long()\n",
    "train_classes = train_classes.long()\n",
    "crit = nn.CrossEntropyLoss\n",
    "\n",
    "train_model(m, train_input, train_target, train_classes,mini_batch_size, crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 25.30% 253/1000\n"
     ]
    }
   ],
   "source": [
    "nb_test_errors = compute_nb_errors(m, test_input, test_target, mini_batch_size = 100)\n",
    "print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),nb_test_errors, test_input.size(0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tirocinio",
   "language": "python",
   "name": "tirocinio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
