{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import torch\n",
    "import various_data_functions\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "import dlc_practical_prologue as prologue\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data generation\n",
    "N=10**3\n",
    "#train_input,train_target,train_classes,test_input,test_target,test_classes=prologue.generate_pair_sets(N)\n",
    "train_input,train_target,train_classes,test_input,test_target,test_classes=various_data_functions.data(N,True,False,nn.CrossEntropyLoss)\n",
    "#train_target=train_target.long()#.float for MSELoss, .long for CrossEntropy\n",
    "#train_input=train_input.float()\n",
    "#train_classes=train_classes.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base functions adapted from the practicals\n",
    "def train_model(model, train_input, train_target,train_classes, mini_batch_size, crit=nn.MSELoss, eta = 1e-3, nb_epochs = 250,print_=False):\n",
    "    criterion = crit()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = eta)\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "        acc_loss1 = 0\n",
    "        acc_loss2 = 0\n",
    "        acc_loss3 = 0\n",
    "\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output,aux_output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            if crit==nn.MSELoss:\n",
    "                loss1 = criterion(output[:,1], train_target.narrow(0, b, mini_batch_size))\n",
    "                #print(torch.argmax(aux_output[:,0:9],dim=1))\n",
    "                #print(train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(torch.argmax(aux_output[:,0:9],dim=1), train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(torch.argmax(aux_output[:,10:19],dim=1), train_classes[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                print('|| loss1 req grad =', loss1.requires_grad, '|| loss2 req grad =',loss2.requires_grad,'|| loss3 req grad =', loss3.requires_grad)\n",
    "            elif crit==nn.CrossEntropyLoss:\n",
    "                loss1 = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "                #print(torch.argmax(aux_output[:,0:9],dim=1))\n",
    "                #print(train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss2 = criterion(aux_output[:,:10], train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "                loss3 = criterion(aux_output[:,10:], train_classes[:,1].narrow(0, b, mini_batch_size))\n",
    "                loss = loss1 + 0.1*(loss2 + loss3)\n",
    "                #print(loss1, loss2.requires_grad, loss3.requires_grad)\n",
    "            else:\n",
    "                print(\"Loss not implemented\")\n",
    "            acc_loss = acc_loss + loss.item()\n",
    "            acc_loss1 = acc_loss1 + loss1.item()\n",
    "            acc_loss2 = acc_loss2 + loss2.item()\n",
    "            acc_loss3 = acc_loss3 + loss3.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if False:\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p -= eta * p.grad\n",
    "\n",
    "        print(e, 'tot loss', acc_loss, 'loss1', acc_loss1, 'loss2', acc_loss2, 'loss3', acc_loss3)\n",
    "            \n",
    "def compute_nb_errors(model, input, target, mini_batch_size=100):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for b in range(0, input.size(0), mini_batch_size):\n",
    "        output , aux_output = model(input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = output.max(1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if target[b + k]!=predicted_classes[k]:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors\n",
    "\n",
    "def run_many_times(model,crit=nn.MSELoss,mini_batch_size=100,n=10,print_=False):\n",
    "    average_error=0\n",
    "    for i in range(n):\n",
    "        m=model()\n",
    "        train_model(m, train_input, train_target,train_classes,mini_batch_size,crit=crit)\n",
    "        nb_test_errors = compute_nb_errors(m, test_input, test_target, mini_batch_size)\n",
    "        print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                      nb_test_errors, test_input.size(0)))\n",
    "        average_error+=(100 * nb_test_errors) / test_input.size(0)\n",
    "    print(\"Average error: \"+str(average_error/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is it better to use groups or not?\n",
    "#Takes about 2 hours to run\n",
    "#about 22.5% error average without groups if we exclude outliers that get stuck and don't move\n",
    "#about 21.5% error average with groups if we exclude outliers that get stuck and don't move\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 100, kernel_size=3,groups=2)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(1600, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "        self.aux_linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        aux_output = F.softmax(self.fc1(x.view(-1, 1600)), dim=1)\n",
    "        x = F.relu(self.fc1(x.view(-1, 1600)))\n",
    "        output = F.softmax(self.fc2(x), dim=1)\n",
    "        aux_output = F.softmax(x, dim=1)\n",
    "        #print(x)\n",
    "        return output, aux_output\n",
    "    \n",
    "    def last_hiddes(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 1600)))\n",
    "        return x\n",
    "\n",
    "class NetGroups(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, groups=2)\n",
    "        #self.conv2 = nn.Conv2d(32, 64, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(512, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 512)))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        #print(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=Net()\n",
    "mini_batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# If you try to run the train using MSELoss like you were doing, you will get that loss2 and loss3 \n",
    "# requires_grad = False\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "|| loss1 req grad = True || loss2 req grad = False || loss3 req grad = False\n",
      "0 tot loss 323.7322196960449 loss1 2.6422192454338074 loss2 121.73000144958496 loss3 199.3599967956543\n"
     ]
    }
   ],
   "source": [
    "train_target = train_target.float()\n",
    "train_classes = train_classes.float()\n",
    "crit = nn.MSELoss\n",
    "\n",
    "train_model(m, train_input, train_target, train_classes,mini_batch_size, crit, nb_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "# This is because you used a sclar target for the 10 classes, using argmax before calling the loss\n",
    "# Since argmax is a non differentiable function it sets the req. grad. of the result to False\n",
    "# Example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, True, False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(5., requires_grad = True)\n",
    "b = torch.argmax(a)\n",
    "a.dtype, a.requires_grad, b.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################\n",
    "# If you don't want to use hot label embedding you need to use the Cross Entropy instead. \n",
    "# I wrote some code in the training function and it seems to be working\n",
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tot loss 7.016153454780579 loss1 7.016153454780579 loss2 23.03327965736389 loss3 23.031555652618408\n",
      "1 tot loss 6.994461953639984 loss1 6.994461953639984 loss2 23.03477692604065 loss3 23.032089710235596\n",
      "2 tot loss 6.9755819439888 loss1 6.9755819439888 loss2 23.036263942718506 loss3 23.03273367881775\n",
      "3 tot loss 6.957402348518372 loss1 6.957402348518372 loss2 23.03774857521057 loss3 23.03362727165222\n",
      "4 tot loss 6.939821124076843 loss1 6.939821124076843 loss2 23.03927254676819 loss3 23.034764289855957\n",
      "5 tot loss 6.922888100147247 loss1 6.922888100147247 loss2 23.040833950042725 loss3 23.03585720062256\n",
      "6 tot loss 6.90782630443573 loss1 6.90782630443573 loss2 23.042372703552246 loss3 23.036945104599\n",
      "7 tot loss 6.893777668476105 loss1 6.893777668476105 loss2 23.043835163116455 loss3 23.038168907165527\n",
      "8 tot loss 6.880793809890747 loss1 6.880793809890747 loss2 23.045262336730957 loss3 23.039315700531006\n",
      "9 tot loss 6.869109869003296 loss1 6.869109869003296 loss2 23.04661536216736 loss3 23.040462255477905\n",
      "10 tot loss 6.858671188354492 loss1 6.858671188354492 loss2 23.047909021377563 loss3 23.041558504104614\n",
      "11 tot loss 6.849185764789581 loss1 6.849185764789581 loss2 23.049158096313477 loss3 23.04262399673462\n",
      "12 tot loss 6.840812385082245 loss1 6.840812385082245 loss2 23.05034565925598 loss3 23.043593168258667\n",
      "13 tot loss 6.833192050457001 loss1 6.833192050457001 loss2 23.051477432250977 loss3 23.04460620880127\n",
      "14 tot loss 6.826273202896118 loss1 6.826273202896118 loss2 23.052505493164062 loss3 23.045591831207275\n",
      "15 tot loss 6.819781243801117 loss1 6.819781243801117 loss2 23.053478479385376 loss3 23.0466091632843\n",
      "16 tot loss 6.81359326839447 loss1 6.81359326839447 loss2 23.05435585975647 loss3 23.047678470611572\n",
      "17 tot loss 6.8076459765434265 loss1 6.8076459765434265 loss2 23.055139780044556 loss3 23.048806190490723\n",
      "18 tot loss 6.801978945732117 loss1 6.801978945732117 loss2 23.05586314201355 loss3 23.04988384246826\n",
      "19 tot loss 6.796592354774475 loss1 6.796592354774475 loss2 23.0565128326416 loss3 23.050899982452393\n",
      "20 tot loss 6.791413366794586 loss1 6.791413366794586 loss2 23.057136297225952 loss3 23.051868438720703\n",
      "21 tot loss 6.786581933498383 loss1 6.786581933498383 loss2 23.05771017074585 loss3 23.05278468132019\n",
      "22 tot loss 6.78194797039032 loss1 6.78194797039032 loss2 23.058279037475586 loss3 23.053638219833374\n",
      "23 tot loss 6.777429699897766 loss1 6.777429699897766 loss2 23.058804988861084 loss3 23.054422616958618\n",
      "24 tot loss 6.773159086704254 loss1 6.773159086704254 loss2 23.05930519104004 loss3 23.055113315582275\n",
      "25 tot loss 6.769140303134918 loss1 6.769140303134918 loss2 23.059786081314087 loss3 23.05570125579834\n",
      "26 tot loss 6.765202522277832 loss1 6.765202522277832 loss2 23.06024956703186 loss3 23.056228160858154\n",
      "27 tot loss 6.761423766613007 loss1 6.761423766613007 loss2 23.060704231262207 loss3 23.05671191215515\n",
      "28 tot loss 6.757756173610687 loss1 6.757756173610687 loss2 23.061139583587646 loss3 23.057167530059814\n",
      "29 tot loss 6.754198849201202 loss1 6.754198849201202 loss2 23.061564922332764 loss3 23.05756711959839\n",
      "30 tot loss 6.75068199634552 loss1 6.75068199634552 loss2 23.061983108520508 loss3 23.057945728302002\n",
      "31 tot loss 6.747244298458099 loss1 6.747244298458099 loss2 23.06240224838257 loss3 23.058287143707275\n",
      "32 tot loss 6.743861675262451 loss1 6.743861675262451 loss2 23.06280279159546 loss3 23.05860161781311\n",
      "33 tot loss 6.740413963794708 loss1 6.740413963794708 loss2 23.06321430206299 loss3 23.058887243270874\n",
      "34 tot loss 6.737058937549591 loss1 6.737058937549591 loss2 23.063628673553467 loss3 23.05915641784668\n",
      "35 tot loss 6.7337289452552795 loss1 6.7337289452552795 loss2 23.06403398513794 loss3 23.0594003200531\n",
      "36 tot loss 6.730372726917267 loss1 6.730372726917267 loss2 23.064459085464478 loss3 23.059634923934937\n",
      "37 tot loss 6.727007687091827 loss1 6.727007687091827 loss2 23.064868211746216 loss3 23.05984878540039\n",
      "38 tot loss 6.723585844039917 loss1 6.723585844039917 loss2 23.06523895263672 loss3 23.060038328170776\n",
      "39 tot loss 6.720187425613403 loss1 6.720187425613403 loss2 23.065592765808105 loss3 23.06020736694336\n",
      "40 tot loss 6.7168591022491455 loss1 6.7168591022491455 loss2 23.065934658050537 loss3 23.06036639213562\n",
      "41 tot loss 6.7135215401649475 loss1 6.7135215401649475 loss2 23.06627058982849 loss3 23.060503005981445\n",
      "42 tot loss 6.710011839866638 loss1 6.710011839866638 loss2 23.06661581993103 loss3 23.06061029434204\n",
      "43 tot loss 6.706497967243195 loss1 6.706497967243195 loss2 23.06693983078003 loss3 23.060701608657837\n",
      "44 tot loss 6.70289146900177 loss1 6.70289146900177 loss2 23.067230463027954 loss3 23.060802936553955\n",
      "45 tot loss 6.699200510978699 loss1 6.699200510978699 loss2 23.067487478256226 loss3 23.060899257659912\n",
      "46 tot loss 6.695245802402496 loss1 6.695245802402496 loss2 23.067734718322754 loss3 23.060980319976807\n",
      "47 tot loss 6.691082835197449 loss1 6.691082835197449 loss2 23.06797480583191 loss3 23.061089515686035\n",
      "48 tot loss 6.686877548694611 loss1 6.686877548694611 loss2 23.068195343017578 loss3 23.06119441986084\n",
      "49 tot loss 6.682797372341156 loss1 6.682797372341156 loss2 23.06840205192566 loss3 23.061312437057495\n",
      "50 tot loss 6.678782224655151 loss1 6.678782224655151 loss2 23.0686137676239 loss3 23.061428785324097\n",
      "51 tot loss 6.6746349930763245 loss1 6.6746349930763245 loss2 23.06881880760193 loss3 23.06156849861145\n",
      "52 tot loss 6.67042750120163 loss1 6.67042750120163 loss2 23.069014072418213 loss3 23.06171154975891\n",
      "53 tot loss 6.666181743144989 loss1 6.666181743144989 loss2 23.06922173500061 loss3 23.061882495880127\n",
      "54 tot loss 6.662079155445099 loss1 6.662079155445099 loss2 23.069427251815796 loss3 23.06205987930298\n",
      "55 tot loss 6.657920956611633 loss1 6.657920956611633 loss2 23.06963849067688 loss3 23.062251091003418\n",
      "56 tot loss 6.653865039348602 loss1 6.653865039348602 loss2 23.069867372512817 loss3 23.06245231628418\n",
      "57 tot loss 6.649802565574646 loss1 6.649802565574646 loss2 23.070122718811035 loss3 23.06265354156494\n",
      "58 tot loss 6.645742774009705 loss1 6.645742774009705 loss2 23.07037878036499 loss3 23.062854290008545\n",
      "59 tot loss 6.641696870326996 loss1 6.641696870326996 loss2 23.070637941360474 loss3 23.063048839569092\n",
      "60 tot loss 6.637664318084717 loss1 6.637664318084717 loss2 23.070899724960327 loss3 23.063252687454224\n",
      "61 tot loss 6.633649230003357 loss1 6.633649230003357 loss2 23.071168899536133 loss3 23.0634605884552\n",
      "62 tot loss 6.629632234573364 loss1 6.629632234573364 loss2 23.071441173553467 loss3 23.06366515159607\n",
      "63 tot loss 6.625624120235443 loss1 6.625624120235443 loss2 23.07170867919922 loss3 23.063870906829834\n",
      "64 tot loss 6.621605157852173 loss1 6.621605157852173 loss2 23.07196307182312 loss3 23.06406831741333\n",
      "65 tot loss 6.6175315380096436 loss1 6.6175315380096436 loss2 23.07220435142517 loss3 23.06425428390503\n",
      "66 tot loss 6.613446593284607 loss1 6.613446593284607 loss2 23.072437047958374 loss3 23.064441680908203\n",
      "67 tot loss 6.609316110610962 loss1 6.609316110610962 loss2 23.072656393051147 loss3 23.06462526321411\n",
      "68 tot loss 6.605180859565735 loss1 6.605180859565735 loss2 23.07288122177124 loss3 23.064806938171387\n",
      "69 tot loss 6.601025819778442 loss1 6.601025819778442 loss2 23.07312321662903 loss3 23.06498670578003\n",
      "70 tot loss 6.596857249736786 loss1 6.596857249736786 loss2 23.07337188720703 loss3 23.065158367156982\n",
      "71 tot loss 6.592689692974091 loss1 6.592689692974091 loss2 23.073633670806885 loss3 23.065329790115356\n",
      "72 tot loss 6.588514864444733 loss1 6.588514864444733 loss2 23.07390594482422 loss3 23.065495491027832\n",
      "73 tot loss 6.584319472312927 loss1 6.584319472312927 loss2 23.074174404144287 loss3 23.06566309928894\n",
      "74 tot loss 6.580135643482208 loss1 6.580135643482208 loss2 23.074422597885132 loss3 23.065821886062622\n",
      "75 tot loss 6.575900018215179 loss1 6.575900018215179 loss2 23.07465147972107 loss3 23.06597900390625\n",
      "76 tot loss 6.571654677391052 loss1 6.571654677391052 loss2 23.074886083602905 loss3 23.066140174865723\n",
      "77 tot loss 6.567298591136932 loss1 6.567298591136932 loss2 23.07512879371643 loss3 23.06629252433777\n",
      "78 tot loss 6.562956392765045 loss1 6.562956392765045 loss2 23.07536220550537 loss3 23.06645107269287\n",
      "79 tot loss 6.558600544929504 loss1 6.558600544929504 loss2 23.075613975524902 loss3 23.066614627838135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 tot loss 6.554215610027313 loss1 6.554215610027313 loss2 23.075857162475586 loss3 23.066778898239136\n",
      "81 tot loss 6.549827814102173 loss1 6.549827814102173 loss2 23.076112270355225 loss3 23.066948175430298\n",
      "82 tot loss 6.545384585857391 loss1 6.545384585857391 loss2 23.076367616653442 loss3 23.067119598388672\n",
      "83 tot loss 6.54087507724762 loss1 6.54087507724762 loss2 23.076631546020508 loss3 23.067291259765625\n",
      "84 tot loss 6.53634512424469 loss1 6.53634512424469 loss2 23.076889991760254 loss3 23.067465782165527\n",
      "85 tot loss 6.531810283660889 loss1 6.531810283660889 loss2 23.07711672782898 loss3 23.067644119262695\n",
      "86 tot loss 6.5272422432899475 loss1 6.5272422432899475 loss2 23.077343702316284 loss3 23.06782555580139\n",
      "87 tot loss 6.522688806056976 loss1 6.522688806056976 loss2 23.077577352523804 loss3 23.068009614944458\n",
      "88 tot loss 6.518051445484161 loss1 6.518051445484161 loss2 23.077810764312744 loss3 23.06818652153015\n",
      "89 tot loss 6.513375520706177 loss1 6.513375520706177 loss2 23.078078985214233 loss3 23.06836485862732\n",
      "90 tot loss 6.508666098117828 loss1 6.508666098117828 loss2 23.078328371047974 loss3 23.06854820251465\n",
      "91 tot loss 6.503965735435486 loss1 6.503965735435486 loss2 23.0785870552063 loss3 23.06873106956482\n",
      "92 tot loss 6.499280750751495 loss1 6.499280750751495 loss2 23.078869104385376 loss3 23.06892490386963\n",
      "93 tot loss 6.494624137878418 loss1 6.494624137878418 loss2 23.079146146774292 loss3 23.069117546081543\n",
      "94 tot loss 6.489959120750427 loss1 6.489959120750427 loss2 23.079431295394897 loss3 23.069311141967773\n",
      "95 tot loss 6.485261917114258 loss1 6.485261917114258 loss2 23.079692602157593 loss3 23.069503784179688\n",
      "96 tot loss 6.480562925338745 loss1 6.480562925338745 loss2 23.079951286315918 loss3 23.069698810577393\n",
      "97 tot loss 6.475887358188629 loss1 6.475887358188629 loss2 23.080212593078613 loss3 23.069899559020996\n",
      "98 tot loss 6.471217751502991 loss1 6.471217751502991 loss2 23.080490112304688 loss3 23.070099353790283\n",
      "99 tot loss 6.4665586948394775 loss1 6.4665586948394775 loss2 23.08076763153076 loss3 23.070294857025146\n",
      "100 tot loss 6.461872637271881 loss1 6.461872637271881 loss2 23.08104109764099 loss3 23.070483207702637\n",
      "101 tot loss 6.457190930843353 loss1 6.457190930843353 loss2 23.08129358291626 loss3 23.070671796798706\n",
      "102 tot loss 6.45251989364624 loss1 6.45251989364624 loss2 23.081541776657104 loss3 23.070865631103516\n",
      "103 tot loss 6.4478291273117065 loss1 6.4478291273117065 loss2 23.081764936447144 loss3 23.0710551738739\n",
      "104 tot loss 6.44311398267746 loss1 6.44311398267746 loss2 23.081984996795654 loss3 23.0712411403656\n",
      "105 tot loss 6.43843138217926 loss1 6.43843138217926 loss2 23.08222985267639 loss3 23.0714271068573\n",
      "106 tot loss 6.433757305145264 loss1 6.433757305145264 loss2 23.08247470855713 loss3 23.071616888046265\n",
      "107 tot loss 6.429065585136414 loss1 6.429065585136414 loss2 23.08270835876465 loss3 23.07180905342102\n",
      "108 tot loss 6.424378037452698 loss1 6.424378037452698 loss2 23.082921266555786 loss3 23.07199501991272\n",
      "109 tot loss 6.419674873352051 loss1 6.419674873352051 loss2 23.0831298828125 loss3 23.07217788696289\n",
      "110 tot loss 6.414963066577911 loss1 6.414963066577911 loss2 23.083343982696533 loss3 23.07236099243164\n",
      "111 tot loss 6.410269558429718 loss1 6.410269558429718 loss2 23.083559274673462 loss3 23.072546005249023\n",
      "112 tot loss 6.405570864677429 loss1 6.405570864677429 loss2 23.0837721824646 loss3 23.072729110717773\n",
      "113 tot loss 6.400864958763123 loss1 6.400864958763123 loss2 23.083985328674316 loss3 23.07290482521057\n",
      "114 tot loss 6.396137297153473 loss1 6.396137297153473 loss2 23.08420205116272 loss3 23.07308340072632\n",
      "115 tot loss 6.391380727291107 loss1 6.391380727291107 loss2 23.084402084350586 loss3 23.07326340675354\n",
      "116 tot loss 6.386648178100586 loss1 6.386648178100586 loss2 23.084606647491455 loss3 23.07344102859497\n",
      "117 tot loss 6.3819305300712585 loss1 6.3819305300712585 loss2 23.08482003211975 loss3 23.073627471923828\n",
      "118 tot loss 6.377204954624176 loss1 6.377204954624176 loss2 23.085044145584106 loss3 23.073811531066895\n",
      "119 tot loss 6.372473180294037 loss1 6.372473180294037 loss2 23.08527112007141 loss3 23.07399296760559\n",
      "120 tot loss 6.367752134799957 loss1 6.367752134799957 loss2 23.08549928665161 loss3 23.074175357818604\n",
      "121 tot loss 6.363035500049591 loss1 6.363035500049591 loss2 23.08571743965149 loss3 23.07436466217041\n",
      "122 tot loss 6.358320534229279 loss1 6.358320534229279 loss2 23.085930585861206 loss3 23.074554443359375\n",
      "123 tot loss 6.353602051734924 loss1 6.353602051734924 loss2 23.086146354675293 loss3 23.074744939804077\n",
      "124 tot loss 6.348895907402039 loss1 6.348895907402039 loss2 23.086369037628174 loss3 23.07493495941162\n",
      "125 tot loss 6.344177007675171 loss1 6.344177007675171 loss2 23.086570501327515 loss3 23.075126886367798\n",
      "126 tot loss 6.339462041854858 loss1 6.339462041854858 loss2 23.086785793304443 loss3 23.075318813323975\n",
      "127 tot loss 6.3347466588020325 loss1 6.3347466588020325 loss2 23.087013483047485 loss3 23.075507402420044\n",
      "128 tot loss 6.330025315284729 loss1 6.330025315284729 loss2 23.08722734451294 loss3 23.07569694519043\n",
      "129 tot loss 6.325285851955414 loss1 6.325285851955414 loss2 23.087438821792603 loss3 23.07588219642639\n",
      "130 tot loss 6.320518136024475 loss1 6.320518136024475 loss2 23.08764934539795 loss3 23.076064109802246\n",
      "131 tot loss 6.315746963024139 loss1 6.315746963024139 loss2 23.087849378585815 loss3 23.076242208480835\n",
      "132 tot loss 6.3109517097473145 loss1 6.3109517097473145 loss2 23.088050842285156 loss3 23.07642388343811\n",
      "133 tot loss 6.306178033351898 loss1 6.306178033351898 loss2 23.088268280029297 loss3 23.076610326766968\n",
      "134 tot loss 6.301428318023682 loss1 6.301428318023682 loss2 23.088481187820435 loss3 23.076802492141724\n",
      "135 tot loss 6.296705424785614 loss1 6.296705424785614 loss2 23.08870816230774 loss3 23.076998949050903\n",
      "136 tot loss 6.2919961810112 loss1 6.2919961810112 loss2 23.088939905166626 loss3 23.07719850540161\n",
      "137 tot loss 6.287296533584595 loss1 6.287296533584595 loss2 23.08917212486267 loss3 23.077396869659424\n",
      "138 tot loss 6.2826038002967834 loss1 6.2826038002967834 loss2 23.089414596557617 loss3 23.07758665084839\n",
      "139 tot loss 6.27790766954422 loss1 6.27790766954422 loss2 23.08964467048645 loss3 23.077776432037354\n",
      "140 tot loss 6.273222982883453 loss1 6.273222982883453 loss2 23.089873790740967 loss3 23.077969789505005\n",
      "141 tot loss 6.268529832363129 loss1 6.268529832363129 loss2 23.090088844299316 loss3 23.07815384864807\n",
      "142 tot loss 6.2638163566589355 loss1 6.2638163566589355 loss2 23.090309381484985 loss3 23.07834005355835\n",
      "143 tot loss 6.259095132350922 loss1 6.259095132350922 loss2 23.090521574020386 loss3 23.07853102684021\n",
      "144 tot loss 6.254400968551636 loss1 6.254400968551636 loss2 23.090738534927368 loss3 23.078724145889282\n",
      "145 tot loss 6.249718546867371 loss1 6.249718546867371 loss2 23.09095597267151 loss3 23.07891869544983\n",
      "146 tot loss 6.245057821273804 loss1 6.245057821273804 loss2 23.09118938446045 loss3 23.079116582870483\n",
      "147 tot loss 6.240410327911377 loss1 6.240410327911377 loss2 23.091413974761963 loss3 23.079317569732666\n",
      "148 tot loss 6.23577481508255 loss1 6.23577481508255 loss2 23.091640949249268 loss3 23.079514503479004\n",
      "149 tot loss 6.231141567230225 loss1 6.231141567230225 loss2 23.09186363220215 loss3 23.079710960388184\n",
      "150 tot loss 6.226525068283081 loss1 6.226525068283081 loss2 23.09209132194519 loss3 23.079912662506104\n",
      "151 tot loss 6.2219085693359375 loss1 6.2219085693359375 loss2 23.092321634292603 loss3 23.080111026763916\n",
      "152 tot loss 6.217320382595062 loss1 6.217320382595062 loss2 23.092548608779907 loss3 23.080307960510254\n",
      "153 tot loss 6.212720513343811 loss1 6.212720513343811 loss2 23.09277105331421 loss3 23.08051109313965\n",
      "154 tot loss 6.208154320716858 loss1 6.208154320716858 loss2 23.092985153198242 loss3 23.08071279525757\n",
      "155 tot loss 6.203603446483612 loss1 6.203603446483612 loss2 23.093215465545654 loss3 23.080913305282593\n",
      "156 tot loss 6.199071824550629 loss1 6.199071824550629 loss2 23.093440771102905 loss3 23.08111572265625\n",
      "157 tot loss 6.1945613622665405 loss1 6.1945613622665405 loss2 23.093666315078735 loss3 23.081318378448486\n",
      "158 tot loss 6.1900540590286255 loss1 6.1900540590286255 loss2 23.093894958496094 loss3 23.081523180007935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 tot loss 6.185571908950806 loss1 6.185571908950806 loss2 23.094122171401978 loss3 23.08172631263733\n",
      "160 tot loss 6.1810959577560425 loss1 6.1810959577560425 loss2 23.094354391098022 loss3 23.081928730010986\n",
      "161 tot loss 6.176643252372742 loss1 6.176643252372742 loss2 23.09458565711975 loss3 23.082136631011963\n",
      "162 tot loss 6.172203600406647 loss1 6.172203600406647 loss2 23.09481430053711 loss3 23.08233642578125\n",
      "163 tot loss 6.167774438858032 loss1 6.167774438858032 loss2 23.095046997070312 loss3 23.082534790039062\n",
      "164 tot loss 6.163355886936188 loss1 6.163355886936188 loss2 23.0952787399292 loss3 23.082732677459717\n",
      "165 tot loss 6.158950865268707 loss1 6.158950865268707 loss2 23.095506191253662 loss3 23.082929849624634\n",
      "166 tot loss 6.154558300971985 loss1 6.154558300971985 loss2 23.095738172531128 loss3 23.083128213882446\n",
      "167 tot loss 6.15018230676651 loss1 6.15018230676651 loss2 23.095962285995483 loss3 23.083324432373047\n",
      "168 tot loss 6.145814597606659 loss1 6.145814597606659 loss2 23.096188068389893 loss3 23.08351969718933\n",
      "169 tot loss 6.141472578048706 loss1 6.141472578048706 loss2 23.096418619155884 loss3 23.083714723587036\n",
      "170 tot loss 6.137142717838287 loss1 6.137142717838287 loss2 23.09664797782898 loss3 23.083911418914795\n",
      "171 tot loss 6.1328296065330505 loss1 6.1328296065330505 loss2 23.096878051757812 loss3 23.084104537963867\n",
      "172 tot loss 6.12852817773819 loss1 6.12852817773819 loss2 23.0971040725708 loss3 23.084296703338623\n",
      "173 tot loss 6.124244511127472 loss1 6.124244511127472 loss2 23.09733748435974 loss3 23.08449077606201\n",
      "174 tot loss 6.119975388050079 loss1 6.119975388050079 loss2 23.097570657730103 loss3 23.08468246459961\n",
      "175 tot loss 6.115725755691528 loss1 6.115725755691528 loss2 23.097797393798828 loss3 23.084871530532837\n",
      "176 tot loss 6.111490070819855 loss1 6.111490070819855 loss2 23.098028898239136 loss3 23.085060358047485\n",
      "177 tot loss 6.107269704341888 loss1 6.107269704341888 loss2 23.098265171051025 loss3 23.085248470306396\n",
      "178 tot loss 6.103070974349976 loss1 6.103070974349976 loss2 23.0985004901886 loss3 23.085433959960938\n",
      "179 tot loss 6.09888356924057 loss1 6.09888356924057 loss2 23.09873080253601 loss3 23.085619926452637\n",
      "180 tot loss 6.0947094559669495 loss1 6.0947094559669495 loss2 23.098960876464844 loss3 23.085806608200073\n",
      "181 tot loss 6.090552985668182 loss1 6.090552985668182 loss2 23.099186658859253 loss3 23.08599591255188\n",
      "182 tot loss 6.0864139795303345 loss1 6.0864139795303345 loss2 23.09941267967224 loss3 23.08618927001953\n",
      "183 tot loss 6.082290351390839 loss1 6.082290351390839 loss2 23.099636554718018 loss3 23.08638048171997\n",
      "184 tot loss 6.078181505203247 loss1 6.078181505203247 loss2 23.099856853485107 loss3 23.086572408676147\n",
      "185 tot loss 6.074097037315369 loss1 6.074097037315369 loss2 23.10007119178772 loss3 23.08677339553833\n",
      "186 tot loss 6.070024371147156 loss1 6.070024371147156 loss2 23.100287437438965 loss3 23.08697199821472\n",
      "187 tot loss 6.065967619419098 loss1 6.065967619419098 loss2 23.100500345230103 loss3 23.08717155456543\n",
      "188 tot loss 6.061922609806061 loss1 6.061922609806061 loss2 23.1007080078125 loss3 23.087369203567505\n",
      "189 tot loss 6.057899594306946 loss1 6.057899594306946 loss2 23.100911617279053 loss3 23.087575435638428\n",
      "190 tot loss 6.053887724876404 loss1 6.053887724876404 loss2 23.101115703582764 loss3 23.087779760360718\n",
      "191 tot loss 6.049892604351044 loss1 6.049892604351044 loss2 23.101313591003418 loss3 23.087986707687378\n",
      "192 tot loss 6.045907616615295 loss1 6.045907616615295 loss2 23.101518869400024 loss3 23.088181734085083\n",
      "193 tot loss 6.041946530342102 loss1 6.041946530342102 loss2 23.101715087890625 loss3 23.088387727737427\n",
      "194 tot loss 6.037995934486389 loss1 6.037995934486389 loss2 23.101917028427124 loss3 23.08859086036682\n",
      "195 tot loss 6.034061849117279 loss1 6.034061849117279 loss2 23.102116107940674 loss3 23.088794708251953\n",
      "196 tot loss 6.030146896839142 loss1 6.030146896839142 loss2 23.102314472198486 loss3 23.088987588882446\n",
      "197 tot loss 6.0262439250946045 loss1 6.0262439250946045 loss2 23.10251021385193 loss3 23.08918809890747\n",
      "198 tot loss 6.0223610401153564 loss1 6.0223610401153564 loss2 23.10270094871521 loss3 23.08938717842102\n",
      "199 tot loss 6.018490254878998 loss1 6.018490254878998 loss2 23.102892637252808 loss3 23.089587450027466\n",
      "200 tot loss 6.014634966850281 loss1 6.014634966850281 loss2 23.103084087371826 loss3 23.089776515960693\n",
      "201 tot loss 6.010802626609802 loss1 6.010802626609802 loss2 23.103272438049316 loss3 23.089972734451294\n",
      "202 tot loss 6.0069809556007385 loss1 6.0069809556007385 loss2 23.103461265563965 loss3 23.090174198150635\n",
      "203 tot loss 6.003181099891663 loss1 6.003181099891663 loss2 23.103652954101562 loss3 23.090362548828125\n",
      "204 tot loss 5.999388694763184 loss1 5.999388694763184 loss2 23.103839635849 loss3 23.090564727783203\n",
      "205 tot loss 5.9956174492836 loss1 5.9956174492836 loss2 23.104024171829224 loss3 23.090765714645386\n",
      "206 tot loss 5.991858124732971 loss1 5.991858124732971 loss2 23.104215621948242 loss3 23.090956687927246\n",
      "207 tot loss 5.988117873668671 loss1 5.988117873668671 loss2 23.104405164718628 loss3 23.09115219116211\n",
      "208 tot loss 5.98438423871994 loss1 5.98438423871994 loss2 23.10459280014038 loss3 23.09133267402649\n",
      "209 tot loss 5.980672657489777 loss1 5.980672657489777 loss2 23.104780435562134 loss3 23.09152102470398\n",
      "210 tot loss 5.976981103420258 loss1 5.976981103420258 loss2 23.104965925216675 loss3 23.091718196868896\n",
      "211 tot loss 5.973300993442535 loss1 5.973300993442535 loss2 23.105157613754272 loss3 23.09190320968628\n",
      "212 tot loss 5.969641804695129 loss1 5.969641804695129 loss2 23.105347156524658 loss3 23.09209680557251\n",
      "213 tot loss 5.965991795063019 loss1 5.965991795063019 loss2 23.10553479194641 loss3 23.09228205680847\n",
      "214 tot loss 5.962353944778442 loss1 5.962353944778442 loss2 23.10573124885559 loss3 23.092463731765747\n",
      "215 tot loss 5.958731472492218 loss1 5.958731472492218 loss2 23.10592293739319 loss3 23.092657327651978\n",
      "216 tot loss 5.955132603645325 loss1 5.955132603645325 loss2 23.106112480163574 loss3 23.092844009399414\n",
      "217 tot loss 5.9515493512153625 loss1 5.9515493512153625 loss2 23.106303691864014 loss3 23.09302830696106\n",
      "218 tot loss 5.9479785561561584 loss1 5.9479785561561584 loss2 23.10647940635681 loss3 23.09322738647461\n",
      "219 tot loss 5.944427490234375 loss1 5.944427490234375 loss2 23.1066575050354 loss3 23.093415021896362\n",
      "220 tot loss 5.940893650054932 loss1 5.940893650054932 loss2 23.10683798789978 loss3 23.093611240386963\n",
      "221 tot loss 5.937370717525482 loss1 5.937370717525482 loss2 23.10702133178711 loss3 23.093799114227295\n",
      "222 tot loss 5.933865189552307 loss1 5.933865189552307 loss2 23.107197284698486 loss3 23.09398627281189\n",
      "223 tot loss 5.930374503135681 loss1 5.930374503135681 loss2 23.107370853424072 loss3 23.09417986869812\n",
      "224 tot loss 5.926904261112213 loss1 5.926904261112213 loss2 23.10755205154419 loss3 23.09436058998108\n",
      "225 tot loss 5.923444747924805 loss1 5.923444747924805 loss2 23.107731103897095 loss3 23.094545364379883\n",
      "226 tot loss 5.91998678445816 loss1 5.91998678445816 loss2 23.107910871505737 loss3 23.094723224639893\n",
      "227 tot loss 5.916543960571289 loss1 5.916543960571289 loss2 23.10808539390564 loss3 23.094899654388428\n",
      "228 tot loss 5.913119077682495 loss1 5.913119077682495 loss2 23.10826086997986 loss3 23.09508228302002\n",
      "229 tot loss 5.909709632396698 loss1 5.909709632396698 loss2 23.108437061309814 loss3 23.095267295837402\n",
      "230 tot loss 5.906317949295044 loss1 5.906317949295044 loss2 23.108609676361084 loss3 23.09545588493347\n",
      "231 tot loss 5.902941584587097 loss1 5.902941584587097 loss2 23.10878014564514 loss3 23.095643043518066\n",
      "232 tot loss 5.89958381652832 loss1 5.89958381652832 loss2 23.108949184417725 loss3 23.095833778381348\n",
      "233 tot loss 5.89624160528183 loss1 5.89624160528183 loss2 23.10911726951599 loss3 23.096025228500366\n",
      "234 tot loss 5.8929163813591 loss1 5.8929163813591 loss2 23.10927963256836 loss3 23.096217155456543\n",
      "235 tot loss 5.889602482318878 loss1 5.889602482318878 loss2 23.109439849853516 loss3 23.096410036087036\n",
      "236 tot loss 5.886302053928375 loss1 5.886302053928375 loss2 23.109598875045776 loss3 23.096604585647583\n",
      "237 tot loss 5.883020341396332 loss1 5.883020341396332 loss2 23.109757900238037 loss3 23.096797466278076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238 tot loss 5.879753649234772 loss1 5.879753649234772 loss2 23.10991358757019 loss3 23.096988201141357\n",
      "239 tot loss 5.8765029311180115 loss1 5.8765029311180115 loss2 23.11006999015808 loss3 23.097179651260376\n",
      "240 tot loss 5.8732717633247375 loss1 5.8732717633247375 loss2 23.110225439071655 loss3 23.097376585006714\n",
      "241 tot loss 5.870048940181732 loss1 5.870048940181732 loss2 23.11038112640381 loss3 23.09757399559021\n",
      "242 tot loss 5.866842448711395 loss1 5.866842448711395 loss2 23.11053705215454 loss3 23.097773790359497\n",
      "243 tot loss 5.863650918006897 loss1 5.863650918006897 loss2 23.110686779022217 loss3 23.097971439361572\n",
      "244 tot loss 5.860475480556488 loss1 5.860475480556488 loss2 23.110838174819946 loss3 23.09816598892212\n",
      "245 tot loss 5.857313454151154 loss1 5.857313454151154 loss2 23.110985279083252 loss3 23.09836435317993\n",
      "246 tot loss 5.854165434837341 loss1 5.854165434837341 loss2 23.11113715171814 loss3 23.09856104850769\n",
      "247 tot loss 5.851036250591278 loss1 5.851036250591278 loss2 23.111284494400024 loss3 23.098759651184082\n",
      "248 tot loss 5.847919404506683 loss1 5.847919404506683 loss2 23.11143660545349 loss3 23.098952770233154\n",
      "249 tot loss 5.844818532466888 loss1 5.844818532466888 loss2 23.11158323287964 loss3 23.09914779663086\n"
     ]
    }
   ],
   "source": [
    "train_target = train_target.long()\n",
    "train_classes = train_classes.long()\n",
    "crit = nn.CrossEntropyLoss\n",
    "\n",
    "train_model(m, train_input, train_target, train_classes,mini_batch_size, crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error Net 23.00% 230/1000\n"
     ]
    }
   ],
   "source": [
    "nb_test_errors = compute_nb_errors(m, test_input, test_target, mini_batch_size = 100)\n",
    "print('test error Net {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([20, 512])\n",
      "torch.Size([20])\n",
      "torch.Size([2, 20])\n",
      "torch.Size([2])\n",
      "torch.Size([20, 20])\n",
      "torch.Size([20])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "for p in m.parameters():\n",
    "    print(p.shape)\n",
    "    num+=1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
